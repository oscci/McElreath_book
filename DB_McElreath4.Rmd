---
title: "McElreath_ch4"
author: "DVM Bishop"
date: "24/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(rethinking)
require(tidyverse)
```

## Chapter 4

McElreath compares linear regression to Ptolemy's thinking about the universe. Quite clever: jolts you into realising that any method we use is just a model.
That's a core message of this book: don't apply stats formulaicly thinking it will give you the true - need to understand limitations/assumptions of models.
Reminder of George Box quote:  "All models are wrong, but some are useful".

## 4.1 Why normal distributions are normal
Illustrates how repeated sampling from binomial leads to normal distribution if enough trials.

```{r manybinoms}
pos <- replicate( 1000 , sum( runif(16,-1,1) ) )
hist(pos) 
# runif means select a random uniform number. runif(16) selects 16 random uniform numbers.
# Default is value from 0 to 1 - not sure why M complicates it, but he's specified range from -1 to 1  : I guess so mean is zero
#Qu; what would you expect mean to be if we just used runif(16)?
posb <- replicate( 1000 , sum( runif(16 ) ))
hist(posb)
plot(density(posb))


```

This reminded me of the Galton Board, or quincunx, which illustrates same principle with binomial distribution.
- see https://www.mathsisfun.com/data/quincunx.html

Nice explanation of why any kind of distribution will produce normal on repeated sampling: will converge on mean, but errors will be random and push to one direction or other, but cancel each other out.

Now try multiplication

```{r multiplic}
prod( 1 + runif(12,0,0.1) )

#need to deconstruct this
runif(12,0,0.1) #12 numbers between 0 and .1
#Presumably added 1, because need numbers > 1 so they don't get smaller when multiplied
#prod is just command for product (something I did not know....)

#So you are generating 12 numbers between 1 and 1.1 and multiplying them together

#Now do this 10000 times

growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) )
dens( growth , norm.comp=TRUE ) 
#Oh wow! I wish I had known about norm.comp earlier in life!

#But we learn that this only works with small numbers. If we change range to 0 to .5, we get deviations from normality

growth <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) )
dens( growth , norm.comp=TRUE ) 

# but change to log scale and you get back normal
growth <- replicate( 10000 , log(prod( 1 + runif(12,0,0.5) ) ))
dens( growth , norm.comp=TRUE ) 
```

Onotological  :to do with nature of being. Normal distribution common in nature.
Epistemological : theory of knowledge. Consistent with focus on means and variance. Minimal assumptions
This epistemological justification is premised on information theory and maximum
entropy.
Gaussian distribution doesn't handle heavy-tailed distributions very well.

```{r normalcurve}
#Monstrous formula but mostly scaling - key bit is exp -(y-mu)^2

curve( exp( -x^2 ) , from=-3 , to=3 )

dnorm(0,mean=0,sd=0.1)
myn <- rnorm(2000,0,.1)
plot(density(myn)) #illustrates point that at point 0, height is nearly 4.
#But overall area under curve is 1
#Gaussian often described with precision (tau), rather than variance.
```
Models; mathematical language
e.g. worlds
W ~ Binomial(N, p)
p ~ Uniform(0, 1)
where W was the observed count of water, N was the total number of tosses, and p was the
proportion of water on the globe. Read the above statement as:
The count W is distributed binomially with sample size N and probability p.
The prior for p is assumed to be uniform between zero and one.

First line defines the likelihood function used in Bayes’ theorem. The other lines define priors.

Both of the lines in this model are stochastic, -ie relationships are probabilistic

```{r modeldefformula}
#Rcode 4.6, p 81
w <- 6; n <- 9
p_grid <- seq(from=0,to=1,length.out=100)
posterior <- dbinom(w,n,p_grid)*dunif(p_grid,0,1)
posterior <- posterior/sum(posterior)

```

A digression to try to understand dunif.

In e.g. above it generates a vector of ones
I tried changing to dunif(p_grid,0,2) and it generated vector of .5 s !!

#OK this is now getting interesting....
dunif(p_grid,0,2) # vector of .5 s 
dunif(p_grid,.5,1) # produces 1-50 zeros, and 51-100 2s
dunif(p_grid,1,2) # produces 1-99 zeros and one one!

Help says dunif gives density for interval from min to max

OK - so for each value of p_grid, it is evaluated as to whether it falls in that interval
That seems to work , except why do we then get .5s for range 0,2
Values go half way to max range?

 Test: with range .3 to 1: should have 30 zeros and 60 ones?
dunif(p_grid,.3,1) 

No!!  30 zeros, and the rest are 1.428571

sum(dunif(p_grid,.3,1)) # The sum is 100

sum(dunif(p_grid,0,2)) # The sum is 50

sum(dunif(p_grid,1,2)) # The sum is one!

dunif(p_grid,1,3) # all zeros, except for last which is .5

Time for some iterated examples, based on
 https://statisticsglobe.com/uniform-distribution-in-r-dunif-punif-qunif-runif
 
 This shows that the value is zero unless the p_grid value is in the range specified.
 But it's still a bit unclear to me how the nonzero value works.
 This may not matter because you'd divide by total of the dunif values to sum to one, but it does still niggle

```{r dunif.explore}
x <- seq(0, 100, by = 1) 
y <- dunif(x,min=0,max=100)
dunif.df <- data.frame(matrix(NA, nrow=9,ncol=5))
colnames(dunif.df)<-c('min','max','sum','mean','maxval')
thisrow<-0
for (mymax in seq(50,200,50)){

for (mymin in seq(25,100,25)){
  thisrow<-thisrow+1
  if(mymin < mymax){
    y <- dunif(x,mymin,mymax)
  plot(x,y,main=paste(' mymin= ',mymin,': mymax = ',mymax),,type='o')
 
  dunif.df[thisrow,1]<-mymin
  dunif.df[thisrow,2]<-mymax
  dunif.df[thisrow,3]<-sum(y)
  dunif.df[thisrow,4]<-mean(y)
  dunif.df[thisrow,5]<-max(y)
 }
}
}
dunif.df<-dunif.df[order(dunif.df$maxval),]



```
With a bit of playing around, found that the nonzero value in these cases can be found by the formula:
n/(range*n)
where n is length of p_grid, and range is difference between max and min

Now check does it work with the original problem with fractional values in p_grid

max(dunif(p_grid,.3,1))
 1.428571

n=100
range=.7
100/(.7*100) = 1.428

Yes!!
Also works if range extends beyond N points
max(dunif(p_grid,0,2)) = .5
equivalent to 100/(2*100) - which is actually just 1/(max-min)

So simple description of dunif:
For values outside range of min,max, it returns zero
For values inside rnage of min,max it returns 1/(max-min)



Now back to McElreath....
# Gaussian model of height

Basic logic: There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large SD. Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of mean and SD and rank them by posterior plausibility. Posterior
plausibility provides a measure of the logical compatibility of each possible distribution with the data and model.

NB the “estimate” here will be the entire posterior distribution, not any point within it.

```{r howelldat}
require(rethinking)
data(Howell1)
myd <- Howell1 #I have changed name from d to myd, as I find single character names for objects very confusing; if I have my at the front, I know it's a variable I have created.
#This also forces me to retype some lines, so pay attention to the code.
precis(myd)

myd2 <- myd[ myd$age >= 18 , ] #select those aged over 17

#NB when first learning R, I found it helpful to break a command like this into 2 steps to make it very clear what the code was doing.

i.e. 
w <- which(myd$age >= 18) #if you inspect w, you see it's a list of rownumbers for cases with age >=18
myd2 <- myd[w,] #now select from myd those with the same rows as w

#This is not efficient programming, but quite often good programming is quite daunting for newbies to understand. And this creates an additional line, but is easier to read back. now I'm more experienced, I'll more often use the single-line version of code, but not always.
#(Pleased to see that McElreath agrees the standard code for doing this in one line is confusing!)

dens(myd2$height)

#plot prior for mu- set mean 178 and SD to 20
curve( dnorm( x , 178 , 20 ) , from=100 , to=250 )
#NB I like the way he explains he uses prior knowledge of human height here - wouldn't use uniform prior as we already know aspects of height.
curve( dunif( x , 0 , 50 ) , from=-10 , to=60 ) #prior for SD - this is flat between reasonable range

#Now plot height distribution from prior (4.14)
sample_mu <- rnorm( 1e4 , 178 , 20 ) 
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h ) #nb I played with increasing rnorm n to 1e6 - as I expected, curve gets smoother - because more samples

#Compare with nonsensible prior with large range of mean ht (4.15)
sample_mu <- rnorm( 1e4 , 178 , 100 ) 
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h )
#Leads to impossibly tall or short
```

N.B. *Prior predictive simulation is very useful for assigning sensible priors*

## 4.3.3. Grid approximation of the posterior distribution.
(M notes code is hard to follow and we shouldn't worry...)
```{r code4.16}
#below I changed d2 to myd2 in line 252
mu.list <- seq( from=150, to=160 , length.out=100 )  #length.out is new to me but useful - ensures you get desired N points from the seq command (here 100 , not 101)
sigma.list <- seq( from=7 , to=9 , length.out=100 )
post <- expand.grid( mu=mu.list , sigma=sigma.list ) #every combination of mu.list and sigmal.list


post$LL <- sapply( 1:nrow(post) , function(i) sum(
dnorm( myd2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) ) #for each row of post, compute log of dnorm value
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
dunif( post$sigma , 0 , 50 , TRUE ) #adding the log values equiv to multiplying to get total likelihood 
post$prob <- exp( post$prod - max(post$prod) )


contour_xyz( post$mu , post$sigma , post$prob )
image_xyz( post$mu , post$sigma , post$prob ) #x is mu, y is sigma, darkness is prob

#Now sample values from posterior in proportion to their probability
#R code 4.19 
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE , prob=post$prob )
sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows ]
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) ) #cex is size; pch is symbol type, last index of col.alpha gives transparency

dens(sample.mu)
dens(sample.sigma)
#look at percentile intervals
PI( sample.mu ) #formula 4.22
PI( sample.sigma )
``` 

## Issues with tail of SD estimates
Can't be zero so tend to have longer R hand tail.
Can illustrate by taking just a small sample.

```{r SDtail.issue}
d3 <- sample( myd2$height , size=20 ) #code 4.23
mu.list <- seq( from=150, to=170 , length.out=200 ) 
sigma.list <- seq( from=4 , to=20 , length.out=200 )
post2 <- expand.grid( mu=mu.list , sigma=sigma.list )
post2$LL <- sapply( 1:nrow(post2) , function(i)
sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] ,
log=TRUE ) ) )
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob <- exp( post2$prod - max(post2$prod) )
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,
prob=post2$prob )
sample2.mu <- post2$mu[ sample2.rows ]
sample2.sigma <- post2$sigma[ sample2.rows ]
plot( sample2.mu , sample2.sigma , cex=0.5 ,
col=col.alpha(rangi2,0.1) ,
xlab="mu" , ylab="sigma" , pch=16 )
dens( sample2.sigma , norm.comp=TRUE )

```

## 4.3.5. Finding the posterior distribution with quap.

Quadratic approximation: handy way to quickly make inferences about the shape of the posterior. The posterior’s peak will lie at the maximum a
posteriori estimate (MAP), and we can get a useful image of the posterior’s shape by using the quadratic approximation of the posterior distribution at this peak.
To build the quadratic approximation, we’ll use quap.

NB using alist - explained later:
The two functions alist and list do the same basic thing: allow you to make a collection of arbitrary R objects. They differ in one important respect: list evaluates the code you embed inside it, while alist does not. So when you define a list of formulas, you should use alist, so the code isn’t executed.
But when you define a list of start values for parameters, you should use list, so that code like mean(d2$height) will be evaluated to a numeric value.

```{r quap.intro}
library(rethinking)
data(Howell1)
myd <- Howell1
d2 <- myd[ myd$age >= 18 , ]

#Create a list of formulae
flist <- alist(
height ~ dnorm( mu , sigma ) ,
mu ~ dnorm( 178 , 20 ) ,
sigma ~ dunif( 0 , 50 )
)

m4.1 <- quap( flist , data=d2 )
precis(m4.1) #this is same as 'summary' in this context

```

These numbers provide Gaussian approximations for each parameter’s marginal distribution.
This means the plausibility of each value of mu after averaging over the plausibilities of each value of sd is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4.
The 5.5% and 94.5% quantiles are percentile interval boundaries, corresponding to an 89% compatibility interval. Why 89%? It’s just the default. It displays a quite wide interval, so it shows a high-probability range of parameter values. 
If you want another interval, such as the conventional and mindless 95%, you can use precis(m4.1,prob=0.95).

```{r startingvalues}
start <- list(
mu=mean(d2$height),
sigma=sd(d2$height)
)
m4.1 <- quap( flist , data=d2 , start=start )
precis(m4.1)
```

```{r incorporateprior}
#now with meaningful prior, sd of .1
#rcode 4.31
m4.2 <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu ~ dnorm( 178 , 0.1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d2 )
precis( m4.2 )

#Note by changing prior for mean, we affect estimate for SD
```

```{r vcov.explore}
diag( vcov( m4.1 ) ) #4.33
cov2cor( vcov( m4.1 ) )

```
 NB correlation matrix is just standardized covariance matrix.
 I.e. the entries on the diagonal will become 1 after conversion to r.
 General formula is r.xy = cov.xy/s.x*s.y
 so if myv <- vcov(m4.1)
 and myv[1,2] is .000218
 then entry for correlation in cell[1,2] will be myv[1,2]/sqrt(myv[1,1]*myv[2,2])

```{r sampleposterior4.34}
library(rethinking) 
post <- extract.samples( m4.1 , n=1e4 )
head(post)
precis(post)

```



# 4.4 Linear prediction

Will look at wt as predictor of ht (seems odd way around...)

```{r htwt}
plot( d2$height ~ d2$weight )
```
Now look at predicting with linear model

```{r 4.38}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 )
b <- rnorm( N , 0 , 10 ) #mean 0 , SD 10, can be -ve
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
col=col.alpha("black",0.2) )
#because b can be negative get negative ht/wt - not sensible
#also extremes not sensible values, including -ve ht

```
Constrain prior beta to positive values
Do this by using lognormal prior
```{r lognormal.prior}
b <- rlnorm( 1e4 , 0 , 1 ) #4.40
dens( b , xlim=c(0,5) , adj=0.1 )

#Now use this as prior
set.seed(2971) 4.41
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 )
b <- rlnorm( N , 0 , 1 )
dens( b , xlim=c(0,5) , adj=0.1 )

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
col=col.alpha("black",0.2) )
```

Notes re priors and p-hacking.
The problem is that when the model is adjusted
in light of the observed data, then p-values no longer retain their original meaning. False results are to be expected. We don’t pay any attention to p-values in this book. But the danger remains, if we
choose our priors conditional on the observed sample, just to get some desired result. The procedure we’ve performed in this chapter is to choose priors conditional on pre-data knowledge of the data— its constraints, ranges, and theoretical relationships. This is why the actual data are not shown in the earlier section. We are judging our priors against general facts, not the sample.

```{r 4.42}
# define the average weight, x-bar
xbar <- mean(d2$weight)
# fit model
#mu is no longer a parameter, since it has become a function of the parameters a and b.

m4.3 <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d2 )

summary(m4.3)
```

Use of logarithms /exponentials
All a bit hair-raising

```{r log.exp}
#4.43 - note log_b is a defined variable, not a function here
m4.3b <- quap( 
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + exp(log_b)*( weight - xbar ),
a ~ dnorm( 178 , 100 ) ,
log_b ~ dnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d2 )
summary(m4.3b)
#Note exp(-.1) equals .904, which was b value in original m4.3

#this gives plot from next section
pairs(m4.3b)
```

Importance of plotting posterior distributions
allows you to inquire about things that are hard to read from tables:
(1) Whether or not the model fitting procedure worked correctly
(2) The absolute magnitude, rather than merely relative magnitude, of a relationship
between outcome and predictor
(3) The uncertainty surrounding an average relationship
(4) The uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty

## 4.4.3.2. Plotting posterior inference against the data.

```{r 4.46}
plot( height ~ weight , data=d2 , col=rangi2 ) 
post <- extract.samples( m4.3 )
a_map <- mean(post$a)
b_map <- mean(post$b)
curve( a_map + b_map*(x - xbar) , add=TRUE )

N <- 10 #4.48
dN <- d2[ 1:N , ]
mN <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*( weight - mean(weight) ) ,
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=dN )

#Now let’s plot 20 of these lines, to see what the uncertainty looks like.

# extract 20 samples from the posterior 4.49
Nsamples<-50
post <- extract.samples( mN , Nsamples )
# display raw data and sample size
plot( dN$weight , dN$height ,
xlim=range(d2$weight) , ylim=range(d2$height) ,
col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",Nsamples))
# plot the lines, with transparency
for ( i in 1:Nsamples )
curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,
col=col.alpha("black",0.3) , add=TRUE )
```


Bayesian perspective: Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model.

```{r moresamples}
post <- extract.samples( m4.3 )
mu_at_50 <- post$a + post$b * ( 50 - xbar )
dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )
PI( mu_at_50 , prob=0.89 )

#What these numbers mean is that the central 89% of the ways for the model to produce the data place the average height between about 159 cm and 160 cm (conditional on the model and data), assuming the weight is 50 kg.
#But need for all possible weights - ie need link function

mu <- link( m4.3 )
str(mu)
dens(mu) #distribution of mu for each individual in the original data.

#We need distribution of my for each unique weight value on the horizontal axis. So create weight_seq

#4.54 # define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq( from=25 , to=70 , by=1 )
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link( m4.3 , data=data.frame(weight=weight.seq) )
str(mu)


# use type="n" to hide raw data
plot( height ~ weight , d2 , type="n" )
# loop over samples and plot each mu value
for ( i in 1:100 )
points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )

# summarize the distribution of mu #4.56
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )

# plot raw data 4.57
# fading out points to make line and interval more visible
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight
lines( weight.seq , mu.mean )
# plot a shaded region for 89%
shade( mu.PI , weight.seq )
```
To summarize, here’s the recipe for generating predictions and intervals from the posterior of a fit model.
(1) Use link to generate distributions of posterior values for mu. The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across.
(2) Use summary functions like mean or PI to find averages and lower and upper bounds of mu for each value of the predictor variable.
(3) Finally, use plotting functions like lines and shade to draw the lines and intervals.

```{r explaining.link}
#Can make link function by code

#4.58 
# Struggling with this...
post <- extract.samples(m4.3)
mu.link <- function(weight) post$a + post$b*( weight - xbar )

weight.seq <- seq( from=25 , to=70 , by=1 )
mu <- sapply( weight.seq , mu.link )
mu.mean <- apply( mu , 2 , mean )
mu.HPDI <- apply( mu , 2 , HPDI , prob=0.89 )


```