---
title: "Chapter 14: Adventures in Covariance"
output: html_notebook
---

Nightmare after upgrading to R4.02 .
Stan stopped working.
Loads of impossible advice on web that tells you to do x, but before you can do x you must do y, and to do y, you must first do z (or possibly make worrying changes to system files as well), and so on and so on.

McElreath suggested could use cmdstan and I have tried this:
https://mc-stan.org/cmdstanr/articles/cmdstanr.html
which led to:
CmdStan path set to: /Users/dorothybishop/.cmdstanr/cmdstan-2.24.1


Example problem of recording average wait for coffee in a cafe. This time you can get an estimate for morning and afternoon. Morning tends to be longer wait. Difference between morning and afternoon gives a slope.  
Intercept corresponds to average wait in morning and slope to effect of treatment (am or pm).

Formally
   $$\mu_i = \alpha_{CAFE[i]} + \beta_{CAFE[i]} A_j$$
   
Pooling gives more efficient estimates - for both intercepts and slopes. Any batch of parameters with exchangeable index values can and probably should be pooled.  
Exchangeable just means they are arbitrary labels - no true ordering. 

But intercepts and slopes are related. Floor and ceiling effects etc.
More scope for big change if high value to start with.  

At this point we are warned this material is difficult!  

It is possible to use pooling with categories such as age or location - we can model covariation among continuous categories.  

## 14.1 Varying slopes by construction  
To pool information across intercepts and slopes we need to moel the join population of intercepts and slopes - ie. model their covariance.  
Need a joint multivariate Gaussian distribution for all the varying effects - rather than separate distributions, assign a 2D Gaussian distribution to include intercepts and slopes as the 2 dimensions. 

We'll do this by simulating the coffee shop example. 

```{r code14.1}
require(rethinking)
a <- 3.5        #average morning wait time
b <- (-1)       #average difference afternoon wait time
sigma_a <-1     #SD of intercepts
sigma_b <- .5   #SD of slopes
rho <- (-.7)    #correlation between intercepts and slopes

#vector of means of a and b for cafes
Mu <- c(a,b)
#use matrix to build entire cov matrix directly
cov_ab <- sigma_a*sigma_b*rho
Sigma <- matrix(c(sigma_a^2, cov_ab,cov_ab,sigma_b^2),ncol=2)

#alternative approach
sigmas <- c(sigma_a, sigma_b)   #standard deviations
Rho <- matrix(c(1,rho,rho,1),nrow=2) #correlation matrix

#matrix multiplication to get the covariance matrix
Sigma <- diag(sigmas)%*% Rho %*% diag(sigmas)

#now simulate cafes
N_cafes <- 20
library(MASS)
set.seed(5) 
vary_effects <- mvrnorm(N_cafes,Mu,Sigma)
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]
colnames(vary_effects) <- c('a','b')
head(vary_effects)

plot(a_cafe,b_cafe,col=rangi2)
#overlay population distribution
library(ellipse)
for (l in c(.1,.3,.5,.8,.99)){
  lines(ellipse(Sigma, centre=Mu,level=l),col=col.alpha("black",.2))
}
```

SO far we simulated cafes and their average properties.  
We now simulate 10 visits to each cafe, 5 in morning and 5 in afternoon.

```{r code14.10}
set.seed(22)
N_visits <- 10
afternoon <- rep(0:1,N_visits*N_cafes/2)
cafe_id <- rep(1:N_cafes,each=N_visits)
mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- .5 #sd within cafes
wait <-  rnorm(N_visits*N_cafes,mu,sigma)
d<- data.frame(cafe=cafe_id,afternoon=afternoon, wait=wait)
head(d)
```

Clusters in data are the cafes. Each cluster observed under 2 conditions, am and pm.
Can estimate an individual intercepts for each cluster and an individual slope.

Everything is balanced - not required for MLM

14.1.3 Varying slopes model.

As well as the usual priors for average intercept and average slope, with have prior for correlation matrix. For this we use the function rlkjcorr. We will just look at what this does.

```{r code14.11}
N = 100000  #I varied N so I could understand better
eta = 2 #when eta =1 , all correlations equally likely; when it gets bigger peaks more at zero
R <- rlkjcorr(N,K=2,eta=eta)
dens(R[,1,2],xlab='correlation')
#R is a 3D array 10000 x 2 x 2

#If you set N to 10, you have 10 values with random correlations in col 1 and 1 in col 2,
#plus 10 values with the reverse assignment. 
# dens(R[,2,1]) gives same as dens(R[,1,2])
# Seems redundant!
```

Now we are ready to fit the model.
```{r code14.12}
set_ulam_cmdstan(TRUE) 
set.seed(867530)
m14.1 <- ulam(
  alist(
    wait ~ normal (mu,sigma),
    mu <- a_cafe[cafe]+b_cafe[cafe]*afternoon,
    c(a_cafe,b_cafe)[cafe] ~ multi_normal(c(a,b),Rho,sigma_cafe),
    a ~ normal(5,2),
    b ~ normal(-1,.5),
    sigma_cafe ~ exponential(1),
     sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),data=d,chains=4,cores=4)

precis(m14.1,depth=3) #with depth 3 you can see the Rho values (matrix parameters)

```

```{r code14.13}
post <- extract.samples(m14.1)
dens(post$Rho[,1,2],xlim=c(-1,1),col='blue',xlab='correlation',main='Correlation between intercepts and slopes') #posterior off-diagonal value of Rho
R <- rlkjcorr(1e4,K=2,eta=2) #prior
dens(R[,1,2],add=TRUE,lty=2)
text(0.1,2,'m14.1\nblue is posterior, dotted black is prior')


```

Posterior r is bunched below zero - model has learned negative correlation from data.

M suggests trying a different prior, so we'll change it to a flat prior

```{r code14.12a}
set.seed(867530)
m14.1a <- ulam(
  alist(
    wait ~ normal (mu,sigma),
    mu <- a_cafe[cafe]+b_cafe[cafe]*afternoon,
    c(a_cafe,b_cafe)[cafe] ~ multi_normal(c(a,b),Rho,sigma_cafe),
    a ~ normal(5,2),
    b ~ normal(-1,.5),
    sigma_cafe ~ exponential(1),
     sigma ~ exponential(1),
    Rho ~ lkj_corr(1)
  ),data=d,chains=4,cores=4)

precis(m14.1a,depth=3) #with depth 3 you can see the Rho values (matrix parameters)
```

```{r plot2priors}
posta <- extract.samples(m14.1a)
dens(posta$Rho[,1,2],xlim=c(-1,1),col='blue',xlab='correlation',main='Correlation between intercepts and slopes') #posterior off-diagonal value of Rho
dens(post$Rho[,1,2],xlim=c(-1,1),col='red',xlab='correlation',main='Correlation between intercepts and slopes',add=TRUE) #posterior off-diagonal value of Rho
R <- rlkjcorr(1e4,K=2,eta=1) #prior
dens(R[,1,2],add=TRUE,lty=2,col='blue')
R <- rlkjcorr(1e4,K=2,eta=2) #prior
dens(R[,1,2],add=TRUE,lty=2,col='red')
text(0.1,2,'m14.1 and 1a\nblue with flat prior, red with eta =2, \npriors as dotted')
```

```{r code14.14}
#compute unpooled estimates directly from data
a1 <- sapply(1:N_cafes,
             function(i) mean(wait[cafe_id==i & afternoon ==0]))
b1 <- sapply(1:N_cafes,
             function(i) mean(wait[cafe_id==i & afternoon ==1]))-a1

# extract posterior means of partially pooled estimates
post <- extract.samples(m14.1)
a2 <- apply(post$a_cafe,2,mean)
b2 <- apply(post$b_cafe,2,mean)

#plot both and connect with lines
plot(a1,b1,xlab='intercept',ylab='slope',
     pch=16,col=rangi2,ylim=c(min(b1)-0.1,max(b1)+.1),
     xlim=c(min(a1)-.01,max(a1)+.01),
     main='Raw unpooled intercepts & slopes (blue) vs \npartially pooled posterior means (unfilled)')
points(a2,b2,pch=1)
for (i in 1:N_cafes) lines(c(a1[i],a2[i]),c(b1[i],b2[i]))

#Now superimpose contours
# compute posterior mean bivariate Gaussian
Mu_est <- c(mean(post$a),mean(post$b))
rho_est <- mean(post$Rho[,1,2])
sa_est <- mean(post$sigma_cafe[,1])
sb_est <- mean(post$sigma_cafe[,2])
cov_ab <-sa_est*sb_est*rho_est
Sigma_est <- matrix(c(sa_est^2,cov_ab,cov_ab,sb_est^2),ncol=2)

#draw contours
library(ellipse)
for (l in c(.1,.3,.5,.8,.99))
  lines(ellipse(Sigma_est,centre=Mu_est,level=l),
        col=col.alpha("black",.2))

```

This plot shows (a) negative correlation between slopes and intercepts and (b) shrinkage, whereby more extreme values become less extreme in the estimates.

Note that the lines connecting observed and estimated are angled, reflecting the negative correlation between slopes and intercepts. Even if intercept is average, its estimated value will change both intercept and slope.

To show same information on the outcome scale, ie waiting times, we have to compute these from the linear model.

```{r code14.16}
wait_morning_1 <- (a1)
wait_afternoon_1 <- (a1+b1)
wait_morning_2 <-(a2)
wait_afternoon_2 <-(a2+b2)  #? what's with the brackets here?

#plot both and connect with lines
plot(wait_morning_1,wait_afternoon_1,xlab='morning wait',
     ylab='afternoon wait',pch=16, col=rangi2,
     ylim=c(min(wait_afternoon_1)-.1,max(wait_afternoon_1)+.1),
     xlim = c(min(wait_morning_1)-.1,max(wait_morning_1)+.1)
     )
points(wait_morning_2,wait_afternoon_2,pch=1)
for (i in 1:N_cafes)
  lines(c(wait_morning_1[i],wait_morning_2[i]),
        c(wait_afternoon_1[i],wait_afternoon_2[i]))
abline(a=0,b=1,lty=2) #dashed line shows when afternoon and morning waits are the same

#simulate data for contour
v <- mvrnorm(1e4,Mu_est,Sigma_est)
v[,2] <-v[,1]+v[,2] #calculate afternoon wait
Sigma_est2 <- cov(v)
Mu_est2 <- Mu_est
Mu_est2[2] <- Mu_est[1]+Mu_est[2]

#draw contours
library(ellipse)
for (l in c(.1,.3,.5,.8,.99))
  lines(ellipse(Sigma_est2,centre=Mu_est2,level=l),
        col=col.alpha("black",.5))


```

Shows that shrinkage on parameter scale produces shrinkage on the outcome scale.
Gray contours imply a population of wait times. 
See positive correlation - ie some cafes generally have longer waits than others. 
But most fall below line, ie shorter waits in afternoon.  

## 14.2 Advanced varying slopes
We are back to the chimpanzees....

Uses as another example of non-centred parameterisation: model converges faster and better when reparameterised.  

model is:
$$ L_i \sim Binomial(1,p_i)$$
$$logit(p_i) =  \gamma_{TID[i]} + \alpha_{ACTOR[i],TID[i]} + \beta_{BLOCK[i],TID[i]}$$
The gamma is an avearge log-odds for each treatment,
alpha is effect for each actor in each treatment
and beta is effect for each block in each treatment

Interaction model - allows effect of treatment to vary by actor and block.  

Gives 4 + 7 x 4 + 6 x 4 = 56 parameters.  

two cluster types, actors and blocks.  
Each has a multivariate Gaussian prior; 4 dimensional because 4 treatments.
Priors don't need means because the average treatment effect, gamma, is in linear model.  

```{r code14.18}
library(rethinking)
data(chimpanzees)
d <- chimpanzees
d$block_id <- d$block
d$treatment <- 1L + d$prosoc_left + 2L*d$condition #converts into 4 levels of treatment

#what is L here ?  log? left? - seems not to do anything?

dat <- list(
  L = d$pulled_left,
  tid = d$treatment,
  actor = d$actor,
  block_id = as.integer(d$block_id))

set.seed(123)
m14.2 <- ulam(
  alist(
    L ~ dbinom(1,p),
    logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],
    
    #adaptive priors
    vector[4]:alpha[actor] ~ multi_normal(0,Rho_actor,sigma_actor),
    vector[4]:beta[block_id] ~ multi_normal(0,Rho_block,sigma_block),
    
    #fixed priors
    g[tid] ~ dnorm(0,1),
    sigma_actor ~ dexp(1),
    Rho_actor ~ dlkjcorr(4),
    sigma_block ~ dexp(1),
    Rho_block ~ dlkjcorr(4)
  ),data=dat,chains=4,cores=4)


precis(m14.2,depth=2)
trankplot(m14.2)

```

trankplots look odd and lots of divergent transitions.  
So we now look at alternative reparameterised version. We need matrices of z-scores - there is command compose-noncentered that does this for us.
(I couldn't get code from book to run, but am not sure why. This version has multiply_lower_tri_self_transpose rathan than Chol_to_Corr, but that doesn't explain the previous problem. Probably something mistyped, but I failed completely to find a bug!
This version, copied and pasted from pdf, works).
```{r code14.19}
set.seed(123)
m14.3 <- ulam( 
  alist(
    L ~ binomial(1,p),
    logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],
    # adaptive priors - non-centered
    transpars> matrix[actor,4]:alpha <-
      compose_noncentered( sigma_actor , L_Rho_actor , z_actor ),
    transpars> matrix[block_id,4]:beta <-
      compose_noncentered( sigma_block , L_Rho_block , z_block ),
    matrix[4,actor]:z_actor ~ normal( 0 , 1 ),
    matrix[4,block_id]:z_block ~ normal( 0 , 1 ),
    # fixed priors
    g[tid] ~ normal(0,1),
    vector[4]:sigma_actor ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky( 2),
    vector[4]:sigma_block ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky( 2 ),
    # compute ordinary correlation matrixes from Cholesky factors
    gq> matrix[4,4]:Rho_actor <<- multiply_lower_tri_self_transpose(L_Rho_actor),
    gq> matrix[4,4]:Rho_block <<- multiply_lower_tri_self_transpose(L_Rho_block)
  ) , data=dat , chains=4 , cores=4 , log_lik=TRUE )


```





```{r code14.2}
neff_nc <- precis(m14.3,3,pars=c("alpha","beta"))$n_eff
neff_c <- precis(m14.2,3,pars=c("alpha","beta"))$n_eff
plot(neff_c,neff_nc,xlab='centered (default)',ylab='non-centered (cholesky)',lwd=1.5)
abline(a=0,b=1,lty=2)
```

Non-centered model more efficient, with more effective samples per parameter.
Fewer iterations needed. 
Because varying effects are regularised, each varying intercept or slope counts less than one effective parameter.

```{r code14.21}
precis(m14.3,depth=2,pars=c("sigma_actor","sigma_block"))
precis(m14.2,depth=2,pars=c("sigma_actor","sigma_block"))
```

```{r code14.22}
#compute mean for each actor in each treatment

# compute mean for each actor in each treatment 
pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )
# generate posterior predictions using link
datp <- list(
actor=rep(1:7,each=4) ,
tid=rep(1:4,times=7) ,
block_id=rep(5,times=4*7) )
p_post <- link( m14.3 , data=datp )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )
# set up plot
plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )
for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )
xo <- 0.1 # offset distance to stagger raw data and predictions
# raw data
for ( j in (1:7)[-2] ) {
lines( (j-1)*4+c(1,3)-xo , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
lines( (j-1)*4+c(2,4)-xo , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28-xo , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28-xo , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )
yoff <- 0.175
text( 1-xo , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2-xo , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3-xo , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4-xo , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )
# posterior predictions
for ( j in (1:7)[-2] ) {
lines( (j-1)*4+c(1,3)+xo , p_mu[(j-1)*4+c(1,3)] , lwd=2 )
lines( (j-1)*4+c(2,4)+xo , p_mu[(j-1)*4+c(2,4)] , lwd=2 )
}
for ( i in 1:28 ) lines( c(i,i)+xo , p_ci[,i] , lwd=1 )
points( 1:28+xo , p_mu , pch=16 , col="white" , cex=1.3 )
points( 1:28+xo , p_mu , pch=c(1,1,16,16) )
```

Notes that cost of non-centered forms is that they look a lot more confusing, so limit code sharing. Also non-centered is not always better.

## 14.3 Instruments and causal designs
Lecture 18 around 19;40 (not in book)

'Multilevel horoscopes'  
How to think about multilevel models  
Think about causal model first.  
Begin with an 'empty' model - random intercepts with relevant clusters - get an idea of how much variation there is in the data before adding predictors.  
Predictors - default behaviour is to standardize, unless something like ordered categories (Dirichlet)
Use regularising priors - simulate  
Add predictors and vary their slopes.  
If repeat measures, cluster on it and do shrinkage.  
Can drop varying effects with tiny sigmas.  
Consider two sorts of posterior predicitons:  
If interested in original data, then predict about this dataset.  (e.g. if predicting about US states: there is a finite set).  
But if interested in generalising to new population, generate to new units.  
Your knowledge of domain trumps everything else.  

Covariance models include social relations, networks, factor analysis, heritability of phenotype, phylogenetic regressions, spatial autocorrelation.

Also __Instrumental variables__
(Mendelian randomisation is an instance of this) - goes back to Sewall Wright


In a DAG, some paths are causal and we want to leave them open. Others are non-causal, e.g. backdoor paths. We want to close them.  

Often you can't close all non-causal paths - there will be unobserved confounds.  

Can sometimes still get at causality.  

Consider impact of education, E, on wages, W.  
May be factors that influence both of these, e.g. how industrious.  

If this is unobserved, we can't control for it.  Backdooor criterion says cannot control for U as cannot measure it.

In this case, look for an __instrumental variable__ .
Terminology: U is unobserved variable, E is education and W is wages.  
We look for Q, an instrumental variable, satisfying these conditions:  
Q influences exposure of interest, but not the outcome.

a. Independent of U
b. Not independent of E
c. Cannot influence W except through E.

c is the exclusion restriction and is often implausible.  

Simple model:
```{r instrumental}
dag <- dagitty( "dag {
    E -> W
    U -> W
    U -> E
    Q -> E
}")
plot(graphLayout(dag))

```

Can't just add E, U and Q into model to predict W; there is noncausal path from Q to W through U
  Q -> E <- U -> W
  We are conditioning on E, and E is a collider of Q and U, so non-causal path is open. This confounds the coefficient on Q - can amplify bias. 
  
Suppose Q is quarter of year one is born in. This is independent of U, but affects E, so is instrumental. 

(also 'intention to treat' as instrumental variable)

Simulate data to understand it:

```{r code14.23}
N <- 500
b <- 0 #made it explicit this is a slope that you can set to 0 or other value
U_sim <- rnorm(N)
Q_sim <- sample(1:4,size=N,replace = TRUE)
E_sim <- rnorm(N,U_sim+Q_sim)
W_sim <- rnorm( N , U_sim + b*E_sim )
dat_sim <- list(
  W = standardize(W_sim),
   E = standardize(E_sim),
   Q = standardize(Q_sim))

#I'm just adding correlation matrix
require(corrr)
correlate(cbind(W_sim,U_sim,E_sim,Q_sim))
```
First model just regresses wages on education

```{r code 14.24}
m14.4 <- ulam( 
alist(
W ~ dnorm( mu , sigma ),
mu <- aW + bEW*E,
aW ~ dnorm( 0 , 0.2 ),
bEW ~ dnorm( 0 , 0.5 ),
sigma ~ dexp( 1 )
) , data=dat_sim , chains=4 , cores=4 )
precis( m14.4 )



```

Note that we know that bEW is zero, because we set it to that value in the simulation, but here it looks as if there is a real effect because of the effect of U.

Next show it is even worse if we add Q as a predictor


```{r code 14.25}
m14.5 <- ulam( 
alist(
W ~ dnorm( mu , sigma ),
mu <- aW + bEW*E +bQW*Q,
aW ~ dnorm( 0 , 0.2 ),
bEW ~ dnorm( 0 , 0.5 ),
bQW ~dnorm( 0 , 0.5 ),
sigma ~ dexp( 1 )
) , data=dat_sim , chains=4 , cores=4 )
precis( m14.5 )



```

To get the right model, need a generative version of DAG, which has submodels.
These are given in the book as formulae, but they actually just reflect the lines of code used in the simulation.  
We then estimate covariance between W and E - this is not descriptive covariance btween variables, but a matrix of sigmas, which is taking U into account.
We have a multivariate linear model with multiple simulataenous outcomes all modelled with join error structure.  

W and E are drawn from a multivariate distribution that has some covariance between them

Correlation between W and E determines how strong the confound is.  (So we dont need to know U - we just look at its effect in inducing a correlation).

In the model this is:

```{r code14.26}
m14.6 <- ulam(
  alist(
    c(W,E) ~ multi_normal(c(muW,muE),Rho,Sigma),
  muW <- aW + bEW*E,
  muE <- aE + bQE*Q,
  c(aW,aE) ~ normal(0,0.2),
  c(bEW,bQE) ~ normal (0,.5),
  Rho ~ lkj_corr(2),
  Sigma ~ exponential(1)
  ), data=dat_sim, chains=4, cores=4)

precis(m14.6,depth=3)

```
Note that bEW is close to zero - this is correct, as it agrees with simulation.
Rho[1,2] is 'residual correlation' between E and W (not raw correlation) - reflects fact that both are influenced by U.

(Could be that the correlation is negative - same mathematical model)  

Small digression. I had actually thought that what we'd do would be to make a kind of sequential model that would reflect the order in which variables are created. But would that work? 

- reading on to the Rethinking box, it looks like what I am describing here is '2 stage least squares' which M says is potentially problematic!


Genuinely useful instrumental variables are hard to find.  

Instrumental variables are natural experiments that impersonate randomized experiments.  E.g. quarter of birth is like an external manipulation of education, E. 

Some instruments are valid but v weak.Problem with Mendelian randomisation - individual SNPs don't do very much.  

Other natural experiments that can be useful.  

## Front door criterion.

Where you have X -> Z -> Y with U also affecting X and Y.  
We are interested in causal influence of X on Y.  
Z is a mediator.  

## Regression discontinuity. 
Trend fit to those just below and above a cutoff. Causal effect is average difference between individuals just above and just below cutoff. Influenced by entire function.

# 14.4 Social relations as correlated varying effects  

Big field of interactions. Behaviour captured in covariances.  

```{r code14.30}
library(rethinking)
data(KosterLeckie)

plot(kl_dyads$giftsAB,kl_dyads$giftsBA)
r<-round(cor(kl_dyads$giftsAB,kl_dyads$giftsBA),3)
abline(0,1,lty=2)
text(40,80,paste0('r = ',r))
```

This loads 2 sets of data - we will use kl_dyads.  
Gift exchanges between A and B in each direction.
did is unique dyad ID number
household ID is hidA and hidB

There are 300 possible dyads from 25 households.
```{r checkcombs}
combn(1:25, 2)
```

Correlation not a good indicator of relation - depends on who is designated A and B in each dyad.
Need to separate special effects reflecting relation between dyads, and general effects related to other factors such as poverty.  
Social relations model, SRM. 
We model gifts from A toB as a combination of varying effects specific to the household and the dyad. 
Gift counts are poisson variables - counts with no obvious upper bound.

First part of model:

$$y_{A->B} \sim Poisson(\lambda_{AB})$$
$$ log\lambda_{AB} = \alpha + g_A + r_B + d_{AB}$$
Intercept, alpha, represents average gifting rate (on log scale) across all dyads. Other effects offset from this.  
The g term is varying effect parametr for generalised giving tendency of household A, regardless of dyad.  
Effect r is generalised receiving of household B regardless of dyad.  
Effect d is dyad-specific rate that A gives to B.  

Parallel model for B->A

Implies each household needs a giving and a receiving rate. And each dyad also has an effect for A->B and B->A.  
(Dyad effects affected by matrilineal relationship)
The g and r terms need to have possibility to be correlated. These are not reciprocal - they relate to specific households regardless of other dyad member. 
Need 2 multinormal priors - one for population of household effects (g and r) and correlation between them; and one for population of dyad effects.
It's not meaningful which household comes first, so there is a single sigma.

One covariance matrix for household effect - one has g and r terms in - these can be correlated.  2 x 2 matrix with one correlation in it. 

Another covariance matrix, for dyad effects. For each dyad, estimate donation effects in both directions; covariance will reflect how far there is reciprocity, with dij and dji being correlated. (In theory could also be negative relationships).  This has same variance on diagonal because A and B are random.  So matrix has one variance and one correlation. 





```{r code14.31}
kl_data <- list(
  N = nrow(kl_dyads),
  N_households= max(kl_dyads$hidB),
  did=kl_dyads$did,
  hidA = kl_dyads$hidA,
  hidB = kl_dyads$hidB,
  giftsAB = kl_dyads$giftsAB,
  giftsBA = kl_dyads$giftsBA
)

m14.7 <- ulam(
  alist(
    giftsAB ~ poisson (lambdaAB),
    giftsBA ~ poisson (lambdaBA),
    log(lambdaAB) <- a +gr[hidA,1]+gr[hidB,2]+d[did,1],
     log(lambdaBA) <- a +gr[hidB,1]+gr[hidA,2]+d[did,2],
    a ~ normal(0,1),
    
    #gr matrix of varying effects
    vector[2]:gr[N_households] ~ multi_normal(0,Rho_gr,sigma_gr),
    Rho_gr ~ lkj_corr(4),
    sigma_gr ~ exponential(1),
    
    ## dyad effects
transpars> matrix[N,2]:d <-
compose_noncentered( rep_vector(sigma_d,2) , L_Rho_d , z ),
matrix[2,N]:z ~ normal( 0 , 1 ),
cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky( 8 ),
sigma_d ~ exponential(1),
## compute correlation matrix for dyads
gq> matrix[2,2]:Rho_d <<- Chol_to_Corr( L_Rho_d )
), data=kl_data , chains=4 , cores=4 , iter=2000
)

precis(m14.7,depth=3)
```

now plot output
```{r code14.32_3}
post <- extract.samples( m14.7 )
g <- sapply( 1:25 , function(i) post$a + post$gr[,i,1] )
r <- sapply( 1:25 , function(i) post$a + post$gr[,i,2] )
Eg_mu <- apply( exp(g) , 2 , mean )
Er_mu <- apply( exp(r) , 2 , mean )

plot( NULL , xlim=c(0,8.6) , ylim=c(0,8.6) , xlab="generalized giving" ,
ylab="generalized receiving" , lwd=1.5,main='
Expected giving and receiving, \nomitting dyad-specific effects.')

abline(a=0,b=1,lty=2)
text(3,6,'Each point is a household and the ellipses \nshow 50% compatibility regions. \nThere is a negative relationship between \naverage giving and average
receiving across households')
# ellipses
library(ellipse)
for ( i in 1:25 ) {
Sigma <- cov( cbind( g[,i] , r[,i] ) )
Mu <- c( mean(g[,i]) , mean(r[,i]) )
for ( l in c(0.5) ) {
el <- ellipse( Sigma , centre=Mu , level=l )
lines( exp(el) , col=col.alpha("black",0.5) )
}
}
# household means
points( Eg_mu , Er_mu , pch=21 , bg="white" , lwd=1.5 )

dy1 <- apply( post$d[,,1] , 2 , mean )
dy2 <- apply( post$d[,,2] , 2 , mean )
plot( dy1 , dy2 ,main='Dyad-specific effects, absent generalized
giving and receiving.',xlab='Household A in dyad',ylab='Household B in dyad')
text(-.5,2,'After accounting for overall \nrates of giving and\nreceiving, residual gifts are strongly \ncorrelated within dyads.')

```

Note in output a negative correlation between generalised giving and generalised receiving, evident in plot. Households that give a lot tend to receive less (tend to be rich).  

Ellipses are 50% compatibility ellipses.  

For dyads ,the covariance matrix is strong and positive. Having conditioned on generalised giving and receiving, the relationship is very strong. May be influenced by zeros - once general effects have been taken out, there is big dyad effect.  

(end of lecture 18 also has a homework problem)


#After discussion
Attempt to see how figure 14.9 on left relates to actual means for giving/receiving

I had expected this to look like figure 14.9 left, but it didn't. But of course! This is because the mean incorporates both the generalised effects AND the dyad effects.

```{r dbtrymeans}

#need to add dummy rows for same household give/receive in order to aggregate data correctly - just create dummy rows at top with zero giving/receiving when same household is A and B.
kl_dyads<-rbind(kl_dyads[1:25,],kl_dyads) #add 25 dummy rows to top
#these are copied from original kl_dyads - we'll overwrite.
for (h in 1:25){
  kl_dyads[h,1:2]<-h
  kl_dyads[h,4:5]<-0 #zero for giving/receiving to self!
}
agg1<-aggregate(kl_dyads$giftsAB,by=list(kl_dyads$hidA),FUN=mean)
agg2<-aggregate(kl_dyads$giftsBA,by=list(kl_dyads$hidA),FUN=mean)

plot(agg2$x,agg1$x,xlab='mean given',ylab='mean received')
abline(0,1)

```

## Section 14.5
Mixture of notes from book and lecture 19


Gaussian process regression, aka Bayesian nonparametric regression (bad name as Bayesian models have parameters ! Here it means there is infinite N parameters)
Very popular in machine learning.

Suitable when there are categories whose order is not arbitrary. Eg. age bands: more likely to be similar to those close in age

Don't have to make assumptions about function shapes.
'Automatic relevance determination' measure importance of covariance matrix of distances

Return to N tools as predicted by population size in islands.

Can take into account geographic difference between islands which is likely to affect how similar they are.

code 14.39 island codes indicate whether being dragged down away from expectcation (negative) or up away from it (positive)

```{r code14.37}
#load the distance matrix
library(rethinking)
data(islandsDistMatrix)
Dmat <- islandsDistMatrix
colnames(Dmat) <- c("Ml","Ti","SC","Ya","Fi","Tr","CJ", "Mn","To","Ha")
round(Dmat,1)
```

Model based on model from ch 11.
Poisson probabiity of outcome ie N tools; expected N tools

$$T_i \sim Poisson(\lambda_i)$$
$$\lambda_i = \alpha P^\beta_i/ \gamma$$
lambda values need to be adjusted by varying in intercept.
To avoid neg values make varying intercepts multiplicative - include multiplier that is 
$$exp(k_{society}_i)$$
For this model we will make the varying intercept be estimated in light of geographic distance.

There is a multivariate prior for the intercepts.  

There is a dimensional Gaussian prior for intercepts. Also vector of means which is all zeros.  This ensures that in linear model average society will multiply lambda by exp(0) = 1  

Covariance matrix is K. Formula has 3 parameters, eta, rho and sigma.
Rho affects covariance matrix  specifies that covariance between 2 socities i and j declines exponentiallly with squared distance between them.  
eta sq is max covariance between any 2 societies i and j.
and sigma provides for extra covariance when i = j

model 14.8

```{r code14.39}
data(Kline2)
d<-Kline2
d$society <- 1:10 #index observations.

dat_list <- list(
  T = d$total_tools,
  P = d$population,
  society = d$society,
  Dmat  = islandsDistMatrix)
set_ulam_cmdstan(TRUE) 
m14.8 <- ulam(
  alist(
    T ~ dpois(lambda),
    lambda <- (a* P^b/g)*exp(k[society]), #I'm getting lost about what b and g signify
    #I think b is 'coefficient for log population' (text says bp, but we don't have a bp)
    #'g parameters ar the Gaussian process varying intercepts for each society'
    #k is society-specific intercept
    vector[10]:k ~ multi_normal(0,SIGMA),
    matrix[10,10]:SIGMA <- cov_GPL2(Dmat,etasq,rhosq,0.01),
    c(a,b,g) ~ dexp(1), #a b and g are all exponentials
    etasq ~ dexp(2),
    rhosq ~ dexp(.5)
  ), data = dat_list, chains=4, cores=4,iter=2000)

precis(m14.8,depth=3)
```

Says log population coeff bp is much as it was before (what is referred to here? Chapter 11?)

Try model without Gaussian?
```{r 14.8minusGauss?}
m14.8a <- ulam(
  alist(
    T ~ dpois(lambda),
    lambda <- (a* P^b/g),
    a~dexp(1),
    b~dexp(1),
    g~dexp(1)
  ), data = dat_list, chains=4, cores=4,iter=2000)
precis(m14.8)
precis(m14.8a)
plot(coeftab(m14.8a,m14.8),par=c('a','b','g'))

```
This does seem to be what he means: can strip out all the Gaussian bits and you get a similar model (the estimates aren't labelled, but from top to bottom are ac, b, g) - this also shows more variance in estimate of b for Gaussian model and less variance in estimate of g.

Back to the book/lecture:  

Cannot interpret etasq and rhosq because they are strongly correlated in the posterior. You can get similar covariance matrices by making one big and the other small or vice versa - they are negatively correlated

More generally, trying to interpret output of a model by looking at parameter coefficients is folly.

Can plot to show this.

```{r code14.41}
#I reordered code a bit here
post <- extract.samples(m14.8)
#compute posterior mean covariance
x_seq <- seq( from =0,to=10,length.out=100)
pmcov <- sapply(x_seq,function(x) post$etasq*exp(-post$rhosq*x^2))
pmcov_mu <- apply(pmcov,2,mean)


#plot posterior median covariance function
plot(NULL, xlab="distance (thousand km)",ylab='covariance',xlim=c(0,10),ylim=c(0,1.5), main='Posterior')
lines(x_seq,pmcov_mu,lwd=2)

for (i in 1:50)
  curve(post$etasq[i]*exp(-post$rhosq[i]*x^2),add=TRUE,col=col.alpha("black",.3))

```
(matches fig 14.11 b)
Illustrates how the mean function (dark line) has -ve relation between distance and covariance. Lot of uncertainty around this
Also note how covariance declines to close to zero around 4000 km  

M doesn't give code for the corresponding map of priors. Will try to work it out.
Presumably we work from the formula in the model code.  This works!!

```{r trypriors}
nrun<-50
myruns <- data.frame(matrix(NA,nrow=nrun*11,ncol=3))
colnames(myruns)<-c('run','distance','cov')
thisrow<-0
for (n in 1:nrun){
etasq<-rexp(n,2)
rhosq<-rexp(n,.5)
for (myd in 1:11){
  thisrow<-thisrow+1
  thisd<-myd-.9 #actual values go from .1 to 10.1
  Dmat <- matrix(c(0,thisd,thisd,0),nrow=2)
covs <- unlist(cov_GPL2(Dmat,etasq,rhosq,.01))
myruns$run[thisrow]<-n
myruns$distance[thisrow]<-thisd
myruns$cov[thisrow]<-covs[2] #offdiagonal
}
}


  plot(NULL, xlab="distance (thousand km)",ylab='covariance',xlim=c(0,10),ylim=c(0,2), main='Prior')
  for (n in 1:nrun){
    mydat<-myruns[myruns$run==n,]
    lines(mydat$distance,mydat$cov,col=col.alpha("black",.3))
}

```
Not as smooth as fig 14.11, but I used lines rather than curve.  
But this is good - getting more confident, as I can see that for prior you need to take formula from the model and sample values of eta and rho, and for each sample then calculate covariance at different distances. And this then shows how the posterior gives tighter range of estimates - but has same basic pattern.  



M notes that it is hard to interpret covariance matrix because it is on log scale. But can compute implied correlations.
(reminder - dependent variable is tool count)

```{r code14.42}
#compute posterior median covariance
K <- matrix(0,nrow=10,ncol=10)
for (i in 1:10){
  for (j in 1:10){
    K[i,j] <- median(post$etasq)* exp(-median(post$rhosq)*islandsDistMatrix[i,j]^2)
  }
}

diag(K) <- median(post$etasq)+.01  #we now have cov matrix.

#now convert to correlation matrix

Rho <- round(cov2cor(K),2)
colnames(Rho)<-c("Ml","Ti","SC","Ya","Fi","Tr","CJ", "Mn","To","Ha")
rownames(Rho)<-colnames(Rho)
Rho
```
Can make scatterplot of latitude and longitude; size of each point is population size.
Line segments shaded in proportion to size of correlation.

this indicates how far they deviate from expected from population.

```{r code14.44}
#make a pretty map
data(Kline2)
d<-Kline2
#start by scaling point size to logpo
psize <- d$logpop/max(d$logpop)
psize <- exp(psize*1.5)-2

#plot raw data and labels
plot(d$lon2,d$lat,xlab='longitude',ylab='latitude',
     col=rangi2,cex=psize,pch=16,xlim=c(-50,30))
labels<-as.character(d$culture)
text(d$lon2,d$lat,labels=labels,cex=.7,pos=c(2,4,3,3,4,1,3,2,4,2))

#overlay lines shaded by Rho
for (i in 1:10){
  for (j in 1:10){
    if (i<j){
      lines(c(d$lon2[i],d$lon2[j]),c(d$lat[i],d$lat[j]),
            lwd=2,col=col.alpha("black",Rho[i,j]^2))
    }
  }
}
```

Can also plot log population vs total tools; trend is avg prediction from population size; line segments reflect size of correlation between islands.  

```{r code14.45}
#plot for 14.12
#compute posterior median relationships, ignoring distance
logpop.seq <- seq(from=6,to=14,length.out=30)
lambda <- sapply(logpop.seq,function(lp) exp(post$a+post$b*lp)) #corrected to b from bp

lambda.PI80 <- apply(lambda,2,PI,prob=.8)
lambda.median <- apply(lambda,2,median)

#plot raw data and labels
plot(d$logpop,d$total_tools,col=rangi2,cex=psize,pch=16,
     xlab='log population',ylab='total tools',ylim=c(0,200))
text(d$logpop,d$total_tools,labels=labels,cex=.7,
     pos=c(4,3,4,2,2,1,4,4,4,2))

#display posterior predictions
lines(logpop.seq,lambda.median,lty=2,col='red')
lines(logpop.seq,lambda.PI80[1,],lty=2)
lines(logpop.seq,lambda.PI80[2,],lty=2)

#overlay correlations
for (i in 1:10){
  for (j in 1:10){
    if(i<j) {
      lines(c(d$logpop[i],d$logpop[j]),
            c(d$total_tools[i],d$total_tools[j]),
            lwd=2,col=col.alpha("black",Rho[i,j]^2))
    }
      
  }
}


```
Something a bit off with my plot! High CI seems wrong

Something wrong with this line
lambda <- sapply(logpop.seq,function(lp) exp(post$a+post$bp*lp))

my post doesn't have bp or lp!
Need to check this out later, as for now have to go forward...

My question at this point is whether we could learn as much about associations by just plotting N tools vs longitude and latitude, with N tools colour coded, for instance. Like a heatmap. Would that show the patterns as clearly?  


Other examples
Seasonality, phylogenetic distance, social networks, nonparametric splines

## Phylogenetic distance example  
Phylogenetic distance : a clue to covariance  
Get common exposures which can act as backdoor confounds when comparing species.  

Use covariance matrix to represent phylogeny  

Dataset primates 301  
Question: body size, brain size and group size relationship.  

Hypothesis: large brain makes it possible to live in big groups.  

Back door: body size - correlated with brainsize.  
Could also have direct causal influence on group size.  

focus on data where we have complete data on all primates with all 3 measured.  
Used to illustrate Gaussian process regression, with phylogeny as part of model.  
M emphasises this is not an evolutionary model: it is a geocentric model.  
 

Idea that phylogenetic distance may account for correlations between brain size,  body size and size of social groups for a species. 
G is group size
M is body mass
B is brain size

P is phylogenetic distance.

Can make the pretty fig from 14.13 with a few lines of code!

```{r code14.47}
library(rethinking)
set_ulam_cmdstan(TRUE) 
data(Primates301)
data(Primates301_nex)
library(ape)
plot(ladderize(Primates301_nex),type='fan',font=1,no.margin=T,label.offset=1,cex=.5)

```




Easiest to understand if we start if ordinary linear regression.  This can be expressed in a weird way, but you can then get at Gaussian regression by just changing one line.  

Think of group size (outcome) as a vector which is function of log body size and log brain size.  
Group size vector is a function of vector of means, mu, and covariance matrix.  

In regular regression, the covariance matrix has sigma sq along diagonal and zero everywhere else.  No correlation between species.  

First model treats brain size as dependent, with group size and body mass as predictors (?)

demonstrates treating regression using covariances.

I'm pretty confused here as I thought group size was our dependent measure.

```{r code14.48_9}
d<-Primates301
d$name<-as.character(d$name)
dstan<-d[complete.cases(d$group_size,d$body,d$brain),]
spp_obs<-dstan$name

dat_list <- list(
  N_spp = nrow(dstan),
  M = standardize(log(dstan$body)),
   B = standardize(log(dstan$brain)),
   G = standardize(log(dstan$group_size)),
  Imat = diag(nrow(dstan)))

m14.9 <- ulam(
  alist(
    B ~ multi_normal(mu,SIGMA),
    mu <- a + bM*M + bG*G,
    matrix[N_spp,N_spp]:SIGMA <- Imat *sigma_sq,
    a ~ normal(0,1),
    c(bM,bG) ~ normal(0,.5),
    sigma_sq ~ exponential(1)
  ), dat =dat_list,chains=4,cores=4)

precis(m14.9)
```
positive assoc between brain size (dep measure) and both body mass and group size.


For our Gaussian regression, we use instead a covariance matrix that has covariances rather than zeros on off diagonals. 

Phylogeny treated like an exposure (unobserved variable) - but we can measure it.  

Brownian motion model implies covariance declines linearly with phylogenetic distance (not realistic but widely used).  Assumes no selection and traits just wander randomly.  Gaussian normal distribution velocity wandering.  

You invert the phylogenetic distance matrix.  

If this is included in the model, effect of brain size becomes negligible.

```{r code14.50}
library(ape)
tree_trimmed <- keep.tip(Primates301_nex,spp_obs)
Rbm <- corBrownian(phy=tree_trimmed)
V <- vcv(Rbm)
Dmat <- cophenetic(tree_trimmed)
plot(Dmat,V,xlab='phylogenetic distance',ylab='covariance (Brownian model)')
```



Now we insert correlation matrix into our model.
```{r code14.51}
#put species in right oder
dat_list$V <- V[spp_obs,spp_obs]
#convert to a correlation matrix
dat_list$R <- dat_list$V/max(V)
#Brownian motion model

m14.10 <- ulam(
  alist(
    B ~ multi_normal(mu,SIGMA),
    mu <- a + bM*M + bG*G,
    matrix[N_spp,N_spp]:SIGMA <- R *sigma_sq,
    a ~ normal(0,1),
    c(bM,bG) ~ normal(0,.5),
    sigma_sq ~ exponential(1)
  ), dat =dat_list,chains=4,cores=4)

precis(m14.10)
```
So now we see no effect of Group on brain size.  
Closely related species have similar brains and similar body sizes 

Brownian motion model: covariance declines in rigid way with increasing distance. More fancy ways to do this 

Demonstrates that can substitute the Ornstein Uhlenbeck process (OU process). This is damped Brownian motion.
```{r code14.52}
#add scaled and reordered distance matrix
dat_list$Dmat <- Dmat[spp_obs,spp_obs]/max(Dmat)
m14.11 <- ulam(
  alist(
    B ~ multi_normal(mu,SIGMA),
    mu <- a + bM*M + bG*G,
    matrix[N_spp,N_spp]:SIGMA <- cov_GPL1(Dmat,etasq,rhosq,.01),
    a ~ normal(0,1),
    c(bM,bG) ~ normal(0,.5),
    etasq ~ half_normal(1,.25),
    rhosq ~ half_normal(3,.25)
  ), dat =dat_list,chains=4,cores=4)

precis(m14.11)
```
There is an association between group size and brain size again, though it is smaller than with the original model. 
Different because inferred covariance function is different from the brownian one. Can look at it with next chunk of code.

```{r code14.53}
post <- extract.samples(m14.11)
plot(NULL, xlim=c(0,max(dat_list$Dmat)), ylim=c(0,1.5),
     xlab='standardized phylogenetic distance',ylab='covariance')

#posterior
for (i in 1:30){
  curve(post$etasq[i]*exp(-post$rhosq[i]*x),add=TRUE,col=rangi2)
}
#prior mean and 89% interval
eta <- abs(rnorm(1e3,1,.25))
rho <- abs(rnorm(1e3,3,.25))
d_seq <- seq(from =0, to = 1, length.out=50)
  K <- sapply(d_seq,function(x) eta*exp(-rho*x))
  lines(d_seq,colMeans(K),lwd=2)
  shade(apply(K,2,PI),d_seq)
  text(.5,.5,'prior')
  text(.2,.1,'posterior',col=rangi2)


```

Note low covariance between species at any distance.  
This model implies low covariance for brain sizes at all levels - so the model doesn't completely explain away association between group size and brain size, as it did in Brownian motion model. 

## Summary  
We extended the basic multilevel strategy of partial pooling to slopes as well as intercepts.  
This meant modeling covariation in statistical population of parameters. 
LKJcorr prior is convenient family of priors for correlation matrices. 


Can redraw tree with group size at tips - see clustering similar group sizes phlyogenetically related.  

Alternative version: Gaussian process version doesnt assume Brownian motion. Estimates correlation of phlyogenetic distance to group size from dataset 

Can do by changing just one line.  

















