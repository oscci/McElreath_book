---
title: "Chapter 14: Adventures in Covariance"
output: html_notebook
---

Example problem of recording average wait for coffee in a cafe. This time you can get an estimate for morning and afternoon. Morning tends to be longer wait. Difference between morning and afternoon gives a slope.  
Intercept corresponds to average wait in morning and slope to effect of treatment (am or pm).

Formally
   $$\mu_i = \alpha_{CAFE[i]} + \beta_{CAFE[i]} A_j$$
   
Pooling gives more efficient estimates - for both intercepts and slopes. Any batch of parameters with exchangeable index values can and probably should be pooled.  
Exchangeable just means they are arbitrary labels - no true ordering. 

But intercepts and slopes are related. Floor and ceiling effects etc.
More scope for big change if high value to start with.  

At this point we are warned this material is difficult!  

It is possible to use pooling with categories such as age or location - we can model covariation among continuous categories.  

## 14.1 Varying slopes by construction  
To pool information across intercepts and slopes we need to moel the join population of intercepts and slopes - ie. model their covariance.  
Need a joint multivariate Gaussian distribution for all the varying effects - rather than separate distributions, assign a 2D Gaussian distribution to include intercepts and slopes as the 2 dimensions. 

We'll do this by simulating the coffee shop example. 

```{r code14.1}
require(rethinking)
a <- 3.5        #average morning wait time
b <- (-1)       #average difference afternoon wait time
sigma_a <-1     #SD of intercepts
sigma_b <- .5   #SD of slopes
rho <- (-.7)    #correlation between intercepts and slopes

#vector of means of a and b for cafes
Mu <- c(a,b)
#use matrix to build entire cov matrix directly
cov_ab <- sigma_a*sigma_b*rho
Sigma <- matrix(c(sigma_a^2, cov_ab,cov_ab,sigma_b^2),ncol=2)

#alternative approach
sigmas <- c(sigma_a, sigma_b)   #standard deviations
Rho <- matrix(c(1,rho,rho,1),nrow=2) #correlation matrix

#matrix multiplication to get the covariance matrix
Sigma <- diag(sigmas)%*% Rho %*% diag(sigmas)

#now simulate cafes
N_cafes <- 20
library(MASS)
set.seed(5) 
vary_effects <- mvrnorm(N_cafes,Mu,Sigma)
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]
colnames(vary_effects) <- c('a','b')
head(vary_effects)

plot(a_cafe,b_cafe,col=rangi2)
#overlay population distribution
library(ellipse)
for (l in c(.1,.3,.5,.8,.99)){
  lines(ellipse(Sigma, centre=Mu,level=l),col=col.alpha("black",.2))
}
```

SO far we simulated cafes and their average properties.  
We now simulate 10 visits to each cafe, 5 in morning and 5 in afternoon.

```{r code14.10}
set.seed(22)
N_visits <- 10
afternoon <- rep(0:1,N_visits*N_cafes/2)
cafe_id <- rep(1:N_cafes,each=N_visits)
mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- .5 #sd within cafes
wait <-  rnorm(N_visits*N_cafes,mu,sigma)
d<- data.frame(cafe=cafe_id,afternoon=afternoon, wait=wait)
head(d)
```

Clusters in data are the cafes. Each cluster observed under 2 conditions, am and pm.
Can estimate an individual intercepts for each cluster and an individual slope.

Everything is balanced - not required for MLM

14.1.3 Varying slopes model.

As well as the usual priors for average intercept and average slope, with have prior for correlation matrix. For this we use the function rlkjcorr. We will just look at what this does.

```{r code14.11}
N = 100000  #I varied N so I could understand better
eta = 2 #when eta =1 , all correlations equally likely; when it gets bigger peaks more at zero
R <- rlkjcorr(N,K=2,eta=eta)
dens(R[,1,2],xlab='correlation')
#R is a 3D array 10000 x 2 x 2

#If you set N to 10, you have 10 values with random correlations in col 1 and 1 in col 2,
#plus 10 values with the reverse assignment. 
# dens(R[,2,1]) gives same as dens(R[,1,2])
# Seems redundant!
```

Now we are ready to fit the model.
```{r code14.12}
set.seed(867530)
m14.1 <- ulam(
  alist(
    wait ~ normal (mu,sigma),
    mu <- a_cafe[cafe]+b_cafe[cafe]*afternoon,
    c(a_cafe,b_cafe)[cafe] ~ multi_normal(c(a,b),Rho,sigma_cafe),
    a ~ normal(5,2),
    b ~ normal(-1,.5),
    sigma_cafe ~ exponential(1),
     sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),data=d,chains=4,cores=4)

precis(m14.1,depth=3) #with depth 3 you can see the Rho values (matrix parameters)

```

```{r code14.13}
post <- extract.samples(m14.1)
dens(post$Rho[,1,2],xlim=c(-1,1),col='blue',xlab='correlation',main='Correlation between intercepts and slopes') #posterior off-diagonal value of Rho
R <- rlkjcorr(1e4,K=2,eta=2) #prior
dens(R[,1,2],add=TRUE,lty=2)
text(0.1,2,'m14.1\nblue is posterior, dotted black is prior')


```

Posterior r is bunched below zero - model has learned negative correlation from data.

M suggests trying a different prior, so we'll change it to a flat prior

```{r code14.12a}
set.seed(867530)
m14.1a <- ulam(
  alist(
    wait ~ normal (mu,sigma),
    mu <- a_cafe[cafe]+b_cafe[cafe]*afternoon,
    c(a_cafe,b_cafe)[cafe] ~ multi_normal(c(a,b),Rho,sigma_cafe),
    a ~ normal(5,2),
    b ~ normal(-1,.5),
    sigma_cafe ~ exponential(1),
     sigma ~ exponential(1),
    Rho ~ lkj_corr(1)
  ),data=d,chains=4,cores=4)

precis(m14.1a,depth=3) #with depth 3 you can see the Rho values (matrix parameters)
```

```{r plot2priors}
posta <- extract.samples(m14.1a)
dens(posta$Rho[,1,2],xlim=c(-1,1),col='blue',xlab='correlation',main='Correlation between intercepts and slopes') #posterior off-diagonal value of Rho
dens(post$Rho[,1,2],xlim=c(-1,1),col='red',xlab='correlation',main='Correlation between intercepts and slopes',add=TRUE) #posterior off-diagonal value of Rho
R <- rlkjcorr(1e4,K=2,eta=1) #prior
dens(R[,1,2],add=TRUE,lty=2,col='blue')
R <- rlkjcorr(1e4,K=2,eta=2) #prior
dens(R[,1,2],add=TRUE,lty=2,col='red')
text(0.1,2,'m14.1 and 1a\nblue with flat prior, red with eta =2, \npriors as dotted')
```

```{r code14.14}
#compute unpooled estimates directly from data
a1 <- sapply(1:N_cafes,
             function(i) mean(wait[cafe_id==i & afternoon ==0]))
b1 <- sapply(1:N_cafes,
             function(i) mean(wait[cafe_id==i & afternoon ==1]))-a1

# extract posterior means of partially pooled estimates
post <- extract.samples(m14.1)
a2 <- apply(post$a_cafe,2,mean)
b2 <- apply(post$b_cafe,2,mean)

#plot both and connect with lines
plot(a1,b1,xlab='intercept',ylab='slope',
     pch=16,col=rangi2,ylim=c(min(b1)-0.1,max(b1)+.1),
     xlim=c(min(a1)-.01,max(a1)+.01),
     main='Raw unpooled intercepts & slopes (blue) vs \npartially pooled posterior means (unfilled)')
points(a2,b2,pch=1)
for (i in 1:N_cafes) lines(c(a1[i],a2[i]),c(b1[i],b2[i]))

#Now superimpose contours
# compute posterior mean bivariate Gaussian
Mu_est <- c(mean(post$a),mean(post$b))
rho_est <- mean(post$Rho[,1,2])
sa_est <- mean(post$sigma_cafe[,1])
sb_est <- mean(post$sigma_cafe[,2])
cov_ab <-sa_est*sb_est*rho_est
Sigma_est <- matrix(c(sa_est^2,cov_ab,cov_ab,sb_est^2),ncol=2)

#draw contours
library(ellipse)
for (l in c(.1,.3,.5,.8,.99))
  lines(ellipse(Sigma_est,centre=Mu_est,level=l),
        col=col.alpha("black",.2))

```

This plot shows (a) negative correlation between slopes and intercepts and (b) shrinkage, whereby more extreme values become less extreme in the estimates.

Note that the lines connecting observed and estimated are angled, reflecting the negative correlation between slopes and intercepts. Even if intercept is average, its estimated value will change both intercept and slope.

To show same information on the outcome scale, ie waiting times, we have to compute these from the linear model.

```{r code14.16}
wait_morning_1 <- (a1)
wait_afternoon_1 <- (a1+b1)
wait_morning_2 <-(a2)
wait_afternoon_2 <-(a2+b2)  #? what's with the brackets here?

#plot both and connect with lines
plot(wait_morning_1,wait_afternoon_1,xlab='morning wait',
     ylab='afternoon wait',pch=16, col=rangi2,
     ylim=c(min(wait_afternoon_1)-.1,max(wait_afternoon_1)+.1),
     xlim = c(min(wait_morning_1)-.1,max(wait_morning_1)+.1)
     )
points(wait_morning_2,wait_afternoon_2,pch=1)
for (i in 1:N_cafes)
  lines(c(wait_morning_1[i],wait_morning_2[i]),
        c(wait_afternoon_1[i],wait_afternoon_2[i]))
abline(a=0,b=1,lty=2) #dashed line shows when afternoon and morning waits are the same

#simulate data for contour
v <- mvrnorm(1e4,Mu_est,Sigma_est)
v[,2] <-v[,1]+v[,2] #calculate afternoon wait
Sigma_est2 <- cov(v)
Mu_est2 <- Mu_est
Mu_est2[2] <- Mu_est[1]+Mu_est[2]

#draw contours
library(ellipse)
for (l in c(.1,.3,.5,.8,.99))
  lines(ellipse(Sigma_est2,centre=Mu_est2,level=l),
        col=col.alpha("black",.5))


```

Shows that shrinkage on parameter scale produces shrinkage on the outcome scale.
Gray contours imply a population of wait times. 
See positive correlation - ie some cafes generally have longer waits than others. 
But most fall below line, ie shorter waits in afternoon.

