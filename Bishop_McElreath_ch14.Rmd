---
title: "Chapter 14: Adventures in Covariance"
output: html_notebook
---

Example problem of recording average wait for coffee in a cafe. This time you can get an estimate for morning and afternoon. Morning tends to be longer wait. Difference between morning and afternoon gives a slope.  
Intercept corresponds to average wait in morning and slope to effect of treatment (am or pm).

Formally
   $$\mu_i = \alpha_{CAFE[i]} + \beta_{CAFE[i]} A_j$$
   
Pooling gives more efficient estimates - for both intercepts and slopes. Any batch of parameters with exchangeable index values can and probably should be pooled.  
Exchangeable just means they are arbitrary labels - no true ordering. 

But intercepts and slopes are related. Floor and ceiling effects etc.
More scope for big change if high value to start with.  

At this point we are warned this material is difficult!  

It is possible to use pooling with categories such as age or location - we can model covariation among continuous categories.  

## 14.1 Varying slopes by construction  
To pool information across intercepts and slopes we need to moel the join population of intercepts and slopes - ie. model their covariance.  
Need a joint multivariate Gaussian distribution for all the varying effects - rather than separate distributions, assign a 2D Gaussian distribution to include intercepts and slopes as the 2 dimensions. 

We'll do this by simulating the coffee shop example. 

```{r code14.1}
require(rethinking)
a <- 3.5        #average morning wait time
b <- (-1)       #average difference afternoon wait time
sigma_a <-1     #SD of intercepts
sigma_b <- .5   #SD of slopes
rho <- (-.7)    #correlation between intercepts and slopes

#vector of means of a and b for cafes
Mu <- c(a,b)
#use matrix to build entire cov matrix directly
cov_ab <- sigma_a*sigma_b*rho
Sigma <- matrix(c(sigma_a^2, cov_ab,cov_ab,sigma_b^2),ncol=2)

#alternative approach
sigmas <- c(sigma_a, sigma_b)   #standard deviations
Rho <- matrix(c(1,rho,rho,1),nrow=2) #correlation matrix

#matrix multiplication to get the covariance matrix
Sigma <- diag(sigmas)%*% Rho %*% diag(sigmas)

#now simulate cafes
N_cafes <- 20
library(MASS)
set.seed(5) 
vary_effects <- mvrnorm(N_cafes,Mu,Sigma)
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]
colnames(vary_effects) <- c('a','b')
head(vary_effects)

plot(a_cafe,b_cafe,col=rangi2)
#overlay population distribution
library(ellipse)
for (l in c(.1,.3,.5,.8,.99)){
  lines(ellipse(Sigma, centre=Mu,level=l),col=col.alpha("black",.2))
}
```

SO far we simulated cafes and their average properties.  
We now simulate 10 visits to each cafe, 5 in morning and 5 in afternoon.

```{r code14.10}
set.seed(22)
N_visits <- 10
afternoon <- rep(0:1,N_visits*N_cafes/2)
cafe_id <- rep(1:N_cafes,each=N_visits)
mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- .5 #sd within cafes
wait <-  rnorm(N_visits*N_cafes,mu,sigma)
d<- data.frame(cafe=cafe_id,afternoon=afternoon, wait=wait)
head(d)
```

Clusters in data are the cafes. Each cluster observed under 2 conditions, am and pm.
Can estimate an individual intercepts for each cluster and an individual slope.

Everything is balanced - not required for MLM

14.1.3 Varying slopes model.

As well as the usual priors for average intercept and average slope, with have prior for correlation matrix. For this we use the function rlkjcorr. We will just look at what this does.

```{r code14.11}
N = 100000  #I varied N so I could understand better
eta = 2 #when eta =1 , all correlations equally likely; when it gets bigger peaks more at zero
R <- rlkjcorr(N,K=2,eta=eta)
dens(R[,1,2],xlab='correlation')
#R is a 3D array 10000 x 2 x 2

#If you set N to 10, you have 10 values with random correlations in col 1 and 1 in col 2,
#plus 10 values with the reverse assignment. 
# dens(R[,2,1]) gives same as dens(R[,1,2])
# Seems redundant!
```

Now we are ready to fit the model.
```{r code14.12}
set.seed(867530)
m14.1 <- ulam(
  alist(
    wait ~ normal (mu,sigma),
    mu <- a_cafe[cafe]+b_cafe[cafe]*afternoon,
    c(a_cafe,b_cafe)[cafe] ~ multi_normal(c(a,b),Rho,sigma_cafe),
    a ~ normal(5,2),
    b ~ normal(-1,.5),
    sigma_cafe ~ exponential(1),
     sigma ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),data=d,chains=4,cores=4)

precis(m14.1,depth=3) #with depth 3 you can see the Rho values (matrix parameters)

```

```{r code14.13}
post <- extract.samples(m14.1)
dens(post$Rho[,1,2],xlim=c(-1,1),col='blue',xlab='correlation',main='Correlation between intercepts and slopes') #posterior off-diagonal value of Rho
R <- rlkjcorr(1e4,K=2,eta=2) #prior
dens(R[,1,2],add=TRUE,lty=2)
text(0.1,2,'m14.1\nblue is posterior, dotted black is prior')


```

Posterior r is bunched below zero - model has learned negative correlation from data.

M suggests trying a different prior, so we'll change it to a flat prior

```{r code14.12a}
set.seed(867530)
m14.1a <- ulam(
  alist(
    wait ~ normal (mu,sigma),
    mu <- a_cafe[cafe]+b_cafe[cafe]*afternoon,
    c(a_cafe,b_cafe)[cafe] ~ multi_normal(c(a,b),Rho,sigma_cafe),
    a ~ normal(5,2),
    b ~ normal(-1,.5),
    sigma_cafe ~ exponential(1),
     sigma ~ exponential(1),
    Rho ~ lkj_corr(1)
  ),data=d,chains=4,cores=4)

precis(m14.1a,depth=3) #with depth 3 you can see the Rho values (matrix parameters)
```

```{r plot2priors}
posta <- extract.samples(m14.1a)
dens(posta$Rho[,1,2],xlim=c(-1,1),col='blue',xlab='correlation',main='Correlation between intercepts and slopes') #posterior off-diagonal value of Rho
dens(post$Rho[,1,2],xlim=c(-1,1),col='red',xlab='correlation',main='Correlation between intercepts and slopes',add=TRUE) #posterior off-diagonal value of Rho
R <- rlkjcorr(1e4,K=2,eta=1) #prior
dens(R[,1,2],add=TRUE,lty=2,col='blue')
R <- rlkjcorr(1e4,K=2,eta=2) #prior
dens(R[,1,2],add=TRUE,lty=2,col='red')
text(0.1,2,'m14.1 and 1a\nblue with flat prior, red with eta =2, \npriors as dotted')
```

```{r code14.14}
#compute unpooled estimates directly from data
a1 <- sapply(1:N_cafes,
             function(i) mean(wait[cafe_id==i & afternoon ==0]))
b1 <- sapply(1:N_cafes,
             function(i) mean(wait[cafe_id==i & afternoon ==1]))-a1

# extract posterior means of partially pooled estimates
post <- extract.samples(m14.1)
a2 <- apply(post$a_cafe,2,mean)
b2 <- apply(post$b_cafe,2,mean)

#plot both and connect with lines
plot(a1,b1,xlab='intercept',ylab='slope',
     pch=16,col=rangi2,ylim=c(min(b1)-0.1,max(b1)+.1),
     xlim=c(min(a1)-.01,max(a1)+.01),
     main='Raw unpooled intercepts & slopes (blue) vs \npartially pooled posterior means (unfilled)')
points(a2,b2,pch=1)
for (i in 1:N_cafes) lines(c(a1[i],a2[i]),c(b1[i],b2[i]))

#Now superimpose contours
# compute posterior mean bivariate Gaussian
Mu_est <- c(mean(post$a),mean(post$b))
rho_est <- mean(post$Rho[,1,2])
sa_est <- mean(post$sigma_cafe[,1])
sb_est <- mean(post$sigma_cafe[,2])
cov_ab <-sa_est*sb_est*rho_est
Sigma_est <- matrix(c(sa_est^2,cov_ab,cov_ab,sb_est^2),ncol=2)

#draw contours
library(ellipse)
for (l in c(.1,.3,.5,.8,.99))
  lines(ellipse(Sigma_est,centre=Mu_est,level=l),
        col=col.alpha("black",.2))

```

This plot shows (a) negative correlation between slopes and intercepts and (b) shrinkage, whereby more extreme values become less extreme in the estimates.

Note that the lines connecting observed and estimated are angled, reflecting the negative correlation between slopes and intercepts. Even if intercept is average, its estimated value will change both intercept and slope.

To show same information on the outcome scale, ie waiting times, we have to compute these from the linear model.

```{r code14.16}
wait_morning_1 <- (a1)
wait_afternoon_1 <- (a1+b1)
wait_morning_2 <-(a2)
wait_afternoon_2 <-(a2+b2)  #? what's with the brackets here?

#plot both and connect with lines
plot(wait_morning_1,wait_afternoon_1,xlab='morning wait',
     ylab='afternoon wait',pch=16, col=rangi2,
     ylim=c(min(wait_afternoon_1)-.1,max(wait_afternoon_1)+.1),
     xlim = c(min(wait_morning_1)-.1,max(wait_morning_1)+.1)
     )
points(wait_morning_2,wait_afternoon_2,pch=1)
for (i in 1:N_cafes)
  lines(c(wait_morning_1[i],wait_morning_2[i]),
        c(wait_afternoon_1[i],wait_afternoon_2[i]))
abline(a=0,b=1,lty=2) #dashed line shows when afternoon and morning waits are the same

#simulate data for contour
v <- mvrnorm(1e4,Mu_est,Sigma_est)
v[,2] <-v[,1]+v[,2] #calculate afternoon wait
Sigma_est2 <- cov(v)
Mu_est2 <- Mu_est
Mu_est2[2] <- Mu_est[1]+Mu_est[2]

#draw contours
library(ellipse)
for (l in c(.1,.3,.5,.8,.99))
  lines(ellipse(Sigma_est2,centre=Mu_est2,level=l),
        col=col.alpha("black",.5))


```

Shows that shrinkage on parameter scale produces shrinkage on the outcome scale.
Gray contours imply a population of wait times. 
See positive correlation - ie some cafes generally have longer waits than others. 
But most fall below line, ie shorter waits in afternoon.  

## 14.2 Advanced varying slopes
We are back to the chimpanzees....

Uses as another example of non-centred parameterisation: model converges faster and better when reparameterised.  

model is:
$$ L_i \sim Binomial(1,p_i)$$
$$logit(p_i) =  \gamma_{TID[i]} + \alpha_{ACTOR[i],TID[i]} + \beta_{BLOCK[i],TID[i]}$$
The gamma is an avearge log-odds for each treatment,
alpha is effect for each actor in each treatment
and beta is effect for each block in each treatment

Interaction model - allows effect of treatment to vary by actor and block.  

Gives 4 + 7 x 4 + 6 x 4 = 56 parameters.  

two cluster types, actors and blocks.  
Each has a multivariate Gaussian prior; 4 dimensional because 4 treatments.
Priors don't need means because the average treatment effect, gamma, is in linear model.  

```{r code14.18}
library(rethinking)
data(chimpanzees)
d <- chimpanzees
d$block_id <- d$block
d$treatment <- 1L + d$prosoc_left + 2L*d$condition #converts into 4 levels of treatment

#what is L here ?  log? left? - seems not to do anything?

dat <- list(
  L = d$pulled_left,
  tid = d$treatment,
  actor = d$actor,
  block_id = as.integer(d$block_id))

set.seed(123)
m14.2 <- ulam(
  alist(
    L ~ dbinom(1,p),
    logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],
    
    #adaptive priors
    vector[4]:alpha[actor] ~ multi_normal(0,Rho_actor,sigma_actor),
    vector[4]:beta[block_id] ~ multi_normal(0,Rho_block,sigma_block),
    
    #fixed priors
    g[tid] ~ dnorm(0,1),
    sigma_actor ~ dexp(1),
    Rho_actor ~ dlkjcorr(4),
    sigma_block ~ dexp(1),
    Rho_block ~ dlkjcorr(4)
  ),data=dat,chains=4,cores=4)


precis(m14.2,depth=2)
trankplot(m14.2)

```

trankplots look odd and lots of divergent transitions.  
So we now look at alternative reparameterised version. We need matrices of z-scores - there is command compose-noncentered that does this for us.
(I couldn't get code from book to run, but am not sure why. This version has multiply_lower_tri_self_transpose rathan than Chol_to_Corr, but that doesn't explain the previous problem. Probably something mistyped, but I failed completely to find a bug!
This version, copied and pasted from pdf, works).
```{r code14.19}
set.seed(123)
m14.3 <- ulam( 
  alist(
    L ~ binomial(1,p),
    logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],
    # adaptive priors - non-centered
    transpars> matrix[actor,4]:alpha <-
      compose_noncentered( sigma_actor , L_Rho_actor , z_actor ),
    transpars> matrix[block_id,4]:beta <-
      compose_noncentered( sigma_block , L_Rho_block , z_block ),
    matrix[4,actor]:z_actor ~ normal( 0 , 1 ),
    matrix[4,block_id]:z_block ~ normal( 0 , 1 ),
    # fixed priors
    g[tid] ~ normal(0,1),
    vector[4]:sigma_actor ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky( 2),
    vector[4]:sigma_block ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky( 2 ),
    # compute ordinary correlation matrixes from Cholesky factors
    gq> matrix[4,4]:Rho_actor <<- multiply_lower_tri_self_transpose(L_Rho_actor),
    gq> matrix[4,4]:Rho_block <<- multiply_lower_tri_self_transpose(L_Rho_block)
  ) , data=dat , chains=4 , cores=4 , log_lik=TRUE )


```



```{r code14.19}
set.seed(123)
m14.3 <- ulam(
  alist(
    L ~ binomial(1,p),
    logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],
    #adaptive priors are non-centered this time
    
    transpars> matrix[actor,4]:alpha <-
      compose_noncentered(sigma_actor, L_Rho_actor, z_actor),
    transpars> matrix[block_id,4]:beta <-
      compose_noncentered(sigma_block, L_Rho_block, z_block),
    matrix[4,actor]:z_actor ~ normal(0 , 1),
    matrix[4:block_id]:z_block ~ normal(0 , 1),
    g[tid] ~ normal(0,1), #fixed priors
    vector[4]:sigma_actor ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky(2),
    vector[4]:sigma_block ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky(2),
    
    gq> matrix[4,4]:Rho_actor <<- multiply_lower_tri_self_transpose(L_Rho_actor),
    gq> matrix[4,4]:Rho_block <<- multiply_lower_tri_self_transpose(L_Rho_block)
    
    #compute ordinary correlaiton matrices from Cholesky factors
    
  ), data=dat, chains=4, cores= 4, log_lik =TRUE)

precis(m14.3,depth=2)

```

```{r code14.2}
neff_nc <- precis(m14.3,3,pars=c("alpha","beta"))$n_eff
neff_c <- precis(m14.2,3,pars=c("alpha","beta"))$n_eff
plot(neff_c,neff_nc,xlab='centered (default)',ylab='non-centered (cholesky)',lwd=1.5)
abline(a=0,b=1,lty=2)
```

Non-centered model more efficient, with more effective samples per parameter.
Fewer iterations needed. 
Because varying effects are regularised, each varying intercept or slope counts less than one effective parameter.

```{r code14.21}
precis(m14.3,depth=2,pars=c("sigma_actor","sigma_block"))
precis(m14.2,depth=2,pars=c("sigma_actor","sigma_block"))
```

```{r code14.22}
#compute mean for each actor in each treatment

# compute mean for each actor in each treatment 
pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )
# generate posterior predictions using link
datp <- list(
actor=rep(1:7,each=4) ,
tid=rep(1:4,times=7) ,
block_id=rep(5,times=4*7) )
p_post <- link( m14.3 , data=datp )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )
# set up plot
plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )
for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )
xo <- 0.1 # offset distance to stagger raw data and predictions
# raw data
for ( j in (1:7)[-2] ) {
lines( (j-1)*4+c(1,3)-xo , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
lines( (j-1)*4+c(2,4)-xo , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28-xo , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28-xo , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )
yoff <- 0.175
text( 1-xo , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2-xo , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3-xo , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4-xo , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )
# posterior predictions
for ( j in (1:7)[-2] ) {
lines( (j-1)*4+c(1,3)+xo , p_mu[(j-1)*4+c(1,3)] , lwd=2 )
lines( (j-1)*4+c(2,4)+xo , p_mu[(j-1)*4+c(2,4)] , lwd=2 )
}
for ( i in 1:28 ) lines( c(i,i)+xo , p_ci[,i] , lwd=1 )
points( 1:28+xo , p_mu , pch=16 , col="white" , cex=1.3 )
points( 1:28+xo , p_mu , pch=c(1,1,16,16) )
```

Notes that cost of non-centered forms is that they look a lot more confusing, so limit code sharing. Also non-centered is not always better.

## 14.3 Instruments and causal designs

In a DAG, some paths are causal and we want to leave them open. Others are non-causal, e.g. backdoor paths. We want to close them.  

Often you can't close all non-causal paths - there will be unobserved confounds.  

Can sometimes still get at causality.  

Consider impact of education, E, on wages, W.  
May be factors that influence both of these, e.g. how industrious.  

If this is unobserved, we can't control for it.  

In this case, look for an __instrumental variable__ .
Terminology: U is unobserved variable, E is education and W is wages.  
We look for Q, an instrumental variable, satisfying these conditions:  

a. Independent of U
b. Not independent of E
c. Cannot influence W except through E.

c is the exclusion restriction and is often implausible.  

Simple model:
```{r instrumental}
dag <- dagitty( "dag {
    E -> W
    U -> W
    U -> E
    Q -> E
}")
plot(graphLayout(dag))

```

Can't just add E, U and Q into model to predict W; there is noncausal path from Q to W through U
  Q -> E <- U -> W
  We are conditioning on E, and E is a collider of Q and U, so non-causal path is open. This confounds the coefficient on Q - can amplify bias. 
  
Suppose Q is quarter of year one is born in. This is independent of U, but affects E, so is instrumental. 

Simulate data to understand it:

```{r code14.23}
N <- 500
b <- 0 #made it explicit this is a slope that you can set to 0 or other value
U_sim <- rnorm(N)
Q_sim <- sample(1:4,size=N,replace = TRUE)
E_sim <- rnorm(N,U_sim+Q_sim)
W_sim <- rnorm( N , U_sim + b*E_sim )
dat_sim <- list(
  W = standardize(W_sim),
   E = standardize(E_sim),
   Q = standardize(Q_sim))

#I'm just adding correlation matrix
require(corrr)
correlate(cbind(W_sim,U_sim,E_sim,Q_sim))
```
First model just regresses wages on education

```{r code 14.24}
m14.4 <- ulam( 
alist(
W ~ dnorm( mu , sigma ),
mu <- aW + bEW*E,
aW ~ dnorm( 0 , 0.2 ),
bEW ~ dnorm( 0 , 0.5 ),
sigma ~ dexp( 1 )
) , data=dat_sim , chains=4 , cores=4 )
precis( m14.4 )



```

Note that we know that bEW is zero, because we set it to that value in the simulation, but here it looks as if there is a real effect because of the effect of U.

Next show it is even worse if we add Q as a predictor


```{r code 14.25}
m14.5 <- ulam( 
alist(
W ~ dnorm( mu , sigma ),
mu <- aW + bEW*E +bQW*Q,
aW ~ dnorm( 0 , 0.2 ),
bEW ~ dnorm( 0 , 0.5 ),
bQW ~dnorm( 0 , 0.5 ),
sigma ~ dexp( 1 )
) , data=dat_sim , chains=4 , cores=4 )
precis( m14.5 )



```

To get the right model, need a generative version of DAG, which has submodels.
These are given in the book as formulae, but they actually just reflect the lines of code used in the simulation.  
We then estimate covariance between W and E - this is not descriptive covariance btween variables, but a matrix of sigmas, which is taking U into account.
We have a multivariate linear model with multiple simulataenous outcomes all modelled with join error structure.  

In the model this is:

```{r code14.26}
m14.6 <- ulam(
  alist(
    c(W,E) ~ multi_normal(c(muW,muE),Rho,Sigma),
  muW <- aW + bEW*E,
  muE <- aE + bQE*Q,
  c(aW,aE) ~ normal(0,0.2),
  c(bEW,bQE) ~ normal (0,.5),
  Rho ~ lkj_corr(2),
  Sigma ~ exponential(1)
  ), data=dat_sim, chains=4, cores=4)

precis(m14.6,depth=3)

```
Note that bEW is close to zero - this is correct, as it agrees with simulation.
Rho[1,2] is 'residual correlation' between E and W (not raw correlation) - reflects fact that both are influenced by U.


Small digression. I had actually thought that what we'd do would be to make a kind of sequential model that would reflect the order in which variables are created. But would that work? 

- reading on to the Rethinking box, it looks like what I am describing here is '2 stage least squares' which M says is potentially problematic!


Genuinely useful instrumental variables are hard to find.  

Instrumental variables are natural experiments that impersonate randomized experiments.  E.g. quarter of birth is like an external manipulation of education, E. 

Other natural experiments that can be useful.  

## Front door criterion.

Where you have X -> Z -> Y with U also affecting X and Y.  
We are interested in causal influence of X on Y.  
Z is a mediator.  

## Regression discontinuity. 
Trend fit to those just below and above a cutoff. Causal effect is average difference between individuals just above and just below cutoff. Influenced by entire function.

# 14.4 Social relations as correlated varying effects  

```{r code14.30}
library(rethinking)
data(KosterLeckie)

plot(kl_dyads$giftsAB,kl_dyads$giftsBA)
r<-round(cor(kl_dyads$giftsAB,kl_dyads$giftsBA),3)
abline(0,1,lty=2)
text(40,80,paste0('r = ',r))
```

This loads 2 sets of data - we will use kl_dyads.  
Gift exchanges between A and B in each direction.
did is unique dyad ID number
household ID is hidA and hidB

Correlation not a good indicator of relation - depends on who is designated A and B in each dyad.
Need to separate special effects reflecting relation between dyads, and general effects related to other factors such as poverty.  
Social relations model, SRM. 
We model gifts from A toB as a combination of varying effects specific to the household and the dyad. 
Gift counts are poisson variables - counts with no obvious upper bound.

First part of model:

$$y_{A->B} \sim Poisson(\lambda_{AB})$$
$$ log\lambda_{AB} = \alpha + g_A + r_B + d_{AB}$$
Intercept, alpha, represents average gifting rate (on log scale) across all dyads. Other effects offset from this.  
The g term is varying effect parametr for generalised giving tendency of household A, regardless of dyad.  
Effect r is generalised receiving of household B regardless of dyad.  
Effect d is dyad-specific rate that A gives to B.  

Parallel model for B->A

IMplies each household needs a giving and a receiving rate. And each dyad also has an effect for A->B and B->A.  
The g and r terms need to have possibility to be correlated. 
Need 2 multinormal priors - one for population of household effects (g and r) and correlation between them; and one for population of dyad effects.
It's not meaningful which household comes first, so there is a single sigma.

```{r code14.31}
kl_data <- list(
  N = nrow(kl_dyads),
  N_households= max(kl_dyads$hidB),
  did=kl_dyads$did,
  hidA = kl_dyads$hidA,
  hidB = kl_dyads$hidB,
  giftsAB = kl_dyads$giftsAB,
  giftsBA = kl_dyads$giftsBA
)

m14.7 <- ulam(
  alist(
    giftsAB ~ poisson (lambdaAB),
    giftsBA ~ poisson (lambdaBA),
    log(lambdaAB) <- a +gr[hidA,1]+gr[hidB,2]+d[did,1],
     log(lambdaBA) <- a +gr[hidB,1]+gr[hidA,2]+d[did,2],
    a ~ normal(0,1),
    
    #gr matrix of varying effects
    vector[2]:gr[N_households] ~ multi_normal(0,Rho_gr,sigma_gr),
    Rho_gr ~ lkj_corr(4),
    sigma_gr ~ exponential(1),
    
    ## dyad effects
transpars> matrix[N,2]:d <-
compose_noncentered( rep_vector(sigma_d,2) , L_Rho_d , z ),
matrix[2,N]:z ~ normal( 0 , 1 ),
cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky( 8 ),
sigma_d ~ exponential(1),
## compute correlation matrix for dyads
gq> matrix[2,2]:Rho_d <<- Chol_to_Corr( L_Rho_d )
), data=kl_data , chains=4 , cores=4 , iter=2000
)

precis(m14.7,depth=3)
```

now plot output
```{r code14.32_3}
post <- extract.samples( m14.7 )
g <- sapply( 1:25 , function(i) post$a + post$gr[,i,1] )
r <- sapply( 1:25 , function(i) post$a + post$gr[,i,2] )
Eg_mu <- apply( exp(g) , 2 , mean )
Er_mu <- apply( exp(r) , 2 , mean )

plot( NULL , xlim=c(0,8.6) , ylim=c(0,8.6) , xlab="generalized giving" ,
ylab="generalized receiving" , lwd=1.5,main='
Expected giving and receiving, \nomitting dyad-specific effects.')

abline(a=0,b=1,lty=2)
text(3,6,'Each point is a household and the ellipses \nshow 50% compatibility regions. \nThere is a negative relationship between \naverage giving and average
receiving across households')
# ellipses
library(ellipse)
for ( i in 1:25 ) {
Sigma <- cov( cbind( g[,i] , r[,i] ) )
Mu <- c( mean(g[,i]) , mean(r[,i]) )
for ( l in c(0.5) ) {
el <- ellipse( Sigma , centre=Mu , level=l )
lines( exp(el) , col=col.alpha("black",0.5) )
}
}
# household means
points( Eg_mu , Er_mu , pch=21 , bg="white" , lwd=1.5 )

dy1 <- apply( post$d[,,1] , 2 , mean )
dy2 <- apply( post$d[,,2] , 2 , mean )
plot( dy1 , dy2 ,main='Dyad-specific effects, absent generalized
giving and receiving.',xlab='Household A in dyad',ylab='Household B in dyad')
text(-.5,2,'After accounting for overall \nrates of giving and\nreceiving, residual gifts are strongly \ncorrelated within dyads.')

```


