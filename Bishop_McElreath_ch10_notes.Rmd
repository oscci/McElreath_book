---
title: "McElreath Chapter 10"
output: html_notebook
---

```{r loadpackages}
require(rethinking)

```

Need to choose distributions for parameters (priors) and a likelihood function (distribution of data).  
Conventional choices are wide Gaussian priors; Gaussian likelihood in linear regression.
Work well in most situations.

For unconventional models, entropy provides a principle to guide choice of probability distributions - distribution with biggest entropy is best for 3 reasons.
1. big entropy - gives wide and uninformative distribution - so probability is spread as evenly as possible. Pick least informative distribution consistent with partial scientific knowledge about the parameters. 
For likelihoods, big entropy means counting all the ways outcomes could arise, consistent with constraints on outcome variable.  
So I guess he's saying here that we are avoiding bias, except bias from prior knowledge.  

2. High entropy distributions consistent with nature.  Gaussian common in nature: results from addition of underlying processes. We need not know about underlying processes; we just know location and variance. Need to be aware of other procsses - natural processes other than addition.  
3. pragmatic - it tends to work. 

### Generalised linear models
Like linear regression but need not use Gaussian likelihoods.  

Can think of Bayesian updating as entropy maximization.  
Posterior distribution has greatest entropy relative to prior (ie smallest cross-entropy).  
Produces least informative distribution that is still consistent with our assumptions - ie smallest divergence from prior that is possible while remaining consistent with constraints and data.  

Most conservative distribution that obeys its constraints.

## Maximum entropy 
Measure of uncertainty that satisfies 3 criteria.
1. continuous
2. increases with N possible events
3. additive

Uncertainty of a probability distribution p, wit probabilities $$p_i$$ for each possible event i is average log probability.  

Illustrates ith 5 buckets and 10 numbered pebbles. Each pebble equally likely to land in each bucket - may get all 10 in one bucket, or 2 in each bucket etc.
Some arrangements more likely. There is only one way to arrange all pebbles so they are all in bucket 3, but there are 113400 ways to have 2 pebbles in each bucket.

```{r code10.1-2}
p <- list()
p$A <- c(0,0,10,0,0)
p$B <- c(0,1,8,1,0)
p$C <- c(0,2,6,2,0)
p$D <- c(1,2,4,2,1)
p$E <- c(2,2,2,2,2)
#so p is just a list of frequencies for each combination

#Now divide each value of each column by its sum
p_norm <- lapply(p,function(q) q/sum(q)) #p_norm is probability of each outcome in a combination
p_norm
```
For entropy we compute 
$$H(p) = -\Sigma p_i log p_i$$
Where probability is zero, just drop it (l'Hopital rule, p 207).  

```{r code 10.3}
(H <- sapply(p_norm,function(q) -sum(ifelse(q==0,0,q*log(q)))))

```

Shows that distribution E has greatest entropy. A has least.

Shows also that entropy scales with log(N unique arrangements). 
Heh! I worked out how to compute unique arrangements.

```{r uniques}
#Note k is selections from n options; next term in formula has n set to remaining choices, ie n-k from previous term
Bunique <- choose(n=10,k=8)*choose(n=2,k=1)
Cunique <- choose(n=10,k=6)*choose(n=4,k=2)*choose(n=4,k=2)
Dunique <- choose(n=10,k=4)*choose(n=6,k=2)*choose(n=4,k=2)*choose(n=2,k=1)
Eunique <- choose(n=10,k=2)*choose(n=8,k=2)*choose(n=6,k=2)*choose(n=4,k=2)

uniqueseq <- c(1,Bunique,Cunique, Dunique,Eunique)
plot(log(uniqueseq),H,ylab='entropy (H)')
```
"Information entropy is a way of counting how many unique arrangements correspond to a distribution"  (p 303)

Me: I find this useful for explaining p-hacking too! 
i.e. need to think of how many ways could you get a particular result.

"The distribution that can happen the greatest number of ways is the most plausible distribution." - here I found myself querying! What if the truth corresponded to some very specific set of causal factors leading to a result?

I guess the answer is that if you got a result that could be caused in many different ways, how would you know that the specific set was correct?
And that maybe is a reason for specifying priors - if you have in advance a good theoretical reason for a specific set of causes, you build that into your model and then constrain it that way.  

### Gaussian and generalised normal distribution. 
p 305 - seems these are not the same thing!!
Generalised normals are continuous with variance of 1. But can be different in terms of peaks and tails. Definition includes alpha and beta.

I have found gnorm package for plotting generalised normal.  
I just played around with alpha and beta. Gets infinite values if alpha or beta set to zero. Looks normal when both are 2.
p 305 - says entropy maximised when curvature matches Gaussian and shape = 2, but I'm not sure how shape is defined.  
https://rdrr.io/cran/gnorm/man/gnorm.html is helpful.  
Alpha is scale parameter, and beta is shape parameter.
(All this is explained on the next page.....)

CHunk below initially confused, but thanks to Filip Melinscak it now works.

```{r do.gnorm}
require(gnorm)
myseq=seq(-4,4,.1) #make a sequence of x values to plot
alist<-1 #this can be a range, but it's not very crucial I found, so stick to 1
blist<-1:4
keepH <- expand.grid(blist,alist)
colnames(keepH)<-c('b','a')
keepH$H <-NA #add column for entropy for this combination
keepH$sd <- NA #try also to compute SD (I thought differences in this might explain why I was not getting this right)
thisrow<-0
#This bit corrected by Filip: loop over sigma, and compute a from sigma.
sigmalist <- 1  
for (mysigma in sigmalist){
  for (myb in blist){
    mya <- mysigma * sqrt(gamma(1/myb)/gamma(3/myb))
    #https://en.wikipedia.org/wiki/Generalized_normal_distribution
    mydens<-dgnorm(myseq, mu = 0, alpha = mya, beta = myb)
    Hbits <- mydens*log(mydens) #I found it easier to break up entropy calculation this way! Attempt to do in one step created problems. But something is wrong....
    H <- sum(Hbits,na.rm=TRUE)*-1 #entropy
    
    thisrow<-thisrow+1
    keepH$H[thisrow]<-H

    plot(myseq,mydens,main=paste0('alpha =',mya,' beta = ', myb,': Entropy = ',round(H,2)))
    
  }
}


#Attempt at plot from fig 10.2, r hand panel. Peaks at 2
plot(keepH$b[keepH$a==1],keepH$H[keepH$a==1],xlab='shape (b)',ylab='entropy')

```


p 306 - If all we are willing to assume about a collection of measurements if that they have a finite variance, then Gaussian represents most conservative probability distribution.  But may be able to make further assumptions, so need other distributions.

### Binomial distribution
Assumptions:a) 2 unordered events; b) constant expected value
Demonstrates how binomial has maximum entropy. p.308
(Some nice simulations here but I am skipping for now as I need to get on to GLM)  

## Generalized linear models
We've seen that if we use Gaussians in model,we can predict impossible out-of-range values. 
e.g. negative or above a ceiling.  

usual Gaussian is

$$y_i \sim Normal(\mu_i, \sigma)$$
$$\mu_i = \alpha + \beta x_i$$

But with a generalised linear model, we can replace a parameter describing the shape of the likelihood with a linear model - to use probability other than Gaussian, e.g.

$$y_i \sim Binomial(n, p_i)$$
$$f(p_i) = \alpha + \beta(x_i - \overline x)$$
This is appropriate for a count outcome y for which each observation arises from n trials with constant expected value np.  
Binomial has max entropy - so is least informative that satisfies prior knowledge of outcomes y.  

The f at start of 2nd line of model represents a **link function**. This is separate from choice of distribution. 
Link function needed in case where there is not just mu - a parameter describing average outcome - and where there may be bounds on parameters. 
Shape of binomial, like Gaussian, determined by 2 parameters, but neither is the mean. Mean outcome, np , is function of both parameters.  n is usually known, but need to attach linear model to unknown part, p.  
p is a probability mass - must be between zero and 1.
But linear model alpha + beta (x-xbar) can fall below zero or exceed 1. 
This is why link function is needed.  

(This is confusing at this point, but 2 pages on it's explained that a logit link function will constrain outcomes between 0 and 1. the alpha + beta (x-xbar) bit is fed in to a logit function.

## Exponential family of distributions

Exponential constrained to be zero or positive.
$$ y \sim Exponential(\lambda)$$
Has one term, lambda, which is a rate.  
This is core of survival and event history analysis.  
In lecture, discusses in terms of complex systems which fail when any one component fails (e.g. dishwasher). So likelihood of failure increases over time.

Binomial: if you count events from an exponential distribution.  
E.g. fruit flies dying: each obeys exponential function, but if we count over many fruit flies, we get binomial.
Binomial has maximum entropy for binary events.  
Note it is not flat, but that is because it is constrained by maximum value.  

Poisson distribution: like binomial but many trials and low probability - it is a special case of the binomial.  Used for counts that do not get close to theoretical maximum.  

Gamma distribution: time to the event for an exponential (how long did you wait before dishwasher broke) - suppose you want to know how long before 10 fruitflies died. Many natural phenomena have gamma distribution - e.g. age to cancer - maybe because many cellular defence mechanisms.  

Gamma distribution with v large mean converges to a normal distribution.  

Generative processes link all the different distributions.  



My attempt to plot these:
```{r dists}
mynormseq <- seq(-3,3,.01)
plot(dnorm(mynormseq))

plot(dexp(mynormseq[mynormseq>0]),main='Exponential',xlab='time')  #one term, which is a rate
#restrict lambda to positive values

plot(dbinom(1:20,size=30,prob=.3)) #probability of observing 1:20 cases for event where p =.3 in series of 30

plot(dgamma(mynormseq,2)) #2nd term is shape

plot(dpois(1:10,4))

```

## Generalised linear models 
Have to pick an outcome distribution based on what you know about the data before it is collected, e.g. is it count data, is there a ceiling, etc. 
Count variables are integers starting at zero.  Constrains to binomial or poisson, or multinomial or geometric.  

For other types of data may glue together link functions.  

Linear model: the units for the mean are the same as the units for the outcome (e.g. height).
Not true for other GLMs.
A link function maps linear space of model like alpha + beta (xi- xbar) onto non-linear space of  a parameter.  
Logit link maps a paramter defined as a probability mass - constrained between zero and one - this is mapped onto a paramter that can take any real value.  

$$y_i \sim Binomial(n,p_i)$$
$$logit(p_i) = \alpha + \beta x+i$$
Logit function is log odds
$$logit(p_i) = log(p_i/(1-p_i))$$
Odds are probability of an event happening divided by probability it does not happen.

```{r logit}
alpha=1
beta=2
pv <-vector()
xseq <-seq(-1,1,.2)
for (x in xseq){
pi <- exp(alpha+beta*x)/(1+exp(alpha+beta*x))
pv<-c(pv,pi)
}
plot(xseq,pv,type='l')


```
Note how the plot constrains pv to be between 0 and 1.  
Compressed near boundaries.  
No longer the case that a unit change in predictor causes constant change in mean of outcome. - will depend how far from zero the log-odds area.  
Compression.  
With GLM, no regression coefficient such as beta from GLM produces constant change on outcome scale.  

The Log link maps parameter defined over only positive real values onto linear model. Using a log link implies exponential scaling of outcome with predictor.  
Parameter is constrained to be positive.  
Sensitivity analysis can be used to consider how changing assumptions influences inference.  Opposite of p-hacking. You do many analyses but you report them all.  



