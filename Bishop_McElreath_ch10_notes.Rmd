---
title: "McElreath Chapter 10"
output: html_notebook
---

```{r loadpackages}
require(rethinking)

```

Need to choose distributions for parameters (priors) and a likelihood function (distribution of data).  
Conventional choices are wide Gaussian priors; Gaussian likelihood in linear regression.
Work well in most situations.

For unconventional models, entropy provides a principle to guide choice of probability distributions - distribution with biggest entropy is best for 3 reasons.
1. big entropy - gives wide and uninformative distribution - so probability is spread as evenly as possible. Pick least informative distribution consistent with partial scientific knowledge about the parameters. 
For likelihoods, big entropy means counting all the ways outcomes could arise, consistent with constraints on outcome variable.  
So I guess he's saying here that we are avoiding bias, except bias from prior knowledge.  

2. High entropy distributions consistent with nature.  Gaussian common in nature: results from addition of underlying processes. We need not know about underlying processes; we just know location and variance. Need to be aware of other procsses - natural processes other than addition.  
3. pragmatic - it tends to work. 

### Generalised linear models
Like linear regression but need not use Gaussian likelihoods.  

Can think of Bayesian updating as entropy maximization.  
Posterior distribution has greatest entropy relative to prior (ie smallest cross-entropy).  
Produces least informative distribution that is still consistent with our assumptions - ie smallest divergence from prior that is possible while remaining consistent with constraints and data.  

Most conservative distribution that obeys its constraints.

## Maximum entropy 
Measure of uncertainty that satisfies 3 criteria.
1. continuous
2. increases with N possible events
3. additive

Uncertainty of a probability distribution p, wit probabilities $$p_i$$ for each possible event i is average log probability.  

Illustrates ith 5 buckets and 10 numbered pebbles. Each pebble equally likely to land in each bucket - may get all 10 in one bucket, or 2 in each bucket etc.
Some arrangements more likely. There is only one way to arrange all pebbles so they are all in bucket 3, but there are 113400 ways to have 2 pebbles in each bucket.

```{r code10.1-2}
p <- list()
p$A <- c(0,0,10,0,0)
p$B <- c(0,1,8,1,0)
p$C <- c(0,2,6,2,0)
p$D <- c(1,2,4,2,1)
p$E <- c(2,2,2,2,2)
#so p is just a list of frequencies for each combination

#Now divide each value of each column by its sum
p_norm <- lapply(p,function(q) q/sum(q)) #p_norm is probability of each outcome in a combination
p_norm
```
For entropy we compute 
$$H(p) = -\Sigma p_i log p_i$$
Where probability is zero, just drop it (l'Hopital rule, p 207).  

```{r code 10.3}
(H <- sapply(p_norm,function(q) -sum(ifelse(q==0,0,q*log(q)))))

```

Shows that distribution E has greatest entropy. A has least.

Shows also that entropy scales with log(N unique arrangements). 
Heh! I worked out how to compute unique arrangements.

```{r uniques}
#Note k is selections from n options; next term in formula has n set to remaining choices, ie n-k from previous term
Bunique <- choose(n=10,k=8)*choose(n=2,k=1)
Cunique <- choose(n=10,k=6)*choose(n=4,k=2)*choose(n=4,k=2)
Dunique <- choose(n=10,k=4)*choose(n=6,k=2)*choose(n=4,k=2)*choose(n=2,k=1)
Eunique <- choose(n=10,k=2)*choose(n=8,k=2)*choose(n=6,k=2)*choose(n=4,k=2)

uniqueseq <- c(1,Bunique,Cunique, Dunique,Eunique)
plot(log(uniqueseq),H,ylab='entropy (H)')
```
"Information entropy is a way of counting how many unique arrangements correspond to a distribution"  (p 303)

Me: I find this useful for explaining p-hacking too! 
i.e. need to think of how many ways could you get a particular result.

"The distribution that can happen the greatest number of ways is the most plausible distribution." - here I found myself querying! What if the truth corresponded to some very specific set of causal factors leading to a result?

I guess the answer is that if you got a result that could be caused in many different ways, how would you know that the specific set was correct?
And that maybe is a reason for specifying priors - if you have in advance a good theoretical reason for a specific set of causes, you build that into your model and then constrain it that way.  

### Gaussian and generalised normal distribution. 
p 305 - seems these are not the same thing!!
Generalised normals are continuous with variance of 1. But can be different in terms of peaks and tails. Definition includes alpha and beta.

I have found gnorm package for plotting generalised normal.  
I just played around with alpha and beta. Gets infinite values if alpha or beta set to zero. Looks normal when both are 2.
p 305 - says entropy maximised when curvature matches Gaussian and shape = 2, but I'm not sure how shape is defined.  
https://rdrr.io/cran/gnorm/man/gnorm.html is helpful.  
Alpha is scale parameter, and beta is shape parameter.
(All this is explained on the next page.....)

In chunk below I succeed with basic idea, except my entropy is  wrong...

```{r do.gnorm}
require(gnorm)
myseq=seq(-4,4,.1) #make a sequence of x values to plot
alist<-1 #this can be a range, but it's not very crucial I found, so stick to 1
blist<-1:4
keepH <- expand.grid(blist,alist)
colnames(keepH)<-c('b','a')
keepH$H <-NA #add column for entropy for this combination
keepH$sd <- NA #try also to compute SD (I thought differences in this might explain why I was not getting this right)
thisrow<-0
alist=2
for (mya in alist){
  for (myb in blist){
    
    mydens<-dgnorm(myseq, mu = 0, alpha = mya, beta = myb)
    Hbits <- mydens*log(mydens) #I found it easier to break up entropy calculation this way! Attempt to do in one step created problems. But something is wrong....
    H <- sum(Hbits,na.rm=TRUE)*-1 #entropy
    
    thisrow<-thisrow+1
    keepH$H[thisrow]<-H
    keepH$sd[thisrow]<-sd(myseq*mydens*length(myseq))
    plot(myseq,mydens,main=paste0('alpha =',mya,' beta = ', myb,': Entropy = ',round(H,2),', SD = ',round(sd(myseq*mydens*length(myseq)),2)))
    
  }
}
#Attempt at plot from fig 10.2, r hand panel. Doesn't peak at 2....
plot(keepH$b[keepH$a==1],keepH$H[keepH$a==1],xlab='shape (b)',ylab='entropy')

```

Wondered if problem was I do not constrain SD.  
Will need to dig out the original code for these figures from book, but meanwhile pressing on.  

p 306 - If all we are willing to assume about a collection of measurements if that they have a finite variance, then Gaussian represents most conservative probability distribution.  But may be able to make further assumptions, so need other distributions.

### Binomial distribution
Assumptions:a) 2 unordered events; b) constant expected value
Demonstrates how binomial has maximum entropy. p.308
(Some nice simulations here but I am skipping for now as I need to get on to GLM)  

## Generalized linear models
We've seen that if we use Gaussians in model,we can predict impossible out-of-range values. 
e.g. negative or above a ceiling.  

usual Gaussian is

$$y_i \sim Normal(\mu_i, \sigma)$$
$$\mu_i = \alpha + \beta x_i$$

But with a generalised linear model, we can replace a parameter describing the shape of the likelihood with a linear model - to use probability other than Gaussian, e.g.

$$y_i \sim Binomial(n, p_i)$$
$$f(p_i) = \alpha + \beta(x_i - \overline x)$$
This is appropriate for a count outcome y for which each observation arises from n trials with constant expected value np.  
Binomial has max entropy - so is least informative that satisfies prior knowledge of outcomes y.  

The f at start of 2nd line of model represents a **link function**. This is separate from choice of distribution. 
Link function needed in case where there is not just mu - a parameter describing average outcome - and where there may be bounds on parameters. 
Shape of binomial, like Gaussian, determined by 2 parameters, but neither is the mean. Mean outcome, np , is function of both parameters.  n is usually known, but need to attach linear model to unknown part, p.  
p is a probability mass - must be between zero and 1.
But linear model alpha + beta (x-xbar) can fall below zero or exceed 1. 
This is why link function is needed.  

(This is confusing at this point, but 2 pages on it's explained that a logit link function will constrain outcomes between 0 and 1)

## Exponential family of distributions
My attempt to plot these:
```{r dists}
mynormseq <- seq(-3,3,.01)
plot(dnorm(mynormseq))
plot(dgamma(mynormseq,2)) #2nd term is shape
plot(dexp(mynormseq))
plot(dpois(1:10,4))
plot(dbinom(10,size=100,prob=seq(.01,.99,.01)))
```




