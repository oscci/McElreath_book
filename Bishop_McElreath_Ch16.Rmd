---
title: "Rethinking ch 16 Notebook"
output: html_notebook
---



```{r loadpackages}
require(rethinking)
set_ulam_cmdstan(TRUE) 
```

Problems with GLMs-plus-DAGs approach. Not everything can be
modeled as a GLM—a linear combination of variables mapped onto a non-linear outcome.  

We will work directly with Stan model code, since ulam() is not flexible enough for some of the examples.  

Consider relation between height and weight. Correlation but does not give us any information about causation. 

Could model this from what we know about a cylinder. 
Weight of the cylinder is a consequence of the volume of the cylinder.
Volume of the cylinder is a consequence of growth in the height and width of the
cylinder. 
If we can relate the height to the volume, then we’d have a model to predict
weight from height  

Volume of cylinder :
$$V = \pi r^2h$$
r is radius  

Assume radius is constant proportion of height, so r = p.h  
Weight is proportion of volume: W = k.v  

So we can work through the algebra, and get a formula that predicts weight from height, assuming we know p and k.  

$$ W = k\pi p^2h^3$$

Obviously an oversimplification, but can use to make predicitions.  

We can use this formula in a statistical model.  

$$W_i \sim LogNormal(\mu_i, \sigma)    $$

$$exp(\mu_i) = k\pi p^2h^3$$
Then we need to specify priors for k and p. For sigma we can select Exponential(1) as usual.   

Note distribution for the observed outcome variable, weight Wi is positive and continuous, so chose Log-Normal distribution. The Log-Normal distribution is parameterized by the mean
of the logarithm, mu_i. Exponential of mu_i is the *median* of the Log-Normal. 

This is a scientifically informed model so parameters have meanings.  

Parameters k and p are multiplied in the model: we estimate their product. The technical
way this problem could be described is that k and p, given this model and these data, are not identifiable.  

We could just replace the product kp2 with a new parameter theta and estimate
that
$$exp(\mu_i) = \pi\theta h^3_i$$
Still need to think about p and k.

p: the ratio of the radius to the height, p = r/h. 
Must be greater than zero and likely to be much less than .5
Need distribution bounded between zero and one
with most of the prior mass below 0.5. A beta distribution will do:

beta(2,18) This prior will have mean 
2/(2 + 18) = 0:1.

From Wikipedia:  beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable and control the shape of the distribution.  

```{r checkbeta}
alpha=2
beta=18
mylabel<-paste0('alpha = ',alpha,' ;beta = ',beta)
allp <- seq(0,1,.01)
myprop <- vector()
myprop2<-myprop
for (i in allp){
  myprop<-c(myprop,dbeta(i,alpha,beta))
  myprop2<-c(myprop2,pbeta(i,alpha,beta))
}
plot(myprop,type='l',main=paste0('Density\n',mylabel))
plot(myprop2,type='l', main=paste0('Cumulative\n',mylabel))

```

k is the proportion of volume that is weight. 
It really just translates measurement scales, because changing the units of volume or weight will change its value. 
eg can be how many kilograms there are per cubic centimeter. We could look
that up, or maybe use our own bodies to get a prior.  

NB priors are only arbitrary when scientists ignore domain knowledge. Even when we stick with GLMs, prior predictive simulations force us to engage with background knowledge to produce useful, nonarbitrary priors.  

A very useful trick is to instead get rid of the measurement scales altogether  
measurement scales are arbitrary human inventions  
eg We can divide divide both height and weight by their mean values so mean is 1.  

Then on this new scaling,plausible value of k would be k ~ Exponential(0.5)  

```{r exponential}
#Just a reminder of what density function looks like
#random sampling with exponent of .5
myexp<-.5
myprop<-rexp(500,myexp)

plot(myprop,type='p',main=paste0('Density\n',myexp))


```


```{r code16.1_2}
data(Howell1)
d <- Howell1
# scale observed variables
d$w <- d$weight / mean(d$weight)
d$h <- d$height / mean(d$height)

m16.1 <- ulam( 
alist(
w ~ dlnorm( mu , sigma ),
exp(mu) <- 3.141593 * k * p^2 * h^3,
p ~ beta( 2 , 18 ),
k ~ exponential( 0.5 ),
sigma ~ exponential( 1 )
), data=d , chains=4 , cores=4 )
precis(m16.1)
```
```{r output16.1}
pairs(m16.1)

```
So as p goes up, k goes down - makes sense as we are estimating from the product?

```{r code16.3}
h_seq <- seq(from =0, to =max(d$h),length.out=30)
w_sim<-sim(m16.1, data=list(h=h_seq))
mu_mean <- apply(w_sim,2,mean)
w_CI <- apply(w_sim,2,PI)
plot(d$h,d$w,xlim=c(0,max(d$h)), ylim=c(0,max(d$w)),col=rangi2,
     lwd=2, xlab='height (scaled)',ylab='weight (scaled)')
lines (h_seq,mu_mean)
shade(w_CI,h_seq)

```

Note general relationship good: nb we did not estimate exponent on height, we made it cubic by theory. 
Poor fit for smallest heights. 
Could be that p is different for children, and possbly k.
So misfit in the model gives us useful hints.

NB looking at age in the dataset, some are babies (age zero!)

We could try re running everything with children excluded.

```{r adultsonly}
d<-d[d$age>16,]

m16.1a <- ulam( 
alist(
w ~ dlnorm( mu , sigma ),
exp(mu) <- 3.141593 * k * p^2 * h^3,
p ~ beta( 2 , 18 ),
k ~ exponential( 0.5 ),
sigma ~ exponential( 1 )
), data=d , chains=4 , cores=4 )
precis(m16.1a)
h_seq <- seq(from =0, to =max(d$h),length.out=30)
w_sim<-sim(m16.1a, data=list(h=h_seq))
mu_mean <- apply(w_sim,2,mean)
w_CI <- apply(w_sim,2,PI)
plot(d$h,d$w,xlim=c(0,max(d$h)), ylim=c(0,max(d$w)),col=rangi2,
     lwd=2, xlab='height (scaled)',ylab='weight (scaled)')
lines (h_seq,mu_mean)
shade(w_CI,h_seq)


```
With adults only, lose the lower height values, but notice that the cluster of points around the line is also more symmetrical.



If we related log weight to height we get

log wi = log(k) + log(pi) + 2 log(p) + 3 log(hi)

So this is now a linear model.  

The first three terms above comprise the intercept.  
Then the term 3 log(hi) is a predictor variable with a fixed coefficient of 3.  

Here GLM approach estimates parameters which are informed by a proper theory (eg need to take cube of height, hence coefficient of 3). 

So would be interesting to compare what happens with a standard linear regression on the logs.


## 16.2. Hidden minds and observed behavior  
Inverse problem: how to figure out causes from observations. Hard because many different causes can lead to same outcome.  

Experiment in which 629 children aged 4 to 14 saw four other children choose among three different colored boxes.  

In each trial, three demonstrators choose the same color. The fourth demonstrator chose a different color. So in each trial, one of the colors was the majority choice, another was the minority choice, and the final color was unchosen. How do we figure out from this experiment whether children are influenced by the majority?

I already find this problematic: why are children choosing? Need to have context to make sense of what they do. Could lead to pressures to be same or different

```{r 16.4code}

data(Boxes)
precis(Boxes)

```

y is choice: 1 indicates the unchosen color, 2 indicates the majority demonstrated color,
and 3 indicates the minority demonstrated color.

Children could choose at random, or could follow majority.
(Me: or could be influenced by serial position of demo choices).  

```{r code16.6}
set.seed(7)
N <- 30 # number of children
# half are random
# sample from 1,2,3 at random for each
y1 <- sample( 1:3 , size=N/2 , replace=TRUE )
# half follow majority
y2 <- rep( 2 , N/2 )
# combine and shuffle y1 and y2
y <- sample( c(y1,y2) )
# count the 2s
sum(y==2)/N


```

About two-thirds of the choices are for the majority color, but only half the children are
actually following the majority. 

Now goes on to consider possible strategies

(1) Follow the Majority: Copy the majority demonstrated color.
(2) Follow the Minority: Copy the minority demonstrated color.
(3) Maverick: Choose the color that no demonstrator chose.
(4) Random: Choose a color at random, ignoring the demonstrators.
(5) Follow First: Copy the color that was demonstrated first. This was either the majority
color (when majority_first equals 1) or the minority color (when 0).

Statistical models run in reverse of generative models. In the generative
model, we assume strategies and simulate observed behavior. In the statistical model,
we instead assume observed behavior (the data) and simulate strategies (parameters).  

We can’t directly measure each child’s strategy.  
Each strategy has a specific probability of producing each choice. We can use that
fact to compute the probability of each choice, given parameters which specify the probability of each strategy.

Me: is the idea that there is one strategy for all children, or do children have mixture of strategies?  

The unobserved variables are the probabilities that a child uses each of the five strategies. This means five values, but since these must sum to one, we need only four parameters. 
Use simplex : vector of values that must sum to some constant, usually one.  

FOrmula on p 544 has me confused. Why is s 1 to 5 and j 1 to 3?
in model, "theta holds the average probability of each behavior, conditional on p."
Assumes each child has probability ps of using strategy s. (so I assume we model just 5 children?)
J 1 to 3 is presumably the 3 colours that can be picked.

p  ~ Dirichlet([4, 4, 4, 4, 4]) - are there 5 values because 5 children modelled?

Already confused

```{r reminderDirichlet}
#my code from ch 12
require(gtools) #needed for rdirichlet
a1<-4
b1<-4
delta <- rdirichlet(5,alpha=rep(a1,b1)) 
#rdirichlet(n, alpha)
# alpha is vector containing shape parameters. 
# The first number in alpha determines how evenly spaced the probabilities are  - the higher the more equal
# The 2nd number determines how many bars in the plot

barplot(t(delta),xlab='index', main=paste0('Dirichlet simulated runs\nalpha=',a1,' beta=',b1)) #t for transpose

```
I still don't understand the vector of 4s in the book.  

Notes that one obs per child so assume all children the same. But children still represented by probabilities rather than being deterministic.  

Coding this model means explicitly coding the logic of each strategy.  

```{r code16.7}
data(Boxes_model)
cat(Boxes_model)
```
now run model.
NB, this works but gives warnings about not working in future. SUspect need to do via cmdstan.
see : https://mc-stan.org/cmdstanr/articles/cmdstanr.html

```{r code16.8}
# prep data
dat_list <- list(
N = nrow(Boxes),
y = Boxes$y,
majority_first = Boxes$majority_first )
# run the sampler
m16.2 <- stan( model_code=Boxes_model , data=dat_list , chains=3 , cores=3 )
# show marginal posterior for p
p_labels <- c("1 Majority","2 Minority","3 Maverick","4 Random","5 Follow First")
plot( precis(m16.2,2) , labels=p_labels )


```

Recall that 45% of the sample chose the majority color. But the posterior distribution is
consistent with somewhere between 20% and 30% of children following the majority copying
strategy. Conditional on this model, a similar proportion just copied the first color that was demonstrated. This is what hidden state models can do for us—prevent us from confusing
behavior with strategy.  

This model can be extended to allow the probabilities of each strategy to vary by age,
gender, or anything else. In principle, this is easy—you just make ps conditional on the predictor variables.  

Hmm - I'd want to see cases where majority and first did not coincide, to help disambiguate.  But otherwise, this seems overly complicated approach to the problem. 
Could use simple chi squ to reject the 'all random' 'maverick' and the 'minority' hypotheses. (In fact, latter rejected just by eyeballing).

```{r dbchi}
myt1<-table(Boxes$y)
myt1
chisq.test(myt1)
#So clearly not random

myt<-table(Boxes$y,Boxes$majority_first)
myt
chisq.test(myt)
#Choice influenced by which is first.
```
And might then design a new experiment to explicitly compare FollowFirst and Majority.

I'd also be interested in a FollowLast strategy.  
Design of study means that because one demo is much more likely than others, it is highly likely to occur in first or last position.  

I can see that the stan approach may start to be useful if you want to look at covariates, but I don't find the outputs it generates all that illuminating.  

The Boxes model above resembles a broader class of model known as a state space model. These models posit multiple hidden states that produce observations. Typically the states are dynamic, changing over time. When the states are discrete categories, the model may be called a hidden Markov model (HMM). Many time series models are state space models, since the true state of the time series is not observed, only the noisy measures.

## 16.3 
nut cracking example  
Data are individual bouts of nut-opening.
```{r code16.9}
library(rethinking) 
set_ulam_cmdstan(TRUE)
data(Panda_nuts)

```

The variables of immediate interest are the outcome nuts_opened, the duration pf the
bout in seconds, and the individual’s age. The research question is how nut opening skill
develops and which factors contribute to it.  

(Over)Simple model: As the individual ages, it gets stronger and nut opening rate increases. 

Size increases in proportion to the distance remaining to maximum size.  
Mt = Mmax(1 -exp(-kt))
k is a constant  
t is time  

```{r tryplot}
Mmax<-30
t<-1:20
k<-.2 #can play with different values of k
mt<-Mmax*(1-exp(-k*t))
plot(t,mt)

```
Need strength: is proportional to mass
St =  beta*Mt
(if beta is constant, why do we need to model strength?)

Strength helps in at least three ways. First, it lets the animal lift a heavier hammer. Heavier hammers have greater momentum. Second, it lets the animal accelerate the hammer faster than gravity. Third, stronger animals also have longer limbs, which gives them more efficient levers. So it makes sense to assume increasing returns to strength.

(Not clear to me that stronger animals *do* have longer limbs...,but I guess if we are just taking average strength deduced from age this is the case)

Idea of a threshold below which an individual cannot open a single nut in reasonable time.  new parameter alpha expresses the proportionality of strength to nut opening. It translates force into nuts per second.  

Does various simplifications to get to:

$$\lambda = \phi (1-exp(-kt))^\theta$$
If we then estimate n
$$ n_i \sim Poisson(\lambda_i)$$
and need to include d, duration of nut-cracking to get N nuts, so:

$$\lambda_i = d_i \phi (1-exp(-kt_i))^\theta$$
Priors  
Chimpanzee reaches adult mass around 12 years of age. So the prior growth curves need to plateau around 12. We need distributions for k and theta that
accomplish this. And then the prior for ϕ should have a mean around the maximum rate
of nut opening. I am not really an expert on nut opening. But let’s suppose a professional
chimpanzee could open one nut per second—several nuts can be pounded at once
```{r code16.10}
N <- 1e4 
phi <- rlnorm( N , log(1) , 0.1 )
k <- rlnorm( N , log(2), 0.25 )
theta <- rlnorm( N , log(5) , 0.25 )
# relative grow curve
plot( NULL , xlim=c(0,1.5) , ylim=c(0,1) , xaxt="n" , xlab="age" ,
ylab="body mass" )
at <- c(0,0.25,0.5,0.75,1,1.25,1.5)
axis( 1 , at=at , labels=round(at*max(Panda_nuts$age)) )
for ( i in 1:20 ) curve( (1-exp(-k[i]*x)) , add=TRUE , col=grau() , lwd=1.5 )
# implied rate of nut opening curve
plot( NULL , xlim=c(0,1.5) , ylim=c(0,1.2) , xaxt="n" , xlab="age" ,
ylab="nuts per second" )
at <- c(0,0.25,0.5,0.75,1,1.25,1.5)
axis( 1 , at=at , labels=round(at*max(Panda_nuts$age)) )
for ( i in 1:20 ) curve( phi[i]*(1-exp(-k[i]*x))^theta[i] , add=TRUE ,
col=grau() , lwd=1.5 )
```
in ulam

```{r code 16.11}

dat_list <- list( 
n = as.integer( Panda_nuts$nuts_opened ),
age = Panda_nuts$age / max(Panda_nuts$age),
seconds = Panda_nuts$seconds )
m16.4 <- ulam(
alist(
n ~ poisson( lambda ),
lambda <- seconds*phi*(1-exp(-k*age))^theta,
phi ~ lognormal( log(1) , 0.1 ),
k ~ lognormal( log(2) , 0.25 ),
theta ~ lognormal( log(5) , 0.25 )
), data=dat_list , chains=4 )

```

```{r code16.12}
post <- extract.samples(m16.4)
plot( NULL , xlim=c(0,1) , ylim=c(0,1.5) , xlab="age" ,
ylab="nuts per second" , xaxt="n" )
at <- c(0,0.25,0.5,0.75,1,1.25,1.5)
axis( 1 , at=at , labels=round(at*max(Panda_nuts$age)) )
# raw data
pts <- dat_list$n / dat_list$seconds
point_size <- normalize( dat_list$seconds )
points( jitter(dat_list$age) , pts , col=rangi2 , lwd=2 , cex=point_size*3 )
# 30 posterior curves
for ( i in 1:30 ) with( post ,
curve( phi[i]*(1-exp(-k[i]*x))^theta[i] , add=TRUE , col=grau() ) )

```

Some of the model parameters make sense as varying by individual
while others do not. The scaling parameter theta for example is a feature of the physics,
not of an individual. Which parameters are allowed to vary by individual is something to
be decided by scientific knowledge of the parameters.  




