---
title: "ch 13 notes"
output: html_notebook
---

Uses eg from tadpoles in tanks. Dependent variable in N surviving.
Initial count is density. We start with 10, 25 or 35 tadpoles per tank.
Tank is a cluster variable.
There may be differences between tanks.
We need multilevel model, in which we estimate intercept for each tank and variation among tanks.
The logit function gives a unique log-odds for each tank.  
The prior is Normal (0, 1.5).

$$ S_i \sim Binomial(N_i,p_i)$$

$$logit(p_i) = \alpha_{T_i}$$
$$\alpha_j \sim Normal(0,1.5) $$

```{r code13.2}
#make tank cluster variable
library(rethinking)
data(reedfrogs)
d <- reedfrogs
d$tank <- 1:nrow(d)

dat <- list(
  S = d$surv,
  N = d$density, 
  tank = d$tank)
#approximate posterior
m13.1 <- ulam(
  alist(
    S ~ dbinom(N,p),
    logit(p) <- a[tank],
    a[tank] ~ dnorm(0,1.5)
  ), data=dat, chains=4, log_lik=TRUE)

```

To try to understand this, compare with original data.
This confirms that if you take the inverse logit of the a-values, they fall on straight line with proportion who survive in each tank.

```{r plot13.1}
posts<-precis(m13.1,depth=2)
mydf<-as.data.frame(posts$mean)
colnames(mydf)<-'mean.est'
mydf$density<-d$density
densityfac<-as.factor(mydf$density)
mydf$surv <- d$surv
plot(mydf$surv/mydf$density,mydf$mean.est,col=densityfac, pch=16,xlab='proportion survive',ylab='ulam intercept estimate')
mydf$invlogit <- exp(mydf$mean.est)/(1+exp(mydf$mean.est)) #formula for inverse logit!
plot(mydf$surv/mydf$density,mydf$invlogit,col=densityfac, pch=16,xlab='proportion survive',ylab='inverse logit (ulam intercept estimate)')
abline(0,1)
```

For multilevel model, pool information across tanks: the prior for the a parameters is a function of new parameters.  
New parameter bar alpha which is average.
The Gaussian distribution with mean of bar alpha and sd of sigma is prior for each tank's intercept.  


$$ S_i \sim Binomial(N_i,p_i)$$
$$logit(p_i) = \alpha_{T_i}$$



$$\alpha_j \sim Normal(\overline{\alpha}, \sigma)$$
$$\overline{\alpha} \sim Normal(0, 1.5)$$
$$\sigma \sim Exponential(1)$$

$$\alpha = \beta$$

Prior for tank intercepts is function of bar alpha and sigma.
Multilevel because there are parameters for prior for each tank intercept, but that prior itself has priors.
The parameters for parameters are known as hyperparameters.
Can't fit model with quap, becuause it can't see all the levels.

```{r code13.3, message=FALSE}
m13.2 <- ulam(
alist(
S ~ dbinom(N, p),
logit(p) <- a[tank],
a[tank] ~ dnorm(a_bar, sigma),
a_bar ~ dnorm(0,1.5),
sigma ~ dexp(1)
), data=dat,chains=4,log_lik=T)

precis(m13.2,depth=2)
compare(m13.1,m13.2)

```
I want to see how the estimates vary, so I will plot as before, but this time with both models in the plot.

```{r plot13.1_2}
posts<-precis(m13.1,depth=2)
mydf<-as.data.frame(posts$mean)
colnames(mydf)<-'mean.est'
mydf$density<-d$density
densityfac<-as.factor(mydf$density)
mydf$surv <- d$surv
mydf$p.surv <- d$surv/d$density
plot(mydf$p.surv,mydf$mean.est,col=densityfac, pch=16,xlab='proportion survive',ylab='ulam intercept estimate')
mydf$invlogit <- exp(mydf$mean.est)/(1+exp(mydf$mean.est)) #formula for inverse logit!
plot(mydf$p.surv,mydf$invlogit,col=densityfac, pch=16,xlab='proportion survive',ylab='inverse logit (ulam intercept estimate)')
posts2<-precis(m13.2,depth=2)
mydf2<-as.data.frame(posts$mean)
colnames(mydf2)<-'mean.est2'
mydf$mean.est2<-mydf2$mean.est2
plot(mydf$mean.est,mydf$mean.est2) #hmm estimates seem the same.

#presumably the difference is in sigmas?
mydfs1<-as.data.frame(posts$sd)
mydfs2<-as.data.frame(posts2$sd)

#nb: for plotting have to specify range, because in addition to 48 sigma values, there are also rows for a_bar and sigma estimates.
#Weirdly, when I check mydf2$mean.est2 there are only 48 values
#But for mydfs2 there are 50.

plot(mydfs1[1:48,1],mydfs2[1:48,1],col=densityfac, pch=16)
abline(0,1)

```


Black are smallest density and green are largest.  
SD of mean estimate for model2 falls above the line, whereas others are closer to the line - this means estimates of sd are higher in model 2 -and this seems esp true for black, where the proportions are estimated based on smaller N.

```{r comparemodels}
precis(m13.1,depth=2)
summary(m13.1)
precis(m13.2,depth=2)
summary(m13.2)
compare(m13.1,m13.2)

```
I do struggle with this, but here is what book says.  
Only 21 effective parameters in multilevel model (where do we see that? is it pWAIC?).
'There are 28 fewer effective parameters than actual parameters' - but are't there 50 actual parameters rather than 49?  
'The prior assigned to each intercept shrinks them all towards the mean, alpha-bar.'
?
(I need to check the model comparison chapter again).  

In code 13.5, M now plots means for the two models (but differently from how I have done it).

```{r code13.5}
#extract Stan samples
post <-extract.samples(m13.2)
#compute mean intercept for each tank
#also transform to probability with logistic
d$propsurv.est <- logistic(apply(post$a,2,mean))

#display raw proportions surviving in each tank
plot(d$propsurv,ylim=c(0,1),pch=16,xaxt='n',xlab='tank',ylab='proportion survival',col=rangi2)
axis(1, at=c(1,16,32,48), labels=c(1,16,32,48))

#overlay posterior means
points(d$propsurv.est)

#mark posterior mean probability across tanks
abline(h=mean(inv_logit(post$a_bar)),lty=2)
abline(h=median(inv_logit(post$a_bar)),lty=2,col='red') #added by DB


#draw vertial dividers between tank densities
abline(v=16.5,lwd=0.5)
abline(v=32.5,lwd=0.5)
text(8,0,'small tanks')
text(16+8,0,'medium tanks')
text(32+8,0,'large tanks')


```
Filled blue points are raw proportions.  
Black circles are estimated intercepts.
Horiz dashed line is esimtated median survival proportion.
(?? it is computed using mean - however, I added median in the code and it gives the same result -red dashed line)

Note that in every case the multilevel estimate is closer to the dashed line than the raw empirical estimate is.  
This is shrinkage - arises from regularisation - see chapter 7 (I cannot remember chapter 7...)

More shrinkage for the small tanks.  
Also more shrinkage for points that are further from median line.

Shrinkage effects arise because of pooling across clusters to improve estimates.  

Now we sample from posterior to visualise inferred population distribution of survival.

```{r code13.6}
plot(NULL, xlim=c(-3,4),ylim=c(0,0.35),xlab='log-odds survive',ylab='density')
for (i in 1:100){
  curve(dnorm(x,post$a_bar[i],post$sigma[i]),add=TRUE,col=col.alpha("black",0.2))
}
  #sample 8000 imaginary tanks from posterior distribution
  sim_tanks <- rnorm(8000,post$a_bar,post$sigma)
  
  #transform to probability and visualise
  
  dens(inv_logit(sim_tanks),lwd=2,adj=0.1)

```

# Varying effects and underfitting/overfitting trade-off

Major benefit of using varying effect estimates instead of empirical raw estimates is they provide more accurate estimates of individual cluster (tank) intercepts. 
This is because they do a better job of trading off underfitting and overfitting.  

Suppose we had frogs in natural ponds. 
To predict survival we could 
1) Assume common intercept for all ponds
2) Assume each pond is different (model with amnesia)
3) Parital pooling - use adaptive regularising prior.  

If we adopt common intercept, estimate is likely to be quite precise, but is unlikely to exactly match the mean of any particular pond. Total sample mean underfits the data. 

If we adopt (2) and each pond has its own intercept, then there is little data for each estimate, so estimates are imprecise.  Especially true of smaller ponds, where less data in the estimate.  Error of estimates is high and there is overfitting. 
In effect, assumes variation among ponds is infinite.

Partial pooling is intermediate. WIll be better especially in cases where there's little data for some cases.  

Now we do a simulation.

We'll use same basic multilevel binomial model as before but with ponds instead of tanks.
```{r code13.7-11}
a_bar <- 1.4 #text says 1.4 but e.g. gave 1.5!
sigma <- 1.5
nponds <- 60
Ni <- as.integer(rep(c(5,10,25,35),each=15))
#Ni <- rep(25,60) #added by me to see what happens if all uniform
set.seed(5005)
a_pond <- rnorm(nponds,mean=a_bar, sd=sigma) #normally distributed log odds values
dsim <- data.frame(pond=1:nponds,Ni=Ni,true_a=a_pond)

#simulate survivors
dsim$Si <- rbinom(nponds, prob=logistic(dsim$true_a),size=dsim$Ni)
dsim$p_nopool <- dsim$Si/dsim$Ni

```

The p_nopool values are empirical probabilities of survival. These correspond to estimate we'd get if we used a model with a dummy variable for each pond and flat priors so no regularisation.  

Now we use ulam to compute partial-pooling estimates.

```{r code13.13}
dat <- list(Si=dsim$Si,Ni=dsim$Ni,pond=dsim$pond)
m13.3 <- ulam(
  alist(
    Si ~ dbinom(Ni,p),
    logit(p) <- a_pond[pond],
    a_pond[pond] ~ dnorm(0,1.5),
    sigma ~ dexp(1)
  ), data=dat,chains=4
 
)

precis(m13.3,depth=2)

```
Now we add to the data frame the estimates with partial pooling

```{r code13.15_18}
post <- extract.samples(m13.3)
dsim$p_partpool <- apply(inv_logit(post$a_pond),2,mean)
dsim$p_true <- inv_logit(dsim$true_a)
nopool_error <- abs(dsim$p_nopool-dsim$p_true)
partpool_error <- abs(dsim$p_partpool - dsim$p_true)

plot(1:60,nopool_error,xlab='pond',ylab='absolute error',
     col=rangi2,pch=16)
points(1:60,partpool_error)

plot(nopool_error,partpool_error,col=as.factor(dsim$Ni),pch=16)
abline(0,1)
abline(h=mean(partpool_error),lty=2)
abline(v=mean(nopool_error),lty=2)
```
I've done my own plot which I find clearer for comparing the two methods.  
Colour coding is black = tiny, red=small, green = med, blue = large.


The mean error (dotted line) is not all that different in my simulation, but it is a bit larger for the no_pool error, which is the point M wants us to understand from this simulation.

Also far less error with larger samples. Though M notes here we have no confounding; with confounding, larger samples can make things worse. 

I'm curious that M hasn't included a comparison with complete pooling! So I will try plotting that. Then you just assume overall mean for all ponds, regardless of size.

```{r completepool}
dsim$avgpond <- mean(dsim$p_true)

allpool_error <- abs(dsim$p_true-dsim$avgpond)

```


```{r code13.19}
nopool_avg <- aggregate(nopool_error, list(dsim$Ni),mean)
partpool_avg <- aggregate(partpool_error, list(dsim$Ni),mean)
allpool_avg <- aggregate(allpool_error,list(dsim$Ni),mean)

mycompare <- cbind(allpool_avg,nopool_avg[,2],partpool_avg[,2])
colnames(mycompare)<- c('size','all.pool','no.pool','part.pool')
mycompare
```

So in this case, where there is a lot of variation in pool size, then we get differences.

But what if you have a lot of similar pools? I tried rerunning with all pools set to size = 5  or all set to 25. NB - M notes you can try different samples without needing to rerun the model.
You still get more error with the allpool case.

## 13.3 More than one type of cluster

In chimpanzee study from earlier chapter, data are lever-pulls by chimps. Each pull belongs to a particular chimpanzee. But each chimp does 6 experimental blocks. Could have unique intercepts for each chim as well as each block.  
With MLL can look at both kinds of clusters simulataneously.  
Where actors are not nested in unique blocks, we have a cross-classified model. Software treats as same as hierarchical design.
 
Model from chapter 11 (m11.4) was
m11.4 <- ulam(
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + b[treatment] ,
    a[actor] ~ dnorm( 0 , 1.5 ),
    b[treatment] ~ dnorm( 0 , 0.5 )
  ) ,
  data=dat_list , chains=4,log_lik=T )
  
  Now we add intercepts for blocks as well
  
```{r code13.21}
library(rethinking)
data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition #codes as 1-4

dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  block_id = d$block,
  treatment = as.integer(d$treatment)
)
set.seed(13)
m13.4 <- ulam(
    alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + g[block_id]+b[treatment] ,
    b[treatment] ~ dnorm( 0 , 0.5 ),
    #adaptive priors
    a[actor]~ dnorm(a_bar,sigma_a),
    g[block_id] ~ dnorm(0,sigma_g),
    ## hyperpriors
    a_bar ~ dnorm(0,1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)
    ), data =dat_list, chains=4, cores=4,log_lik=TRUE)

  ) ,
  data=dat_list , chains=4,log_lik=T )

  
```
This one ran faster than I expected!  
Warning about DIVERGENT TRANSITIONS.  
Will explain more about this later (also covered in lecture).  

Now we will explore the posterior.  Precis shows all the estimated parameters, based on 2000 samples from 4 independent chains.
```{r code 13.22}
precis(m13.4, depth=2)


```

Note that number of effective samples varies a lot between parametrs. Sigma_g is low - it spends a lot of time near a boundary (minimum of zero). Rhat values above 1.00 indicate inefficient sampling.  

Compare sigma_a to sigma_g - sigma_a is much bigger; indicates estimated variation among actors is a lot larger than estimated variation among blocks.  
Can see this also in plot. Note, the b values are priors for blocks, the a-values are adaptive priors for actors, which depend on a_bar and sigma_a, and the g-values are adaptive priors for blocks, which depend on sigma_g.  
a_bar, sigma_a and sigma_g are hyperpriors.  

```{r plotm13.4}
plot(precis(m13.4,depth=2))
```

We can compare this model with a model that ignores block.  
```{r code13.23}

set.seed(14)
m13.5 <- ulam(
    alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] +b[treatment] ,
    b[treatment] ~ dnorm( 0 , 0.5 ),
    #adaptive priors
    a[actor]~ dnorm(a_bar,sigma_a),
    ## hyperpriors
    a_bar ~ dnorm(0,1.5),
    sigma_a ~ dexp(1)
    ), data =dat_list, chains=4, cores=4,log_lik=TRUE)

compare(m13.4,m13.5)
#pWAIC is effective N parameters
  
```

Note that the addition of block makes very little difference to WAIC. Sigma_g is very low - mean of each of the g parameters are strongly shrunk towards zero.  

I will now try to create figure 13.4. Had to google to remember how to extract posterior samples, but here it is.

```{r tryfig13.4b}

mye <- extract.samples(m13.4)
dens(mye$sigma_g,xlim=c(0,4),main='Model 13.4 estimates of hyperprior sigmas\nblack is block, red is actor')
dens(mye$sigma_a, add=TRUE,col='red')
#Yes!!
```

Note on model selection. M cautions against selecting m13.4 as best model - better to show both models to understand underlying processes - including the fact that block adds v little to the model.  

Can we also use partial pooling on the treatment effects. This would mean treating as random effects rather than fixed effects.  
The fact that the treatment was determined by the experimenter does not make it a fixed effect. The key question is whether you could reassign the index values without changing meaning of the model - ie. units are exchangeable.

Shows the model as m13.6, but notes that adding treatment doesn't change much. 

```{r code13.25}

set.seed(15)
m13.6 <- ulam(
    alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] +g[block_id] + b[treatment] ,

    #adaptive priors
    b[treatment] ~ dnorm( 0 , sigma_b ),
    a[actor]~ dnorm(a_bar,sigma_a),
    g[block_id] ~ dnorm(0,sigma_g),
    ## hyperpriors
    a_bar ~ dnorm(0,1.5),
    sigma_a ~ dexp(1),
    sigma_b ~ dexp(1),
    sigma_g ~ dexp(1)
    ), data =dat_list, chains=4, cores=4,log_lik=TRUE)
#compare coefficients of models
coeftab(m13.4,m13.6)

```

coeftab is v useful for comparing coefficients. Can see v little impact of including treatment as a random effect. 

## Divergent transitions
Hamiltonian Monte Carlo simulates frictionless flow of  aparticle on a surface. Each transition is a flick of the particle, and total energy at start should be equal to energy at end. In a purely mathematical system this is true. 
In numerical system not always true - energy can be divergent. Esp when posterior distribution is v steep in some region of parameter space. 
Divergent transitions are rejected. But they indicate a region that is hard to explore correctly. Chain is inefficient - common with multilevel models. 
2 ways to deal with it. Can use adapt_delta in Stan - tunes the simulation. 
But often better to reparameterise the model - specify in a different form that is mathematically identical but numerically different. 

Reparameterisation illustrated with v simple model.

```{r code13.26}
m13.7 <-ulam(
  alist(
    v ~ normal(0,3),
    x ~ normal(0,exp(v))
  ), data=list(N=1),chains=4)

precis(m13.7)

traceplot(m13.7)

```
To visualise problem, we can extract sample

```{r lookatm13.7}
e<-extract.samples(m13.7)
plot(e$x[1:200],e$v[1:200])

```

Can see that at low values of v, x contracts around zero.

I just played with this and found the problem is completely fixed if you change v to make the range much smaller


```{r code13.26}
m13.7a <-ulam(
  alist(
    v ~ normal(0,.1),
    x ~ normal(0,exp(v))
  ), data=list(N=1),chains=4)

precis(m13.7a)

traceplot(m13.7a)
e<-extract.samples(m13.7a)
plot(e$x[1:200],e$v[1:200])

```

But that is not what we are doing here! Rather we are learning to reparameterise the model by specifying it in a different way.

```{r code13.27}
m13.7nc <- ulam(
  alist(
    v ~ normal(0,3),
    z ~ normal(0,1),
    gq > real[1]:x< <-z*exp(v)
    #I have *no idea* how that code in line above
#though I think the middle bit  between > and < is somehow defined as a range. 
#I assume this syntax applies the formula z*exp(v) to the range.
  ), data = list(N=1),chains=4)

precis(m13.7nc)
traceplot(m13.7nc)
e<-extract.samples(m13.7nc)[1:200]
plot(e$x,e$v,main='x and v have same \ndistribution as before')
plot(e$z,e$v,main='This is what m13.7nc is sampling')
plot(e$z*exp(e$v),e$x,main='Recomputing x from z and v')
```


Here's more explanation from p 421-422 about the logic of this. 
In original model ,as v changes, x changes dramatically. This is a centred parameterization - this just means the distribution of x is conditional on one or more other parameters. 

Alternative is non-centred parameterisation, accomplished by moving v out of the definition of x.

  $$v \sim Normal(0,3)$$
  $$z \sim Normal(0,1)$$
  x = z exp(v)
  
  z is defined as standardized x, ie mean zero and sd of 1. To compute x, we reverse the standardization by multiplying z by the standard deviation, exp(v)
. Don't need to add mean, as it is zero. 
x is same joint distribution of v and x, but we are not sampling x directly. 


## Back to chimps

Notes that you can run ulam with adapt_delta set to a higher value. Will be slower but will have smaller step size in warmup phase. Default step is .95.
Does reduce N divergent transitions.

```{r code13.28}
set.seed(13)
m13.4b <- ulam(m13.4,chains=4,cores=4,control=list(adapt_delta=.99))
divergent(m13.4b)

```

Helps but far from perfect. Still inefficient.

Non-centred version of model will be better.
Need to get the parameters out of the adaptive priors and into the linear model. 

```{r code13.29}
set.seed(13)
m13.4nc <- ulam(
    alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- z[actor]*sigma_a +  #actor intercepts
      x[block_id]*sigma_g + #block intercepts (x is also a zscore really)
    +b[treatment] ,
    b[treatment] ~ dnorm( 0 , 0.5 ),
 
    z[actor]~ dnorm(0,1),
    x[block_id] ~ dnorm(0,1),

    a_bar ~ dnorm(0,1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1),
    gq> vector[actor]:a <<- a_bar+z*sigma_a,
        gq> vector[block_id]:g <<-x*sigma_g
    ), data =dat_list, chains=4, cores=4,log_lik=TRUE)

precis(m13.4nc)
traceplot(m13.4nc)
coeftab(m13.4,m13.4nc)

```

Comparing parameters helps understand the model; most estimates are similar, but a_bar very different = presumably because now it is recomputed with addition of z values

M also shows plot which shows how n_eff samples are larger for the new nc model, which is better.  

When to use non-centred? Typically performs better for cluster with low variation, like blocks in m 13.4.  

## Multilevel posterior predictions  
AIC and WAIC provide simple estimates of out-of-sample model accuracy. (KL divergence).

```{r code13.31}
#posterior predictions for chimp 2
chimp <- 2
d_pred <- list(
actor=rep(chimp,4),
treatment=1:4,
block_id=rep(1,4) #just looking at block 1
)
p <- link (m13.4, data=d_pred)
p_mu <- apply(p,2,mean)
p_ci <- apply(p,2,PI)
p_mu
p_ci
#So these are mean predictions for 4 treatments for chimp 2 in block 1

#He goes on to explain how you can extract information if you extract.samples and then wrangle the output from that. 
```

## Posterior predictions for new clusters
Can simulate new chimps using a_bar and sigma_a, because they define the population, so we can simulate new cases from it.  

This gets hairy. 

```{r code13.36}
#First we make a link function. This ignores block - gives estimates for treatments by adding treatment effect to a_bar, ie the mean

p_link_abar <- function(treatment){
  logodds <- with(post,a_bar+b[,treatment]) #with allows you to just specify the dataframe once
  return(inv_logit(logodds))
}
```


Now we call the function

```{r code13.37}
post <- extract.samples(m13.4)
p_raw <- sapply(1:4,function(i) p_link_abar(i))
p_mu <- apply(p_raw,2,mean)
p_ci <- apply(p_raw,2,PI)

plot(NULL,xlab='treatment',ylab='proportion pulled left',
     ylim=c(0,1),xaxt='n',xlim=c(1,4),main='average actor')
axis(1,at=1:4,labels=c('R/N','L/N','R/P','L/P'))
lines(1:4,p_mu)
shade(p_ci,1:4)

```


Now we'll simulate some new chimpanzees using a_bar and sigma_a.

```{r code13.38}
a_sim <- with(post,rnorm(length(post$a_bar),a_bar,sigma_a))
p_link_asim <- function(treatment){
  logodds <- with(post,a_sim+b[,treatment])
  return(inv_logit(logodds))
}
p_raw_asim <- sapply(1:4,function(i) p_link_asim(i))
p_mu_raw <- apply(p_raw_asim,2,mean)
p_ci_raw <- apply(p_raw_asim,2,PI)

plot(NULL,xlab='treatment',ylab='proportion pulled left',
     ylim=c(0,1),xaxt='n',xlim=c(1,4),main='random simulated actor\n(marginal of actor)')
axis(1,at=1:4,labels=c('R/N','L/N','R/P','L/P'))
lines(1:4,p_mu_raw)
shade(p_ci_raw,1:4)


plot(NULL,xlab='treatment',ylab='proportion pulled left',
     ylim=c(0,1),xaxt='n',xlim=c(1,4),main='individual simulated actors')
axis(1,at=1:4,labels=c('R/N','L/N','R/P','L/P'))
for(i in 1:100) lines(1:4,p_raw_asim[i,],col=grau(0.25),lwd=2)
```


Note also floor and ceiling effects. 

## Post stratification
M rather runs out of steam here, but clearly thinks we need to be told about this.  
It's too much at this point, but the general idea is that you can fit a model using weights that are appropriate for a stratified section of a population.  But you really need estimates from whole population to generate weights - not just the current sample. 

But problems with selection bias.


