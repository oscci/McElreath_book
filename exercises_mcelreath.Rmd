---
title: "McElreath exercises"
author: "DVM Bishop"
date: "15/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercises/code chunks from chapter 2

```{r grid}
# define grid
p_grid <- seq( from=0 , to=1 , length.out=20)
# define prior
#prior <- rep( 1 , 20 )
prior <- ifelse( p_grid < 0.5 , 0 , 1 )  #this was defined in ch 2 as alternative to uniform prior - means that values < .5 have prior of zero
# compute likelihood at each value in grid
likelihood <- dbinom( 5 , size=7 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

plot( p_grid , posterior , type="b" ,
      xlab="probability of water" , ylab="posterior probability" )
mtext( "20 points" )

```

```{r quadratics}
#quadratics
require(rethinking)

globe.qa <- quap(
  alist(
    W ~ dbinom( W+L ,p) , # binomial likelihood
    p ~ dunif(0,1) # uniform prior
  ) ,
  data=list(W=6,L=3) )
# display summary of quadratic approximation
precis( globe.qa )

# mean   sd 5.5% 94.5%
#p 0.67 0.16 0.42  0.92

```
Trying to make sense of this.
Mean is approx probability in previous plot that gives max value.
In next chunk I try to check out the CIs

```{r quadcompare}
w<-which(posterior==max(posterior))
p_grid[w]
#sum of posterior = 1, so to find CI need to find when sum is .055 or .945
cilo <-0
cihi <-0
myci <-.055
i=1
while(cilo==0){
  i<-i+1
  thissum<-sum(posterior[1:i])
  if(thissum > myci){
  cilo<- p_grid[i]
   }
}
i=length(posterior)
while(cihi==0){
  i<-i-1
  thissum<-sum(posterior[i:length(posterior)])
  if(thissum > myci){
    cihi<- p_grid[i]
  }
}
print(cilo)
print(cihi)
```

The lower CI is good match to quadratic estimate - upper CI is lower - may be to do with grain?
I checked by increasing N points to 100, but that does not help it. Maybe because quadratic is symmetric and likelihood function is not?
Some discussion of this in the book - reassuring that upper end of curve tends to fit less well in examples he gives.

```{r betacalc}
# analytical calculation 2.7
W <- 6
L <- 3
curve( dbeta( x , W+1 , L+1 ) , from=0 , to=1 )
# quadratic approximation
curve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE )

```


```{r montecarlo}
n_samples <- 1000 
p <- rep( NA , n_samples ) #initialise p vector
p[1] <- 0.5 #start value for p at .5
W <- 6
L <- 3
for ( i in 2:n_samples ) {
  p_new <- rnorm( 1 , p[i-1] , 0.1 ) #random p value based on previous p value with small sd
  if ( p_new < 0 ) p_new <- abs( p_new ) #avoid negative numbers
  if ( p_new > 1 ) p_new <- 2 - p_new  #avoid numbers > 1
  q0 <- dbinom( W , W+L , p[i-1] ) #binomial probability for previous p
  q1 <- dbinom( W , W+L , p_new ) #binomial probability for this p
  p[i] <- ifelse( runif(1) < q1/q0 , p_new , p[i-1] ) #depends on random prob relative to likelihood of both q values
#either stick with prior p or take the new one
  }
plot(p[1:500],type='l') #DB added to show homing process - values cluster around true p
#but values are not converging! sd as great for last 100 as first 100
dens( p , xlim=c(0,1) )
curve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )



```


2M3. Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” (Pr(Earth|land)), is 0.23.

Earth prior = .5
Mars prior = .5
Land|Earth = .3
Land|Mars = 1
What is p(Earth|Land)

pLE <- .3 #Land|Earth = .3
pE <- .5 #Earth prior = .5
pL <- .65 = .5 * (Land|earth + Land|mars)

pEL <- pLE*pE/pL
= 0.2307692

 2M4. Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don’t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side facing up on the table).

pA <- 0
pB <- .5
pC <- 1

Cannot be card A
 so p of card C is 1/1.5 = 2/3

2M5. Now suppose there are four cards: B/B, B/W, W/W, and another B/B. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.

Add card D which is B/B

pA<-0
pB<-.5
pC<-1
pD<-1

So need pC or D, which is pC = 1/2.5, pD = 1/2.5 : since they are mutually exclusive is 2/2.5
Or could compute as not card B: Card B is .5/2.5, so this seems to work. 
So answer is .8

2M6. Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume there are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that
the probability the other side is black is now 0.5. Use the counting method, as before.

Probability of black on single draw

A = 1
B = .5
C = 0

differential likelihood of selecting card means in effect we have

A = 1
B1 = .5
B2 = .5
C1 = 0
C2 = 0
C3 = 0

probability that other side is black is probability of A relative to A+B1+B2 = .5

2M7. Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possible first card.

3 options for 2 draws
We are looking for probability of A+B or A+C
These are all the pairings that could give an initial black and 2nd white
A+ B  
B+ C
A +C

Hm - looks like I have probability of 2/3 rather than .75
So something wrong - need to take into account prob of black?

A + B  black + white is 1 then .5
B + C  black then white is .5 then 1
A + C  black then white is 1 then 1

Ah hah - so if we multiply probls we have .5, .5, 1 for the 3 combinations

So total probs are 2, and the options with A come to 1.5, so 1.5/2 .
Yes!

2H1. Suppose there are two species of panda bear. Both are equally common in the wild and live in the same places. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field
research.
Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins?

prob (twin|A) = .1
prob (twin|B) = .2

prob (A|twin) = p (twin|A)* p(A) / p (twin) = (.1 * .55)/.15 = .36
prob (B|twin) = (.2 * .5)/.15 = .66 

so probability of 2nd cub being twin is .36 * .1 + .66*.2 = .168

2H2. Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins.

Presumably this is .36/(.36 + .66) = .352

Seems right range - i.e p goes down from prior of .5.

2H3. Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A.

Prior probability of species A is now .352 rather than .5.
So same Bayes formula but now:

prob (A|single) = p (single|A)* p(A) / p (single) = .9 * .352/.85 = .372

seems in right ballpark, i.e. p goes up a bit


2H4. A common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of the data, even if the data are of different types.
So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information
you have about the test:
 The probability it correctly identifies a species A panda is 0.8.
 The probability it correctly identifies a species B panda is 0.65.
The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well

p(testpos|A) = .8
p(testpos|B) = .65
(presumably test is not just between A and B?)

p(A|testpos) = p(testpos|A * p(A)/p(testpos))

This is a bit confusing as we haven't been told about sens and spec of the test, and don't know if same test for all pandas? But if we assume p(testpos) is average of .8 and .65, then  we get

p(A|testpos) = .8 * .5 / .725 = .55

Also unclear what is meant by 'the birth data' as this varies from problem to problem, but if we take the most recent problem where p(A) is .372


p(A|testpos) = .8 * .372 / .725 = .41






# Homework from website
https://github.com/rmcelreath/statrethinking_winter2019/blob/master/homework/week01.pdf

# Homework week 1
```{r globes}
#1. Globe tossing, 8 in 15

p_grid <- seq( from=0 , to=1 , length.out=20)
# define prior
prior <- rep( 1 , 20 )
# compute likelihood at each value in grid
likelihood <- dbinom( 8 , size=15 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

p1<-plot( p_grid , posterior , type="b" ,
      xlab="probability of water" , ylab="posterior probability",ylim=c(0,.30) )
mtext( "20 points" )
abline(v=.7)
abline(h=posterior[14],lty=2)

#2 Globe tossing with biased prior
p_grid <- seq( from=0 , to=1 , length.out=20)
# define prior
prior <- ifelse( p_grid < 0.5 , 0 , 1 ) 
# compute likelihood at each value in grid
likelihood <- dbinom( 8 , size=15 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

p2 <- plot( p_grid , posterior , type="b" ,
      xlab="probability of water" , ylab="posterior probability" ,ylim=c(0,.30) )
mtext( "20 points" )
abline(v=.7)
abline(h=posterior[14],lty=2)

```

Q3. This problem is more open-ended than the others. Feel free to collaborate
on the solution. Suppose you want to estimate the Earth’s proportion of
water very precisely. Specifically, you want the 99% percentile interval of the
posterior distribution of p to be only 0.05 wide. This means the distance between
the upper and lower bound of the interval should be 0.05. How many
times will you have to toss the globe to do this? I won’t require a precise
answer. I’m honestly more interested in your approach.

I'd use simulation! Seems like a power calculation
Explore various intervals to home in on optimal.
But have to assume particular value is correct to do this: let's go for .7, with prior excluding values < .5
Could exclude more than this? Clearly, the prior will make it easier with small N obs.


```{r Q3_week1}
truep <- .7
allN <- seq(25,500,25) #sample sizes to test
lengthN <- length(allN)
myresults<-data.frame(matrix(NA,nrow=lengthN,ncol=6))
colnames(myresults) <- c('N','mean','sd','lowCI','hiCI','diff')
thisrow <- 0
for (myn in allN){
  thisrow <- thisrow+1
  W <- round(myn*truep,0)
  L <- myn - W
globe.qa <- quap(
  alist(
    W ~ dbinom( myn ,p) , # binomial likelihood
    p ~ dunif(0,1) # uniform prior
  ) ,
  data=list(W=W,L=L) )
# display summary of quadratic approximation
myprec <- precis( globe.qa )
myresults[thisrow,2:5] <- myprec
myresults[thisrow,1]<-myn
myresults[thisrow,6] <- myprec[4]-myprec[3]
}
```
This gives the required table for output, but the precis gives wrong CI - not sure how to adjust for 99 percentile.

And when I changed the range of N to include much bigger, it crashed.

Also, attempt to change the prior to

      p ~ ifelse( runif(1) < 0.5 , 0 , 1 ) 
    
Led to weird error: Error in rfelse

So I'm still pretty confused by quap, but pleased I got as far as this....

```{r gridapproach}
#2 Globe tossing with biased prior, using grid
p_grid <- seq( from=0 , to=1 , length.out=100)
# define prior
prior <- ifelse( p_grid < 0.5 , 0 , 1 ) 

for (numtries in seq(1000,2000,100)){
# compute likelihood at each value in grid
likelihood <- dbinom( .7*numtries , size=numtries , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior
# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
myhead <- paste0('N tries = ',numtries)
p2 <- plot( p_grid , posterior , type="l" ,
      xlab="probability of water" , ylab="posterior probability" ,ylim=c(0,.50),main=myhead )

abline(v=.7,col='red')

}

```
# Chapter 3

code 3.2- onward

I was unsure about  meaning of 'bandwidth' so the code is tweaked to change sample size.
This confirms that bandwidth goes down with sample size, but it is still unclear exactly what it is.

```{r code3.2}
require(rethinking)
p_grid <- seq( from=0 , to=1 , length.out=1000 )
prob_p <- rep( 1 , 1000 )
prob_data <- dbinom( 6 , size=9 , prob=p_grid )
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)
#modifying script to check meaning of bandwidth
for (b in c(100,1000,10000)){
samples <- sample( p_grid , prob=posterior , size=b , replace=TRUE )
dens(samples)
}

```

Turned to Google, which gives me this
https://stats.stackexchange.com/questions/61374/what-does-bandwidth-mean

**The bandwidth is a measure of how closely you want the density to match the distribution.

See help(density):

    bw the smoothing bandwidth to be used. The kernels are scaled such that this is the standard deviation of the smoothing kernel. (Note this differs from the reference books cited below, and from S-PLUS.)

See also

    adjust the bandwidth used is actually adjust*bw. This makes it easy to specify values like ‘half the default’ bandwidth.**
    
Still unsure, since I don't understand kernel, and the help for density is no help!
But below is the chunk given with example.

Basically, the higher the bandwidth, the more smoothing applied to the plot.
Can think of it like filtering a signal - just related to the bin size over which pts are averaged.


```{r bandwidth}
set.seed(201010)
x <- rnorm(1000, 10, 2)
par(mfrow = c(2,2))
plot(density(x))  #A bit bumpy - so I guess this is the default bandwidth computed in r
plot(density(x,adjust = 10)) #Very smmoth
plot(density(x,adjust = .1)) #crazy bumpy


```
 Next goes on to consider how to compute intervals by sampling from distribution.
 Prefers 'compatibility interval' to confidence or credibility (argh, another term is not going to really help)
 
 This is 3/3 in a binomial test with unbiased prior 
 
```{r intervals} 
 p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep(1,1000)
likelihood <- dbinom( 3 , size=3 , prob=p_grid )
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )
#this means you are sampling FROM pgrid a total of 10000 values without replacement, with a probability that relates to the posterior. So for each sample, you use the posterior relating to the p_grid. !!!

hist(samples) #I added this - shows most at 1

PI( samples , prob=0.5 ) #PI is part of 'rethinking' package, gives percentile intervals
#these use quantile and assign equal mass to each tail - ie this is middle 50%

#DB: If we change the value of prob, we get interval still centred at same point, but narrower if prob is smaller
PI(samples,prob=.3)



```

HPDI is different from PI. It is the narrowest interval containing the specified
probability mass. If you think about it, there must be an infinite number of posterior intervals
with the same mass. But if you want an interval that best represents the parameter values
most consistent with the data, then you want the densest of these intervals. That’s what the
HPDI is. Compute it from the samples with HPDI (also part of rethinking):

```{r hpdi}
HPDI(samples,prob=.5)
HPDI(samples,prob=.3)
HPDI(samples,prob=.95)
```

Yup, this makes sense, esp if you include the plots (need to check code)
Code from version 1 of book is here
https://bookdown.org/content/3890/sampling-the-imaginary.html#sampling-from-a-grid-like-approximate-posterior

This code below is useful training in tidyverse, and also shows how geom_ribbon can be used to colour part of a density function. I will next see if I can do that for the code above!

```{r densplots}
# how many grid points would you like?
n <- 1001
n_success <- 6
n_trials  <- 9

(
  d <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = n),
         # note we're still using a flat uniform prior
         prior  = 1) %>% 
  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %>% 
  mutate(posterior = (likelihood * prior) / sum(likelihood * prior))
)  #DB behaves like a dataframe and has 4 cols which are p_grid, prior, likelihood and posterior

# how many samples would you like?
n_samples <- 1e4

# make it reproducible
set.seed(3)

samples <-
  d %>% 
  sample_n(size = n_samples, weight = posterior, replace = T)

glimpse(samples)
samples %>% 
  mutate(sample_number = 1:n()) %>%   #n here seems to pick up number of rows - I tried resetting and got error saying that sample_number must be length of N rows
  
  ggplot(aes(x = sample_number, y = p_grid)) +
  geom_line(size = 1/10) +
  scale_y_continuous("proportion of water (p)", limits = c(0, 1)) +
  xlab("sample number")

samples %>%  #this just means 'using the data from samples....'
  ggplot(aes(x = p_grid)) +
  geom_density(fill = "black") +
  scale_x_continuous("proportion of water (p)", limits = c(0, 1)) 
d %>% 
  sample_n(size = 1e6, weight = posterior, replace = T) %>% 
  ggplot(aes(x = p_grid)) +
  geom_density(fill = "black") +
  scale_x_continuous("proportion of water (p)", limits = c(0, 1))

p1 <-
  d %>% 
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = d %>% filter(p_grid < .5),
              aes(ymin = 0, ymax = posterior)) +
  labs(x = "proportion of water (p)",
       y = "density")

```


My attempt to apply geom_ribbon to my original data

```{r useribbon}


# Generate data

mynrow <- 1000
p_grid <- seq( from=0 , to=1 , length.out=mynrow )
prior <- rep(1,mynrow)
likelihood <- dbinom( 8 , size=9 , prob=p_grid )
posterior <- likelihood * prior

nsample=100
rownumber<-1:mynrow
mysample <- sample(rownumber,nsample,prob=posterior)


mydf <- data.frame(cbind(p_grid[mysample],posterior[mysample]))
colnames(mydf)<-c('p_grid','posterior')

plot(mydf$p_grid,mydf$posterior)

myrange <- PI(mydf$p_grid , prob=0.5 ) #PI is part of 'rethinking' package, gives percentile intervals
#these use quantile and assign equal mass to each tail - ie this is middle 50%

myrange2 <- HPDI(mydf$p_grid,prob=.5)

  mydf %>% 
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = mydf%>% filter( p_grid<myrange[2],p_grid>myrange[1]),
              aes(ymin = 0, ymax =posterior),fill='lightblue') +
      geom_ribbon(data = mydf%>% filter( p_grid<myrange2[2],p_grid>myrange2[1]),
              aes(ymin = 0, ymax =posterior),fill='darkblue') +
  labs(x = "proportion of water (p)",
       y = "density")
  
```

Phew!