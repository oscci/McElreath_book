---
title: "R Notebook"
output: html_notebook
---

## Bishop notes on McElreath, Chapter 5
Multiple regression.

Using more than one predictor to model an outcome in order to:  
a) Control for confounds  
b) Study complex causation  
c) Study interactions between causal factors.  

Notion of spurious associations, e.g. between N waffle houses and rate of divorce.  Important associations can also be *masked*.  

Multiple regression also dangerous: need to consider causal influences.  

Next example, look at divorce as predicted by age at marriage and marriage rate for each state.

Have to say, I *hate* this example, for four reasons.
First, the data are mostly all bunched up at the low end, so slope of regression seems unduly influenced by a few larger values (?).  
Second, seems weird to have 'state' as unit of analysis, when interested in behaviour like divorce - seems that people want to conclude things like 'early marriage more likely to end in divorce', but that is a conclusion about people, not about states. Not sure why, but this makes me uncomfortable.  
Third, it's confusing to have 'age at marriage' - what about people who don't get married? They *are* included presumably in 'marriage rate', but presumably not in 'age at marriage'. And is 'divorce rate' restricted just to those who got married, or is it base don whole population.  
Fourth - why is any of this remotely interesting? States vary so much on many things, it seems the odds of isolating key variables that can explain anything much is very remote. I assume that things like income, legal practices, religion etc all differ, so to treat on variable (age at marriage in this case) as even a potential cause seems a waste of time to me.  
But let's proceed....  

In lecture, M says what we have been doing, without realising it, is Bayesian networks.  


When variables are standardized, means are zero, and expect the intercept, alpha also to be zero: this is the mean (standardized)divorce rate when both of the predictors are average (ie zero on this scale)

```{r code5.1to3}
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce
str(d) #check out variables in d
d$D <- standardize(d$Divorce)
d$M <- standardize(d$Marriage)
d$A <- standardize(d$MedianAgeMarriage)

#we'll model divorce rate as function of median age at marriage
m5.1 <- quap(
  alist(
    D ~ dnorm(mu,sigma),
    mu <- a + bA*A,  #nb - (within quap can't use equals sign for assignment!)
    a ~ dnorm(0,.2),
    bA ~ dnorm(0,.5),
    sigma ~ dexp(1)),data=d)

precis(m5.1)


```

Note that mean for intercept (a) is indeed zero - see above.
When selecting priors for betas, not so obvious what would be a good range, but don't want it too extreme. Here value is -.57: perhaps larger than we need?

NB: each slope reflects influence of that variable *after* accounting for any others in the equation. Independent influence. (Here with just one variable, it reflects raw correlation, but not when other variables introduced)

Now extract priors.  
(I've gone back to chapter 4 and plugged this kind of code into the final exercise.)
You can use the link function to get posterior distribution with a specific range of values for the predictor.  
See chapter 4 last exercise for another example where the list in the last 'data' statement had two elements (b1 and b2). I guess it needs information on all the parameters that are used as weights for the independent variable. 

Lecture: Prior is full of lines - some more probable than others. We can use extract.prior to look at the range of lines generated by the priors.
These lines are slopes for relationship between independent and dependent variables.
I think we use c(-2,2) below because we only need 2 points to estimate the line, since it is linear. It actually should not matter which 2 points we take?....  

```{r code5.4}
set.seed(10)
prior <- extract.prior(m5.1)
mu <- link(m5.1,post=prior,data=list(A=c(-2,2)))
#mu is matrix with 1000 rows and 2 columns
#I thought these were x and y, but they are NOT!
#They are predicted values of y for x = -2 and x = 2
#(I checked and found I could change A = c(-2,0,2) - then mu has 3 columns!)

plot(NULL,xlim=c(-2,2),ylim=c(-2,2),xlab='z age marriage',ylab='z divorce rate',
     main = 'Plausible regression lines implied by priors') #all variables standardized so range now used is suitable for z-scores centred on zero.

for (i in 1:50) lines(c(-2,2),mu[i,],col=col.alpha("black",.4))
```
Confused here: equal number of slopes are positive and negative, but don't we know that relationship is negative - more divorce with earlier marriage?
I guess question is whether we predicted that before knowing the data - I'd argue that, as least for US society, that is known already, so could be used to constrain prior.  

Ooh! Proud of myself; Changed prior for b to require negative slope
 bA ~ dunif(-.8,0)
 WHen I plugged that in to the model, all slopes were negative.!
 
 But general point here is that prior should generate lines that are in sensible range (e.g. not predicting a zscore of 5 on D for a z-score of 2 on A, for instance - this is demonstrated later on when we get to  model with 2 predictors).
 
 Why exponential for sigma: works well - doesnt' take negative values, and doesn't generate huge numbers either.
 
 Lecture: In practice, prior is not very important for simple models, as the data will overshadow the prior. But for more complex models, priors become important.

```{r postpredm5.1} 
#code 5.5
#compute percentile interval of mean
A_seq <- seq( from =-3, to = 3.2, length.out =30)
mu <- link(m5.1,data=list(A=A_seq))
mu.mean <- apply(mu,2,mean)
mu.PI <- apply(mu,2,PI)

#plot it all
plot(D ~ A, data=d,col=rangi2,main='Figure 5.2b',xlab='Age marriage z',ylab='Divorce z')
lines(A_seq,mu.mean,lwd=2)
shade(mu.PI,A_seq)
precis(m5.1)

```
Now we just do the same thing for Marriage rate x Divorce

```{r m5.2}
m5.2 <- quap(
  alist(
    D ~ dnorm(mu,sigma),
    mu <- a + bM*M,  #nb - (within quap can't use equals sign for assignment!)
    a ~ dnorm(0,.2),
    bM ~ dnorm(0,.5),
    sigma ~ dexp(1)),data=d)

precis(m5.2)
```
Goes on to introduce us to DAGs.  
Have to say I find this example a bit weird, because the unit of analysis is a country. Countries don't do things like getting divorced - it's the people in them. Not sure if this is a valid objection, but something about this feels not quite right for causal reasoning.  

Discusses mediation. 

```{r drawDAGdag}
require(dagitty)
dag5.1 <- dagitty("dag{A -> D; A -> M; M -> D}")
coordinates(dag5.1)<- list(x=c(A=0,D=1,M=2),y=c(A=0,D=1,M=0))
drawdag(dag5.1) #this is part of rethinking package
```

Lecture: Notes that can use modelling to identify dependencies and this may be useful for prediction. But distinguishes prediction from intervention. You need to understand causality for intervention, not for prediction.  
Amen. V pertinent to my studies on child language, where correlations between social background and children's languauge are interpreted causally too often.  

Implied dependencies can be computed from the DAG!

```{r code5.8-9}
#just added the drawings and a bit more explanatory code
DMA_dag2 <- dagitty('dag{ D <- A -> M }')
drawdag(DMA_dag2)
ici2<-impliedConditionalIndependencies( DMA_dag2 )
print('DAG2, Implied Conditional Independencies (blank if none):')
ici2


DMA_dag1 <- dagitty('dag{ D <- A -> M -> D }') 
drawdag(DMA_dag1)
ici1<-impliedConditionalIndependencies( DMA_dag1 )

print('DAG1,Implied Conditional Independencies (blank if none):')
ici1
```
As M remarks, this notation is deeply irritating.

Now on to multiple regression. Analogous to polynomial, except that instead of using powers of one variable, we have different variables. 

```{r code5.10}
#reload data to start
data(WaffleDivorce)
d <- WaffleDivorce
str(d) #check out variables in d
d$D <- standardize(d$Divorce)
d$M <- standardize(d$Marriage)
d$A <- standardize(d$MedianAgeMarriage)

m5.3 <- quap(
  alist(
    D ~ dnorm( mu , sigma ) ,
    mu <- a + bM*M + bA*A ,
    a ~ dnorm( 0 , 0.2 ) ,
    bM ~ dnorm( 0 , 0.5 ) ,
    bA ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 )
  ) , data = d )
precis( m5.3 )

plot( coeftab(m5.1,m5.2,m5.3), par=c("bA","bM") ) #Head explodes at seeing that this produces a nice plot with confidence intervals. How? Why?
#If you omit the par statement, it just plots ALL the things, a, bA, sigma and bM. All with CIs!
#(Thanks to Matt who explained that coeftab no doubt contains code for plotting within it)
```


R code to simulate data

```{r code5.12}
N <- 500 # number of simulated States

age <- rnorm( N ) # sim A (defaults to mean 0 and SD 1)
mar <- rnorm( N , -age ) # sim A -> M - creates random var neg correl with age
div <- rnorm( N , age ) # sim A -> D - create random var + correl with age

#I don't understand this use of rnorm. 
#How does the command know how correlated vars should be?
#In fact they are correlated around .7 - but mar and div have higher variances than age.

#I think it means that it uses the value of each value of age as the mean for the distribution of each of the newly created numbers- yes : test with mybit <- rnorm(5, c(1,2,3,4,5))
#To get sd for mar and div closer to 1, use rnorm(N, -age,.75)
```

## Predictor residual plots
These plots show the outcome against residual predictor values. "They are useful for understanding the statistical model, but not much else."

In our multivariate model of divorce rate, we have two predictors: (1) marriage rate (M)
and (2) median age at marriage (A). To compute predictor residuals for either, we just use
the other predictor to model it. So for marriage rate, this is the model we need:

$$M_i \sim Normal(\mu_i, \sigma)$$

$$\mu_i = a + \beta A_i$$
$$a \sim Normal(0, 0.2)$$
$$\beta \sim Normal(0, 0.5)$$
$$\sigma \sim Exponential(1)$$
```{r 5.13}
m5.4 <- quap(
  alist(
    M ~ dnorm( mu , sigma ) ,
    mu <- a + bAM * A ,
    a ~ dnorm( 0 , 0.2 ) ,
    bAM ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 )
  ) , data = d) #d has 50 rows of data
```
And then we compute the residuals by subtracting the observed marriage rate in each State
from the predicted rate, based upon the model above

```{r 5.14}
mu <- link(m5.4) #matrix with 1000 rows and 50 values, based on d (columns)
mu_mean <- apply( mu , 2 , mean )
mu_resid <- d$M - mu_mean #observed M minus predicted for that value
#Had to work out how to do the plot...
plot(d$M~d$A,xlab='Age at marriage (std)',ylab='Marriage rate (std)')
text(d$A, d$M, d$Location, cex=0.6, col="red") 
lines(mu_mean~d$A)
segments(d$A,d$M,y1=mu_mean) #for drawing vertical lines from prediction to obs

```
M also predicts residuals vs Divorce. Argues value of this is in seeing model-based predictions displayed against outcome, after subtracting influence of other predictor.

Lecture and book: DO NOT USE RESIDUALS AS DATA.
This is often done, but you should do multiple regression instead.  
I *have* used residuals as data, esp when regressing out age. Seem comparable to standardized scores - and I had thought better as based on all data, rather than different means/sds for different ages. M says they will be biased, and we need to remember that residuals are distributions rather than point estimates.  
But from a psychometric perspective, all data could be seen that way - i.e. an observation with error of measurement. (??)  

ANyhow, onwards. This gets confusing as it looks now as if Divorce is the dependent variable (DV).
So redo model first. It does seem as if Divorce is generally being used as DV but there was a confusing digression to consider an alternative model?

```{r bottomplot5.4}
m5.5 <- quap(
  alist(
    D ~ dnorm( mu , sigma ) ,
    mu <- a + bM*M + bA*A ,
    a ~ dnorm( 0 , 0.2 ) ,
    bM ~ dnorm( 0 , 0.5 ) ,
    bA ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 )
  ) , data = d )
precis( m5.5 )
plot(mu_resid~d$D)
lines(mu_mean~d$D)
mu <- link(m5.5) #matrix with 1000 rows and 50 values, based on d (columns)
mu_mean <- apply( mu , 2 , mean )
mu_resid <- d$D - mu_mean #observed D minus predicted for that value

plot(D~mu_resid,data=d)

```

```{r makebookfig5.5postpredplot_code5.15}
#Observed vs predicted divorce - my model 5.5, so code changed a bit
#Call link without specifying new data so it uses original data
mu <- link(m5.5)
#summarise samples across cases
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)

#simulate observations - again no new data, so using original
D_sim <- sim(m5.3,n=10000)
D_PI <-apply(D_sim,2,PI)
#code 5.16
plot(mu_mean ~d$D,col=rangi2,ylim=range(mu_PI),
     xlab='Observed divorce',ylab='Predicted divorce')
abline(a=0,b=1,lty=2)
#I've unpacked the next bit of code so I can comment it
for(i in 1:nrow(d)){ #for each state
  lines(rep(d$D[i],2), #2 lines; start of each line is the observed value for that state
        mu_PI[,i], #end of the lines is the 5% or 95% CI
        col=rangi2) #rangi seems to be a colour command specific to rethinking
}
```
Can see value in top L is Idaho - observed value much lower than predicted. Attributes to Mormons!

## Counterfactuals

Warning - this section is the most confusing in this chapter I think.
I originally skipped it, and was glad I did! Tried to revisit it so I could do one of the exercises.

We're using this DAG
```{r m5.3DAG}
library(dagitty)
mydag <- dagitty("dag{A->M; A->D;M->D}")
coordinates(mydag) <- list(x=c(A=0,D=1,M=2),y=c(A=0,M=0,D=1))
drawdag(mydag)

```
Original model 5.3 had both A and M predicting D, but it ignored the assumption that A influences M.
It is not necessary to include that in the model if we just want to estimate A -> D.
But we do need it to predict consequences of manipulating A, because some of the effect of A acts through M.  

So we need to specify a more complicated model which reflects this. In effect, it comes in two stages: first is standard model predicting D from A and M.
Stage 2 estimates effect of A on M.
Both regressions included in the quap model.


```{r code5.19}
data(WaffleDivorce)
d<-list()
d$A <- standardize(WaffleDivorce$MedianAgeMarriage)
d$D <-standardize(WaffleDivorce$Divorce)
d$M <- standardize(WaffleDivorce$Marriage)

m5.3_A <- quap(
  alist(
    ##A -> D <- M
    D <- dnorm(mu, sigma),
    mu <- a + bM*M + bA*A,
    a ~ dnorm(0,.2),
    bM ~ dnorm (0,.5),
    bA ~ dnorm (0,.5),
    sigma ~ dexp(1),
    ## A -> M
    M ~ dnorm(mu_M,sigma_M), 
    mu_M <- aM + bAM*A, #M depends on another intercept and a slope for A.
    aM <- dnorm(0,.2),
    bAM <- dnorm(0,.5),
    sigma_M <- dexp(1)),
  data=d)

precis(m5.3_A)
```
The bAM coefficient estimates association between A and M. 
(At this stage in the model, it's a coefficient for predicting M from A).

I think I get this: there is a sequential aspect to it - M features in estimate of mu for D, but M is also modelled in relation to prediction from A.

Now we simulate what happens when we manipulate A.
```{r seqA}
A_seq <- seq(from = -2, to = 2,length.out=30) #range of values for A
```

Defines list of imaginary interventions. Now use the model to simulate values from model M5.3_A. It has to simulate M and D in that order. (Agrees with what I said above re sequential order - first simulate influence of A on M, and then use that value of M when simulating A and M on D.)

```{r code5.21}
##prep data
sim_dat <- data.frame(A = A_seq)
#Simulate M and then D
s <- sim(m5.3_A, data=sim_dat, vars = c("M","D"))

#now plot the predictions
plot(sim_dat$A, colMeans(s$D),ylim = c(-2,2), type='l',
     xlab='manipulated A',ylab='counterfactual D')
shade(apply(s$D,2,PI),sim_dat$A)
mtext("Total counterfactual effect of A on D")

plot(sim_dat$A, colMeans(s$M),ylim = c(-2,2), type='l',
     xlab='manipulated A',ylab='counterfactual M')
shade(apply(s$M,2,PI),sim_dat$A)
mtext("counterfactual effect of A ->M")

```
Can also do numerical summaries. Suppose median age at marriage increases from 20 to 30
Makes a new data frame standardized to mean 26.1 and SD 1.24

```{r code5.23}
#Those numbers are obtained from original raw data... Have modified code to make that clearer
origmean <- mean(WaffleDivorce$MedianAgeMarriage)
origsd <- sd(WaffleDivorce$MedianAgeMarriage)

sim2_dat <- data.frame(A =(c(20,30)-origmean)/origsd)
#This is a data frame with just 2 values, which I think are z-score equivalents of MedianAgeMarriage of 20 or 30. This then feeds in to simulation in s2
s2 <- sim(m5.3_A,data=sim2_dat,vars=c("M","D"))
#s2 has 1000 simulated values of M and 1000 values of D
mean(s2$D[,2]-s2$D[,1])
```

## 5.2 Masked relationship

Happens when you have correlated predictors which are correlated with each other but have correlation with DV of opposite sign, so in effect cancel each other out.  
I've tried to think of an example in my own area and can't. I do suspect this is more common when you have these rather weird entities such as counties or species as your units of analysis, than when the person is the unit of analysis. But important, of course, to nevertheless be alert to this.  

Lecture: M also notes that if you have noisy measurement that can mask true relationships but that is a different issue and he'll deal with it later).



```{r 5.28}
library(rethinking)
data(milk)
d <- milk
str(d)

```
Q: to what extent is energy content of milk (kCal) related to percent brain mass that is neocortex?
(NB notes problems with these species comparisons- indeed!)

```{r 5.29}
#standardise variables
d$K <- standardize(d$kcal.per.g)
d$N <- standardize(d$neocortex.perc)
d$M <- standardize(log(d$mass))
```

## Model 5.5
Simple bivariate regression between K and N

$$K_i \sim Normal(\mu_i, \sigma)$$
$$\mu_i = \alpha + \beta_N N_i$$
```{r 5.30}
#first drop missing values (see p 146)
dcc <- d[complete.cases(d$K,d$N,d$M),]
m5.5_draft <- quap(
  alist(
    K ~ dnorm(mu,sigma),
    mu <- a + bN*N,
    a ~ dnorm(0,1),
    bN ~ dnorm(0,1),
    sigma ~ dexp(1)
  ), data =dcc )


```
Now look at priors.
"To simulate from the priors, we can use extract.prior and link as in the previous chapter."
Hmm.As far as I can see this is the first mention of 'extract.prior' - and it's not in the book index.
So we'll have to see what it does.

(NB I had already used m5.5 above, but this should overwrite that - at first it is called m5.5_draft anyhow)  

To understand M's point about 'silly' priors, I extended the scale of X and Y axes to -10,10. His point is that values that are in range of -2,2 for X should predict values within similar range for Y, given that all are z-scores. So if you need to extend axis to include predicted values, something is wrong.
```{r model5.5}
set.seed(10)
prior <- extract.prior( m5.5_draft )
xseq <- c(-10,10) #Code changed by DB
mu <- link( m5.5_draft , post=prior , data=list( N = xseq) )
plot( NULL , xlim=xseq , ylim=xseq,xlab='z Neocortex percent',ylab='z Kcal per g', main = 'a ~ dnorm(0,1) \nbN ~ dnorm(0,1)')
for ( i in 1:50 ) lines(xseq , mu[i,] , col=col.alpha("black",0.4) )
#add lines to show plausible values
abline(h=-2,lty=2,col='red')
abline(h=2,lty=2,col='red')
abline(v=-2,lty=2,col='red')
abline(v=2,lty=2,col='red')
```
These priors give silly values that range all over the place. - in particular, some lines would give extreme values
Need to tighten alpha prior so it sticks closer to zero.
And slope needs to be tighter so doesnt' give impossibly strong relationships.
```{r revisedm5.5}
m5.5 <- quap( 
alist(
K ~ dnorm( mu , sigma ) ,
mu <- a + bN*N ,
a ~ dnorm( 0 , 0.2 ) ,
bN ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data=dcc )

set.seed(10)
prior <- extract.prior( m5.5 )
xseq <- c(-2,2)
#Note that I've used original range here: all lines in sensible range
mu <- link( m5.5 , post=prior , data=list( N = xseq) )
plot( NULL , xlim=xseq , ylim=xseq,xlab='z Neocortex percent',ylab='z Kcal per g', main = 'a ~ dnorm(0,.2) \nbN ~ dnorm(0,.5)')
for ( i in 1:50 ) lines(xseq , mu[i,] , col=col.alpha("black",0.4) )


precis(m5.5)
```
As with earlier example, I think you could probably justify restricting prior slope to positive values, but we know how to do that!

```{r 5.37}
#I've modified this so we can efficiently do both plots
#Even better would be to use a function, but this is a bit more transparent perhaps
for (i in 1:2){ #same scripts for N and M
  dcc$X <- dcc$N
  myxlab <- 'Neocortex z'
  if(i==2)
  {dcc$X <- dcc$M
    myxlab <- 'Log Body mass z'}
m5.5 <- quap( 
alist(
K ~ dnorm( mu , sigma ) ,
mu <- a + bX*X ,
a ~ dnorm( 0 , 0.2 ) ,
bX ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data=dcc )

xseq <- seq( from=min(dcc$X)-0.15 , to=max(dcc$X)+0.15 , length.out=30 )
mu <- link( m5.5 , data=list(X=xseq) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( K ~ X , data=dcc ,xlab=myxlab,ylab='z kCal per g',main='Posterior') #original data
lines( xseq , mu_mean , lwd=2 ) #predicted means
shade( mu_PI , xseq ) #predicted interval
print(precis(m5.5))
}

```
# Counterfactual

M also plots the counterfactual - this shows that if one variable held constant, you can see the effect of the other. I'll try to do this by analogy with the code from 5.19

```{r counterfactualplot}

#assume we still have primate data in memory with z-scores for K N and M

## Model with M and N
  m5.7 <- quap( 
    alist(
      K ~ dnorm( mu , sigma ) ,
      mu <- a + bM*M + bN*N ,
      a ~ dnorm( 0 , 0.2 ) ,
      bM ~ dnorm( 0 , 0.5 ) ,
      bN ~ dnorm( 0 , 0.5 ) ,
      sigma ~ dexp( 1 )
    ) , data=dcc )
precis(m5.7)
plot(coeftab(m5.5,m5.7),pars=c("bM","bN"))

#plot counterfactual
#Again, I'll do both plots in a loop
for (i in 1:2){

xseq <- seq(from =-2,to=2,length.out=30) #modified to simplify it, since z-scores this is good range
if(i==1){
mu <- link(m5.7,data=data.frame(M=xseq,N=0)) #so consider all values of M when N is set to zero
myxlab='z M'
mymain='Counterfactual: N set to zero'
}
if(i==2){
  mu <- link(m5.7,data=data.frame(N=xseq,M=0)) #so consider all values of N when M is zero 
  myxlab='z N'
  mymain='Counterfactual: M set to zero'
}
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot(NULL,xlim=c(-2,2),ylim=c(-2,2),xlab=myxlab,main=mymain,ylab='predicted zK')
lines(xseq,mu_mean,lwd=2)
shade(mu_PI,xseq)
}
```







## EXERCISES

5E1. Which of these are multiple linear regression?
$$(1) \mu_i = \alpha + \beta x_i$$

$$(2) \mu_i = \beta_x x_i + \beta_z z_i$$
$$(3) \mu_i = \alpha + \beta (x_i - z_i)$$
$$(4) \mu_i = \alpha + \beta_x x_i + \beta_z z_i$$
(1) No. Just one independent predictor
(2) Yes, despite lack of intercept
(3) No - confusing because has x and z, but they have a common beta and I think that means x-z can be viewed as a single IV.
(4) Yes. Standard multiple regression

5E2. write down MR to evaluate claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity.
Start by trying to draw the DAG:

Where L is latitude, P is plant diversity and A is animal diversity
```{r trydag}
library(dagitty)
mydag <- dagitty("dag{L->P; P->A;L->A}")
coordinates(mydag) <- list(x=c(A=1,L=0,P=2),y=c(A=1,P=0,L=0))
drawdag(mydag)

#or should it be?
mydag1 <- dagitty("dag{L->P; P->A}")
coordinates(mydag1) <- list(x=c(A=1,L=0,P=2),y=c(A=1,P=0,L=0))
drawdag(mydag1)

```


$$D_i \sim Normal(\mu_i, \sigma)$$
$$\mu_i = \alpha + \beta_1 L_i + \beta_2 P_i$$
$$ \alpha \sim Normal (0, .2)$$
$$\beta_1 \sim Normal (0, .5)$$
$$\beta_2 \sim Normal (0, .5)$$
$$\sigma \sim Exponential (1)$$
This model allows us to test the 2nd part of the claim, because b1 would be significant. But the first part of the claim would require a model that did not inlcude the P term, where there's only be a b for L, and its interval would span zero.
??

## 5E3
Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory by itself is a good predictor of time to PhD degree, but together these variables are both postively associated with time to degree. Indicate which side of zero each slope parameter should be on.

So this is a masked relationship. You'd treat it like the examples in code 5.35, 5.38, 5.39, 5.40, where you compare the models that included IVs of F alone, S alone or both F and S. 

## 5E4
Why is this designated as easy?!  
Utterly baffling.
(2) is the full model with an intercept and betas for all categories.
(1) and (3) each drop one of the categories - does that mean its contribution gets subsumed in alpha?
(4) doesn't seem right as it has no betas!
(5) also lack of betas seems weird, quite apart from the maths, which seems designed to scare us.

So my guess is 1-3 *may* be inferentially equivalent, but I am very unsure.

## 5M1
Invent own example of spurious correlation - outcome correlated with both prdictors. WHen both predictors entered in model, one disappears.
 Child language predicted by parent language and family history of language problems.
 
## 5M2
Invent example of masked relationship
Much harder for my field. 
How about child physical fitness in pandemic.
Amount of exercise and amount of reading . Exercise correlated with positive fitness and reading with negative fitness, so they cancel each other out, if children from wealthier families get more chance for exercise and reading, so they are correlated, and then it looks like neither impact fitness.

## 5M3
Best predictor of fire risk is presence of firefighters. Reversed causal inference.
How might higher divorce rate cause higher marriage rate?  
DB: If there's more divorces there are more single people who can then remarry.
To evaluate that you'd need to distinguish first and subsequent marriages.
Expect correlation between divorce rate and remarriage rate.
Also since divorcees will be older, age at marriage/remarriage might come into it.
But I'm still puzzled about how divorce rate was calculated: presumably as percentage of married people, rather than percentage of whole population?

## 5M4
Ok. try to rise to this challenge....
Start with original model of the divorce data

```{r code5.1to3}
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce
str(d) #check out variables in d
d$D <- standardize(d$Divorce)
d$M <- standardize(d$Marriage)
d$A <- standardize(d$MedianAgeMarriage)

mylds <- read.csv('LDSstats.csv',stringsAsFactor=F)
hist(mylds$LDS..p) #very skewed
hist(log(mylds$LDS..p)) #still v skewed but not so wild
#import the data into the d dataframe, using standardized log values
mylds<-mylds[1:(nrow(mylds)-1),] #lop off last row which is total
mylds$zLDSlog <- standardize(log(mylds$LDS..p))
#I'm sure there is better way than a loop, but here we go
d$zLDSlog <- NA #initialise column
for (i in 1:nrow(d)){
  mystate<- paste0(as.character(d$Location[i])," ") #note space at end of name in lds file
  w<-which(mylds$State==mystate)
  if (length(w)>0){
    d$zLDSlog[i]<-mylds$zLDSlog[w]
  }
}
```


Now we'll model divorce rate , adding zLDSlog in to the mix
```{r modelLDS}
m5LDS <- quap(
  alist(
    D ~ dnorm(mu,sigma),
    mu <- a + bA*A + bM*M + bL*zLDSlog,  #nb - (within quap can't use equals sign for assignment!)
    a ~ dnorm(0,.2),
    bA ~ dnorm(0,.5),
     bM ~ dnorm(0,.5),
     bL~ dnorm(0,.5),
    sigma ~ dexp(1)),data=d)

plot(d$D ~ d$zLDSlog)
precis(m5LDS)

plot(coeftab(m5.3,m5LDS),par=c('bA','bM','bL'))


```

## 5M5
Gasoline price (predictor) related with lower obesity (outcome).
Could be less driving means more exercise.
Could be less driving means less eating out.
How could we test?

Get measures of exercise and of eating out!
(NB I guess DAG could not handle situation where obesity -> more driving etc, as that would introduce feedback loop)
The DAG would look like this:

```{r dag2}
mydag2 <- dagitty("dag{G->D; D->X; D->E; X->O; E->O}")
coordinates(mydag2) <- list(x=c(G=0,D=1,X=0,O=1,E=2),
                            y=c(G=0,D=0,X=1,O=2,E=1))
drawdag(mydag2)
```
This is a start, but I'm pondering how to turn this into a regression model, other than just throwing in all the terms to start with and then looking at effect of dropping some?

## 5H1
If DAG is M -> A -> D
What are implied conditional dependencies
Are data consistent

```{r mydag3}
mydag3 <- dagitty('dag{M -> A ->D}')
impliedConditionalIndependencies(mydag3)

```
Result: D _||_ M|A
This means D is independent of A if we control for M.

So this is answered by m5.3 - means we should see zero value for bA in this model

```{r rcode5.10}
m5.3 <- quap(
  alist(
    D ~ dnorm(mu,sigma),
    mu <- a + bM*M + bA*A,
    a ~ dnorm(0, .2),
    bM ~ dnorm(0,.5),
    bA ~ dnorm(0,.5),
    sigma <- dexp(1)),
  data=d)
precis(m5.3)
```
In fact, bA has interval from -.37 to -.85, so data are not consistent with model.

## 5H3 - wrong (see below)
**OK - I went all around the houses with this one - mainly because I failed to note the instruction to use the example from p 140 as a template. So I tried other routes. I have preserved some of my thoughts/codes here, but I think they will lead us astray, so better to scroll down to the point where I actually did what was asked of us - i.e. section new 5H3 below.**

Assuming that DAG really is M -> A -> D, fit a new model and use it to estimate counterfactual effect of halving state's marriage rate.

I am assuming that we are being asked here to simulate data that do fit the model? (rather than using existing data?)
Later comment: actually, I don't think that's what he wants, but I retain this bit just in case it is useful for discussion at some point.

```{r simulateMADmodel}
#see e.g. rcode 5.12
Nstates <- 50 #(i was going to simulate more, but there aren't more!)
M <- rnorm(Nstates) #default to mean 0 and SD 1, so z-score
#I was confused by syntax in code 5.12, in part because it assumes default SD of 1, and I was unsure how to specify correlation between things
#In fact, this method just ensures things are correlated, but you don't get to specify the size of correlation.
#What we are doing is taking the values of M simulated above as the means for new normal deviates that represent A; then taking those A values as the means for new normal deviates that represent D. So this directly maps on to the idea of a causal chain. No idea if this is what we're supposed to do but it seems to make sense to me.
A <- rnorm(Nstates,-M) #assume lower age assoc with higher marriage rate, so neg
D <- rnorm(Nstates,-A) #assume lower age assoc with higher divorce
mysim <- data.frame(cbind(M,A,D)) #stick them all into a data frame
#Now fit new model
m5MADsim <- quap(
  alist(
    D ~ dnorm(mu,sigma),
    mu <- a + bM*M + bA*A,
    a ~ dnorm(0, .2),
    bM ~ dnorm(0,.5),
    bA ~ dnorm(0,.5),
    sigma <- dexp(1)),
  data=mysim)
precis(m5MADsim)
```
I think that's OK - a is close to zero, and bM close to zero because no direct effect of M on D once A is allowed for. Big effect of M on D
(But see later - this model doesn't represent the sequential effects. For the moment I press on...)

```{r counterfactualplot}

xseq <- seq(from =-2,to=2,length.out=nrow(mysim)) #range to plot on x axis

#So how do we model halving state's marriage rate!
#We could use the simulated data in the link function, but with a new variable that is halved marriage?
mysim$trueM <- mysim$M
mysim$M <- mysim$trueM/2 #to keep the label M for the halved variable so it works in the link function???
mu <- link(m5MADsim,data=data.frame(A=xseq)) 
#I think this is wrong, because we haven't actually used the halved marriage rate anywhere?
myxlab='z M'
mymain='Counterfactual: marriage rate halved'

mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot(NULL,xlim=c(-2,2),ylim=c(-2,2),xlab=myxlab,main=mymain,ylab='predicted D')
lines(xseq,mu_mean,lwd=2)
shade(mu_PI,xseq)

```
I wondered about re-running the original model but with the simulated value of M halved, rather than starting with the link function.
But I guess the idea is to plug in the parameters from the original model, with new values.
In old-fashioned non-Bayesian approach this is trivial - just use the regression formula. But I guess that Bayesian gives us sensible PI around the estimates?
Also, with this approach, we do capture the sequential causal relationship - not sure if you can also do that in regular multiple regression.

## 5H3 - with recommended code!

Well here is attempt to follow eg on p 140-1

```{r MADagain}
m5.3B <- quap(
  alist(
    ##M -> A -> D
    D <- dnorm(mu, sigma),
    mu <- a + bA*A, #I *think* we just omit M here, as paths are in linear seq?
    a ~ dnorm(0,.2),
    bA ~ dnorm (0,.5),
    sigma ~ dexp(1),
    ## M -> A
    A ~ dnorm(mu_A,sigma_A),
    mu_A <- aA + bM*M,
    aA <- dnorm(0,.2),
    bM <- dnorm(0,.5),
    sigma_A <- dexp(1)),
  data=d)

precis(m5.3B)
```

  If I have understood this, then bM is the slope for predicting A from M, and bA is the slope for predicting D from A.
  
  Now we have to do the counterfactual bit. Really struggling with this.
  
  Base on code 5.21
  
  But now are supposed to estimate effect of halving M.
  Closest seems to be code 5.23
  
```{r simdataMAD}
 #so we have to return to original dataset
data(WaffleDivorce)
d <- WaffleDivorce

d$D <- standardize(d$Divorce)
d$M <- standardize(d$Marriage)
d$A <- standardize(d$MedianAgeMarriage)

nuM <- d$Marriage/2
origmean <- mean(d$Marriage)
origsd <- sd(d$Marriage)
nuMz <- (nuM-origmean)/origsd
oldMz<-(d$Marriage-origmean)/origsd
#we have a z-score again, but it is based on halved marriage rate, but standardized against original marriage rate

#sanity check
plot(nuMz,oldMz)

mycheck <-cbind(d$Marriage,nuM,oldMz,nuMz,d$M) 
numean <- mean(nuMz)

#Halving marriage rate moves zM mean from 0 to -2.65

sim2_dat <- data.frame(M=c(-2.65,0))
s2 <- sim(m5.3B,data=sim2_dat,vars=c('A','D'))
print ('original value of mean D is:')
print(mean(s2$D[1]))
print ('revised value of mean D is:')
print(mean(s2$D[2]))
print ('original value of mean A is:')
print(mean(s2$A[1]))
print ('revised value of mean A is:')
print(mean(s2$A[2]))
```
mean(s2$D,[1])
```
I'm very uncertain about this, esp since original z-scores are not zero....

