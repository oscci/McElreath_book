---
title: "R Notebook for McElreath chapter 4"
output: html_notebook
---

```{r loadpackages}
require(rethinking)
require(tidyverse)
```
https://github.com/oscci/McElreath_book

In our discussions, this chapter was challenging. Does help to also view lectures 3-4 which cover the same material. 

Sequence here largely follows the book; I try to add more commentary - both explaining the code, as far as I understand it. Other thing is to be clear about what the graphs are showing - often we found this confusing.

McElreath compares linear regression to Ptolemy's thinking about the universe. Quite clever: jolts you into realising that any method we use is just a model.
That's a core message of this book: don't apply stats formulaicly thinking it will give you the truth - need to understand limitations/assumptions of models.
Reminder of George Box quote:  "All models are wrong, but some are useful".

## 4.1 Why normal distributions are normal
This section was pretty straightforward - useful illustration of basic point about central limit theorem.

Illustrates how repeated sampling from binomial leads to normal distribution if enough trials.

```{r manybinoms}
pos <- replicate( 1000 , sum( runif(16,-1,1) ) )
hist(pos,main ='Results from 1000 runs\n Sum of 16 random numbers from -1 to 1',xlab='Sum of 16 selections') 
# runif means select a random uniform number. runif(16) selects 16 random uniform numbers.
# Default is value from 0 to 1 - not sure why M complicates it, but he's specified range from -1 to 1  : I guess he does that to ensure mean is zero

#Check: what would mean be if we just used runif(16)?

posb <- replicate( 1000 , sum( runif(16 ) ))
hist(posb,main='Results from 1000 runs\n Sum of 16 random numbers from 0 to 1',xlab='Sum of 16 selections')


```
This reminded me of the Galton Board, or quincunx, which illustrates same principle with binomial distribution.
- see https://www.mathsisfun.com/data/quincunx.html

Nice explanation of why *any* kind of distribution will produce normal on repeated sampling: will converge on mean, but errors will be random and push to one direction or other, but cancel each other out.

Now try multiplication

```{r multiplic}
prod( 1 + runif(12,0,0.1) )

#need to deconstruct this
runif(12,0,0.1) #12 numbers between 0 and .1
#Presumably added 1, because need numbers > 1 so they don't get smaller when multiplied
#prod is just command for product (something I did not know....)

#So you are generating 12 numbers between 1 and 1.1 and multiplying them together

#Now do this 10000 times

growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) )
dens( growth , norm.comp=TRUE,main='Product of 12 random numbers from 1 to 1.1',xlab='Product',lty=2 ) 
#Oh wow! I wish I had known about norm.comp earlier in life!
#I've made simulated data line dashed (lty=2) to distinguish from normal curve comparison.

#But we learn that this only works with small numbers. If we change range to 0 to .5, we get deviations from normality

growth <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) )
dens( growth , norm.comp=TRUE,main='Product of 12 random numbers from 1 to 1.5',xlab='Product' ,lty=2) 

# but change to log scale and you get back normal
growth <- replicate( 10000 , log(prod( 1 + runif(12,0,0.5) ) ))
dens( growth , norm.comp=TRUE,main='Log of product of 12 random numbers from 1 to 1.1',xlab='Log of product',lty=2 ) 
```

Onotological: to do with nature of being. Normal distribution common in nature.
Epistemological : theory of knowledge. Consistent with focus on means and variance. Minimal assumptions
This epistemological justification is premised on information theory and maximum
entropy.
Gaussian distribution doesn't handle heavy-tailed distributions very well.

Epistemological justification: from lecture 3: "If all you're willing to say is that there is a mean and a variance, then the Gaussian is the best distribution in terms of being conservative. It assumes nothing but that there is a mean and a variance".
Even if there is skew, the Gaussian covers a wider range than other distributions - it is conservative. Any other distribution will be narrower - have more information build into it.
Least surprising and most conservative distribution has maximum entropy. All distributions used in statistics have maximum entropy.
(Other distributions to be handled in later lectures)

```{r normalcurve}
#Monstrous formula but mostly scaling - key bit is exp -(y-mu)^2

curve( exp( -x^2 ) , from=-3 , to=3 ,main='Normal curve from formula exp(-x^2)')

dnorm(0,mean=0,sd=0.1) #dnorm gives height of normal curve at x, where x is the first number in the expression

myn <- rnorm(2000,0,.1) #abbreviated code for rnorm(n=2000,mean=0,sd=.1)
##rnorm generates n normally distributed numbers, with specified mean and sd. 

plot(density(myn),main='2000 simulated values, mean 0 and sd .1',xlab='Value of myn') #illustrates point that at point 0, height is nearly 4.
#But overall area under curve is 1

#Gaussian often described with precision (tau), rather than variance.
```
Models; mathematical language
e.g. worlds  
W ~ Binomial(N, p)  
p ~ Uniform(0, 1)  
where W was the observed count of water, N was the total number of tosses, and p was the proportion of water on the globe. 

~ means 'is distributed as'

Read the above statement as:
The count W is distributed binomially with sample size N and probability p.
The prior for p is assumed to be uniform between zero and one.

First line defines the likelihood function used in Bayes’ theorem. The other lines define priors.

Both of the lines in this model are stochastic, -ie relationships are probabilistic

```{r modeldefformula}
#Rcode 4.6, p 81
W <- 6; N <- 9
p_grid <- seq(from=0,to=1,length.out=100)
posterior <- dbinom(W,N,p_grid)*dunif(p_grid,0,1)
posterior <- posterior/sum(posterior)
plot(p_grid,posterior,main='Posterior given observed W = 6/9',xlab='True proportion of water')
```
What this plot is showing:

We start with a likelihood function, W ~ Binomial(N, p)  

Where W is the observed count of water, N is the total number of tosses, and p is the proportion of water on the globe. 
Then p_grid shows possible values of p: this is our prior - i.e. p could be anything from 0 to 1.
We computed the likelihood function when W was 6 and N was 9.
So the y-axis shows posterior likelihood for each value of the prior, p.

The likelihood of p being zero is zero (because we have observed 6 cases of water, so truth cannot be zero).  

The likelihood of p being .1 is also vanishingly small, as it would be very unlikely that we'd observe 6/9 waters if there was really only p  = .1.  

In fact, we can check that: p_grid is close to .1 for its 11th value.
so we look at posterior[11]  
This gives value of 6.547829e-06 - i.e. nonzero but very very small.  

The likelihood of p being 1 is also zero, because we had 3 throws where we did not observe water.  
If we look at p_grid, the 91st term is .909, so we can check what is likelihood of around 90% water
posterior [91] = .00359.

Also makes sense that likelihood peaks at .667, which is actual value = 6/9

## A digression to try to understand dunif.

In e.g. above it generates a vector of ones, so it seems a bit redundant. 
But it comes into its own when you want to use a different prior.

I tried changing to dunif(p_grid,0,2) and it generated vector of .5 s !!

#OK this is now getting interesting....
dunif(p_grid,0,2) # vector of .5 s 
dunif(p_grid,.5,1) # produces 1-50 zeros, and 51-100 2s
dunif(p_grid,1,2) # produces 1-99 zeros and one one!

Help says dunif gives density for interval from min to max

OK - so for each value of p_grid, it is evaluated as to whether it falls in that interval
That seems to work , except why do we then get .5s for range 0,2
Values go half way to max range?

 Test: with range .3 to 1: should have 30 zeros and 60 ones?
dunif(p_grid,.3,1) 

No!!  30 zeros, and the rest are 1.428571

Below I explored this a bit more and discovered that:  

For values outside range of min,max, it returns zero  
For values inside range of min,max it returns 1/(max-min)

```{r dunif.explore}
x <- seq(0, 100, by = 1) 
y <- dunif(x,min=0,max=100)
dunif.df <- data.frame(matrix(NA, nrow=9,ncol=5))
colnames(dunif.df)<-c('min','max','sum','mean','maxval')
thisrow<-0
for (mymax in seq(50,200,50)){

for (mymin in seq(25,100,25)){
  thisrow<-thisrow+1
  if(mymin < mymax){
    y <- dunif(x,mymin,mymax)
  plot(x,y,main=paste(' mymin= ',mymin,': mymax = ',mymax),,type='o')
 
  dunif.df[thisrow,1]<-mymin
  dunif.df[thisrow,2]<-mymax
  dunif.df[thisrow,3]<-sum(y)
  dunif.df[thisrow,4]<-mean(y)
  dunif.df[thisrow,5]<-max(y)
 }
}
}
dunif.df<-dunif.df[order(dunif.df$maxval),]



```


Now back to McElreath....  

## Gaussian model of height

Basic logic: There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large SD. Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of mean and SD and rank them by posterior plausibility. Posterior plausibility provides a measure of the logical compatibility of each possible distribution with the data and model.

NB the “estimate” here will be the entire posterior distribution, not any point within it.

```{r howelldat}
require(rethinking)
data(Howell1)
myd <- Howell1 #I have changed name from d to myd, as I find single character names for objects confusing - can look like a function; if I have 'my' at the front, I know it's a variable I have created.
#This also forces me to retype some lines, so pay attention to the code.
#But later on I stop doing this, as it's preferable to have compatible code with McElreath!

precis(myd)

myd2 <- myd[ myd$age >= 18 , ] #select those aged over 17

#NB when first learning R, I found it helpful to break a command like this into 2 steps to make it very clear what the code was doing.

#i.e. 
w <- which(myd$age >= 18) #if you inspect w, you see it's a list of rownumbers for cases with age >=18
myd2 <- myd[w,] #now select from myd those with the same rows as w

#This is not efficient programming, but quite often good programming is quite daunting for newbies to understand. And this creates an additional line, but is easier to read back. now I'm more experienced, I'll more often use the single-line version of code, but not always.
#(Pleased to see that McElreath agrees the standard code for doing this in one line is confusing!)

dens(myd2$height,main='Distribution of heights, aged 18+, Howell data',xlab='height')
```

### Gaussian model specification
(Need to use dollar symbols and backslashes to format math formulae)

$$h_i \sim Normal (\mu, \sigma)$$
Any model has list of terms, some of which you can observe, like heights, and some of which are not observable, like means or regression slopes. 
Have to list these variables and then define them.


This equation can be read as: h is distributed normally with mean of mu and sd of sigma.

3 variables; one observed, two are not.
Mu and sigma need definitions. These are **priors**.
$$\mu \sim Normal (178, 20)$$
$$\sigma \sim Uniform (0, 50)$$
Note that the mean value in prior for mu is just a plausible value.
Important: the SD in the mu definition is SD for the mean - it is NOT for the population. We will simulate for these priors.

This is a key difference from regular regression etc. We are not just estimating a mean, we are going to compute the *relative plausibility* of a range of possible mean values.  

Sigma reflects scatter of heights in the population, and this prior just says we know it is less than 50.  

We'll plot priors. Note: at this point we are not using the observed data at all.


```{r plotpriors}

#plot prior for mu- set mean 178 and SD to 20
curve( dnorm( x , 178 , 20 ) , from=100 , to=250,main='Fig 1. Prior for mean height',xlab='height' )
#NB I like the way he explains he uses prior knowledge of human height here - wouldn't use uniform prior as we already know aspects of height.

curve( dunif( x , 0 , 50 ) , from=-10 , to=60 ,main='Fig 2. Prior for sigma in the population',xlab='sigma') #prior for SD - this is flat between reasonable range
```

This next bit is key. Really important to understand this!

NB. THE PRIOR PREDICTIVE DISTRIBUTION DOES NOT USE ANY OBSERVED DATA!  
It just computes distributions of priors for all the parameters, and then selects combinations of these, so we can see what kinds of result we'd get if we just relied on these priors.
We can then compare the prior predictive distribution with common sense.
and (b) the actual data to see if it is reasonable.  
NB in the lecture he says this is what the model believes *before* it sees the data. Not what you believe, but what the model believes (but who tells the model what to believe?)
Lecture: "This is not p-hacking. We are *not* using the data. We are designing the model from the scientific information we have about the phenomenon."


```{r create_prior_predictive_dist}
#Now compute height distribution by sampling from priors (code R4.14)
sample_mu <- rnorm( 1e4 , 178 , 20 ) #just sample 10000 random values from prior mu
sample_sigma <- runif( 1e4 , 0 , 50 )#just sample 10000 random values from prior sigma
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) #h prior distribution is sampled by using the values sampled from mu and sigma
dens( prior_h,main='Fig 3. Prior predictive distribution for height\nHeights sampled from prior distributions for mu and sigma' ,xlab='height') #nb I played with increasing rnorm n to 1e6 - as I expected, curve gets smoother - because more samples
```
NB. From lecture: This is NOT a normal distribution. This is because the SD varies, and so you get 'fat tails'. It is a t-distribution.  

Need to understand that the Prior Predictive Distribution for height (Fig 3) is different from the distribution of the height prior (Fig 1).

The height prior specifies what we think is a feasible range for the mean of height and how it is distributed (ie normally).  
The prior predictive distribution for height is computed using normal distribution, using our priors for both mean and sigma to sample values that are in the range specified by priors.  

I think this is one of the hardest points to get ones head around.  
Assuming I have understood it, it has to do with the fact that we need to differentiate between two types of 'sigma'. The first one, which occurs within the definition of the mu prior, corresponds to a measurement of the error of the mean: this reflects how confident we are in our estimate of the prior mean (?).
The second one is the prior for sigma, which specifies how much variation there is in the population.  

FWIW, I tried rerunning the chunk about with mu defined as Normal(178,0) - i.e. assuming we are confident the prior mean is 178 with no variation. It still runs, and just creates a really sharply peaked version of Fig 3. Presumably the variation in the distribution is then solely due to the variation in sigma.

```{r assumepriorht}
#Identical to prior chunk, but no variation in height prior; set its sigma to zero
sample_mu <- rnorm( 1e4 , 178 , 0 ) 
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) #h prior distribution is sampled by using the values sampled from mu and sigma
dens( prior_h,main='Fig 3a. Prior predictive distribution for height\nAssuming zero variation in height prior' ,xlab='height') 
```

N.B. *Prior predictive simulation is very useful for assigning sensible priors*

```{r sillyprior}
#Compare with nonsensible prior with large range of mean ht (4.15)
sample_mu <- rnorm( 1e4 , 178 , 100 ) 
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h,main='Prior predictive distribution with unrealistic priors',xlab='height' )
#Leads to having real probabilities assigned to impossibly tall or short and negative heights.
```

Should select priors to avoid impossible values.

```{r codequery}
#My question: could you do this in a single line?
#First just repeat with sensible priors
sample_mu <- rnorm( 1e4 , 178 , 20 )
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) #h prior distribution is sampled by using the values sampled from mu and sigma
dens( prior_h,main='Prior predictive distribution' )

#now try in one step
prior_h2 <- rnorm(1e4,rnorm( 1e4 , 178 , 20 ),runif( 1e4 , 0 , 50 ))
dens(prior_h2,col='red',add=T)  #add = T means add the plot to previous one
#Yes, this achieves similar result (plots are overlaid with red plot being from h2) - but code is harder to understand
```

To further check my understanding, just take first 800 values of mu and sigma: should not be correlated. Next chunk confirms that.  
It also shows that for the mu values (x axis), there is bunching in the middle (because they are normally distributed), but for sigma (y axis) there is no bunching - because of uniform distribution.

```{r tryplot}

plot(sample_mu[1:800],sample_sigma[1:800])
abline(h=mean(sample_sigma))
abline(v = mean(sample_mu))

```
So when we simulate a prior distribution, we are just picking one pair of values from this scatterplot and using that to specify a mean and SD when taking a sample of height from normal distribution. We keep doing that to get the prior distribution. 

We're still not using the actual data, because these are prior probabilities.  

For posteriors, we need to use the raw data - in effect we are going to see how well the observed data matches the model for different values of mu and sigma.


## 4.3.3. Computing the posterior distribution.
(McElreath notes the code is hard to follow and we shouldn't worry...)

For this example, posterior probabilities will be two-dimensional because we have two parameters to estimate.  
Can do this with grid approximation: just take many combinations of mu and sigma.

In this chunk, we're just making a list of mu values and a list of sigma values and creating a dataframe, 'post', which has a row for each combination of these.

This is similar to what we did before with the grid method in the 'waterworld' example, but instead of one dimension (probability of water) we need to compute probabilities for scenarios with 2 parameters (mu and sigma) - so we create a grid with a column for every combination of mu and sigma.  

```{r code4.16}

mu.list <- seq( from=150, to=160 , length.out=100 )  #length.out is new to me but useful - ensures you get desired N points from the seq command (here 100 , not 101)
sigma.list <- seq( from=7 , to=9 , length.out=100 )
post <- expand.grid( mu=mu.list , sigma=sigma.list ) #every combination of mu.list and sigmal.list

head(post) #shows start of all combinations of mu and sigma; will be 100 x 100 rows altogether.
post[495:505,] #show a later point in the file, where sigma is bigger

```

We can now add to the post dataframe the log likelihood.
This is just to explain the idea - v onerous to do this and quickly becomes impossible with many variables.


```{r computepostLL}
post$LL <- sapply( 1:nrow(post) , function(i) sum(
dnorm( myd2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) ) #for each row of post, compute log of dnorm value
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
dunif( post$sigma , 0 , 50 , TRUE ) #adding the log values equiv to multiplying to get total likelihood 
post$prob <- exp( post$prod - max(post$prod) )
contour_xyz( post$mu , post$sigma , post$prob,xlab='mu',ylab='sigma' ,main='Combinations of mu and sigma')
image_xyz( post$mu , post$sigma , post$prob ,xlab='mu',ylab='sigma' ,main='Posterior probabilities for combinations of mu and sigma;\nHigher posterior probabilities are darker') #x is mu, y is sigma, darkness is prob
```

We are computing the probability of each observed value of height in d2, given each combination of mu and sigma that we've specified in the post dataframe. We also compute the probability of observing that mu and sigma combination, given our priors. These two values are then multiplied (though because we are using logs, we add them)  

V hard to follow this, and McElreath says you don't really need to, but it might help to try. Let's take the first row of the 'post' dataframe, where mu is 150 and sigma is 7 and work through the sums.
Note that in heat map, these values are right on the edge of the map, so probability is vanishingly small. 

```{r onerowcheckcode}
#We'll check how we compute the new columns of LL, prod and prob, by just doing this for one row of post, with a given value of mu and sigmal.

mypostrow <- 1 #select which row of post you want to check
mymu <- post$mu[mypostrow]
mysigma <- post$sigma[mypostrow] 
print('Values in post are: ')
print(post[mypostrow,]) #this shows the values that were computed in previous chunk
#For row 1 these are
#  mu sigma       LL      prod         prob
# 150     7 -1299.03 -1307.837 1.962639e-35

#For each row of the raw data, you calculate the probability of getting the observed result if mean is 150 and SD is 7, take the log of these and then add all #the values together to get a log likelihood



#By using logs, you can combine probabilities by adding them together - this is equivalent to multiplying the raw probabilities, which is what you really need to work out the probability of the entire observed set of data.

myLL <- sum(dnorm(myd2$height,mymu,mysigma,log=TRUE)) #mymu and my sigma will be defined above as 150 and 7 if we are just looking at row 1 of post
myLL #Yes this agrees with the value computed for row 1 in previous chunk

#Now we work out how likely this pattern of results is on basis of our priors
#We are adding the log probability for mu and sigma - again equivalent to multiplying raw probabilities
mypriorprob <- dnorm( mymu , 178 , 20 , log= TRUE ) + dunif( mysigma , 0 , 50 ,log= TRUE ) 

myLL+mypriorprob #Yes! this agrees with the prod value for post[1,]
#Again, we are adding log probabilities, so in effect multiplying the observed likelihood with the prior likelihood

#I think the final step of making 'prod' is just a form of rescaling back from a log value to a probability by taking an exponential.  You need to subtract from max(post$prod) which is -1227.921

mymax<-max(post$prod) #This is from previous chunk where prod was computed for all rows.

myprob <- exp( mypriorprob+myLL - mymax )
myprob #Yes, this agrees with prior computed value

print(paste('mu = ',mymu,'; sigma = ',mysigma,';Our computed values are: LL = ',myLL,'; prod = ',myLL+mypriorprob,'; prob = ',myprob))
#For row 1, this is a vanishingly small probability, which makes sense as it is the probability that the observed data were obtained from a distribution with a very small mean and SD.
#For row 550, the probability is higher because mu is closer to middle of distribution

```
This next bit is another key concept that is hard.  

The heatmap we produced above is a 2D likelihood function, so for any combination of mu and sigma, we know how likely it is, given the data and given our priors.  

The next step is the one that seems weird - having gone to all the trouble of generating this function, we now use it to simulate new data.  

Why are we doing this? This is my best explanation, which may be wrong.
Two reasons: first, it is to on the one hand do a sanity check - the simulated data should resemble real data. It's possible it would not resemble real data if the model was badly specified - e.g. we might have a model that assumed a non-normal distribution of mu, when it really was normal. If so, the computations we did would all work, but the regenerated data would presumably look odd.

The other thing we can do with simulated data is to  put boundaries around estimates because the simulated data will generate a range of values, rather than just a single point. We can't do this easily just by computational formula, because the final values will depend on combining the effects of two probabilistic parameters, mu and sigma.

```{r sampledemo}
#small digression for Mickey Mouse e.g. of how the sample function works.
#sample in its simplest form just takes a sample from a vector. e.g.
sample(1:10,size=20,replace=T) #just get 20 randomly selected numbers.

#when we use sampling below, however, we include a prob term. This specifies a probability for every value in range we're sampling from. So in our simple example:
mysample <- sample(1:10,size=20,replace=TRUE,prob=c(0,0,0,0,0,.25,.25,.5,.0,0))
mysample
#in this example, picks number from 1:10, but probability of picking 1-5 or 9,10 is zero, whereas 6,7 have .25 chance of selection, and 8 has .5 chance).
```

We'll now see how we can use the sample function to select rows from the post dataframe in proportion to their posterior probability
```{r postmake}

#Now sample values from posterior in proportion to their probability
#R code 4.19 
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE , prob=post$prob )
#previous line just picks 10000 row numbers, using the posterior probability 

sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows ]
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) ) #cex is size; pch is symbol type, last index of col.alpha gives transparency
```
I was initially puzzled by the stripy appearance of the figure above, but this is just because we have specified fixed values of mu at regular intervals (when specifying mu.list and sigma.list above). This also explains up and down density for mu, below.

```{r mudensity}
dens(sample.mu,xlab='mu')
dens(sample.sigma,xlab='sigma')
#look at percentile intervals
PI( sample.mu ) #formula 4.22
PI( sample.sigma )
``` 


## Issues with tail of SD estimates
Can't be zero so tend to have longer R hand tail.
Can illustrate by taking just a small sample.

```{r SDtail.issue}
d3 <- sample( myd2$height , size=20 ) #code 4.23
mu.list <- seq( from=150, to=170 , length.out=200 ) 
sigma.list <- seq( from=4 , to=20 , length.out=200 )
post2 <- expand.grid( mu=mu.list , sigma=sigma.list )
post2$LL <- sapply( 1:nrow(post2) , function(i)
  sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] ,
              log=TRUE ) ) )
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
  dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob <- exp( post2$prod - max(post2$prod) )
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,
                        prob=post2$prob )
sample2.mu <- post2$mu[ sample2.rows ]
sample2.sigma <- post2$sigma[ sample2.rows ]
plot( sample2.mu , sample2.sigma , cex=0.5 ,
      col=col.alpha(rangi2,0.1) ,
      xlab="mu" , ylab="sigma" , pch=16 )
dens( sample2.sigma , norm.comp=TRUE,xlab='sigma' )

```

## 4.3.5. Finding the posterior distribution with quap.

Quadratic approximation: handy way to quickly make inferences about the shape of the posterior. 

quap is a function devised by McElreath that does a lot of the computations that we've just laboured through.  

Estimate 2 things: a) Peak of posterior, = maximum a posteriori (MAP)  
b) Standard deviations and correlations of parameters (covariance matrix)

With flat priors, this is the same as conventional maximum likelihood estimation.

The posterior’s peak will lie at the maximum a posteriori estimate (MAP), and we can get a useful image of the posterior’s shape by using the quadratic approximation of the posterior distribution at this peak.  
To build the quadratic approximation, we’ll use quap.  

This approximation makes it possible to build higher dimensional models.

It does assert that all parameters are normally distributed. Reasonable; for linear regression this generally works well.  

QUAP uses hill-climbing algorithm.  

NB using alist - explained later:
"The two functions alist and list do the same basic thing: allow you to make a collection of arbitrary R objects. They differ in one important respect: list evaluates the code you embed inside it, while alist does not. So when you define a list of formulas, you should use alist, so the code isn’t executed.""
But when you define a list of start values for parameters, you should use list, so that code like mean(d2$height) will be evaluated to a numeric value.

```{r quap.intro}
library(rethinking)
data(Howell1)
myd <- Howell1
d2 <- myd[ myd$age >= 18 , ]

#Create a list of formulae
flist <- alist(
height ~ dnorm( mu , sigma ) ,
mu ~ dnorm( 190 , 20 ) , #define prior mu (I've changed this to check effect!)
sigma ~ dunif( 0 , 50 ) #define prior sigma 
)

m4.1 <- quap( flist , data=d2 )
precis(m4.1) #this is same as 'summary' in this context

```

These numbers provide Gaussian approximations for each parameter’s marginal distribution.
This means the plausibility of each value of mu after averaging over the plausibilities of each value of sd is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4.  
The 5.5% and 94.5% quantiles are 89% percentile interval boundaries, corresponding to an 89% compatibility interval. Why 89%? It’s just the default. It displays a quite wide interval, so it shows a high-probability range of parameter values. 

"If you want another interval, such as the conventional and mindless 95%, you can use precis(m4.1,prob=0.95)."


```{r extractsamples}
mypost <- extract.samples(m4.1,n=100) #I've just drawn 100 samples, so noisy output
head(post)
dens(mypost$mu,xlab='mu')
dens(mypost$sigma,xlab='sigma')
```
```{r startingvalues}
start <- list(
mu=mean(d2$height),
sigma=sd(d2$height)
)
m4.1 <- quap( flist , data=d2 , start=start )
precis(m4.1)
```

```{r incorporateprior}
#now with meaningful prior, sd of .1
#rcode 4.31
m4.2 <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu ~ dnorm( 178 , 0.1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d2 )
precis( m4.2 )

#Note by changing prior for mean, we affect estimate for SD
```
Lecture 3: McE explains that quap is a scaffold that will teach you to understand models. But when you get more sophisticated and start to use Generalised Linear Models, with nonlinear outcomes, it can lead you astray.

```{r vcov.explore}
diag( vcov( m4.1 ) ) #4.33
cov2cor( vcov( m4.1 ) )

```
 NB correlation matrix is just standardized covariance matrix.
 I.e. the entries on the diagonal will become 1 after conversion to r.
 General formula is r.xy = cov.xy/s.x*s.y
 so if myv <- vcov(m4.1)
 and myv[1,2] is .000218
 then entry for correlation in cell[1,2] will be myv[1,2]/sqrt(myv[1,1]*myv[2,2])

```{r sampleposterior4.34}
library(rethinking) 
post <- extract.samples( m4.1 , n=1e4 )
head(post)
precis(post)

```


# 4.4 Linear prediction

Will look at wt as predictor of ht (seems odd way around, given that weight seems more dependent on environment...)

```{r htwt}
plot( d2$height ~ d2$weight )
```
Now look at predicting with linear model

 Likelihood $$h_i \sim Normal(\mu_i,\sigma)$$
Linear model $$\mu_i = \alpha + \beta(x_i-\bar{x})$$ 
alpha prior $$\alpha \sim Normal(178,20)$$ 
 beta prior $$\beta \sim Normal(0,10)$$
sigma prior $$\sigma \sim Uniform(0,50)$$ 
  
Mu now has subscript to denote that mu depends on the person.

Mu associated with = sign, so is deterministically defined by right hand side.

Lecture: explains centring, which is implicit in the term where mean x is subtracted from xi. This means that alpha has the value of the population mean.
Centring of the predictor should be default behaviour: makes everything easier to understand.

Prior predictive distribution.  
What are we now predicting?  LINES!!

THIS IS PRIOR DISTRIBUTION AGAIN - data not used to generate it.

All possible combinations from priors of a and b are shown.

```{r 4.38}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 )
b <- rnorm( N , 0 , 10 ) #slope has mean 0 , SD 10, can be -ve
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
      xlab="weight" , ylab="height" ) #set up the plot
abline( h=0 , lty=2 ) #horizontal dotted line at zero
abline( h=272 , lty=1 , lwd=0.5 ) #horizontal line at 272
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
                        from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
                        col=col.alpha("black",0.2) )
#because b can be negative get negative ht/wt - not sensible
#also extremes not sensible values, including -ve ht

```


Constrain prior beta to positive values
Do this by using lognormal prior (ie distribution that looks normal when logged, but values are all positive)
```{r lognormal.prior}
b <- rlnorm( 1e4 , 0 , 1 ) #4.40
dens( b , xlim=c(0,5) , adj=0.1 )

#Now use this as prior
set.seed(2971) 
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 )
b <- rlnorm( N , 0 , 1 )
dens( b , xlim=c(0,5) , adj=0.1 )

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , #rcode 4.39
      xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
                        from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
                        col=col.alpha("black",0.2) )
```

Notes re priors and p-hacking.  
The problem is that when the model is adjusted in light of the observed data, then p-values no longer retain their original meaning. False results are to be expected. We don’t pay any attention to p-values in this book. But the danger remains, if we choose our priors conditional on the observed sample, just to get some desired result. The procedure we’ve performed in this chapter is to choose priors conditional on pre-data knowledge of the data— its constraints, ranges, and theoretical relationships. This is why the actual data are not shown in the earlier section. We are judging our priors against general facts, not the sample.



## Digression - lists
I've never really got seriously to grips with lists.
Now is the time to understand them better.
Here's what M says about them:
'The two functions alist and list do the same basic thing: allow you to make a collection of arbitrary R objects. They differ in one important respect: list evaluates the code you embed inside it, while alist does not. So when you define a list of formulas, you should use alist, so the code isn’t executed.
But when you define a list of start values for parameters, you should use list, so that code like mean(d2$height) will be evaluated to a numeric value.'


```{r milan_lists}
# Milan came to the rescue!
# Basic expression evaluation example ----# Create 'list' with some expressions
expressions_list <- list(
  1 + 1,
  1 / 5
  )# Look at expressions
# Here expressions get evaluated
expressions_list
# Create 'alist' with some expressions
expressions_alist <- alist(
  1 + 1,
  1 / 5
  )
# Look at expressions
# Here we actually see the expressions
# I think it's called 'defusing' the expression
expressions_alist

# Regression example ----# Create 'list' with some lm formulas
lm_formulas_list <- list(
  mpg ~ cyl,
  mpg ~ cyl + disp
  )

# Run regressions, this could also be in a loop or apply function
# This seems to work fine
lm(lm_formulas_list[[1]], data = mtcars)
lm(lm_formulas_list[[2]], data = mtcars)

# Create 'alist' with some lm formulas
lm_formulas_alist <- alist(
  mpg ~ cyl,
  mpg ~ cyl + disp
  )

# Run regressions, this could also be in a loop or apply function
# This also seems to work fine, no difference to 'list'
lm(lm_formulas_alist[[1]], data = mtcars)
lm(lm_formulas_alist[[2]], data = mtcars)
```

Use of logarithms /exponentials
All a bit hair-raising

```{r log.exp}
#4.43 - note log_b is a defined variable, not a function here
m4.3 <- quap( 
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + (log_b)*( weight - xbar ),
a ~ dnorm( 178 , 20 ) ,
log_b ~ dnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d2 )
summary(m4.3)
#Note exp(-.1) equals .904, which was b value in original m4.3

#this gives plot from next section
pairs(m4.3)
```

Importance of plotting posterior distributions
allows you to inquire about things that are hard to read from tables:
(1) Whether or not the model fitting procedure worked correctly
(2) The absolute magnitude, rather than merely relative magnitude, of a relationship
between outcome and predictor
(3) The uncertainty surrounding an average relationship
(4) The uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty

## 4.4.3.2. Plotting posterior inference against the data.

```{r 4.46}
plot( height ~ weight , data=d2 , col=rangi2 ) #scatterplot
post <- extract.samples( m4.3b )
a_map <- mean(post$a) #154.6
b_map <- mean(post$log_b) #.903
curve( a_map + b_map*(x - xbar)) #x is vector of x values; this just plots regression line from formula. add=TRUE means add to scatterplot
#this is just best fitting line

#can see range of lines generated in posterior with
#extract.sample(m4.3) - this has 10000 rows, each with a, b and sigma

N <- 10 # code 4.48
dN <- d2[ 1:N , ]
mN <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*( weight - mean(weight) ) ,
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=dN )

#Now let’s plot 20 of these lines, to see what the uncertainty looks like.

# extract 20 samples from the posterior 4.49
Nsamples<-20
post <- extract.samples( mN , Nsamples )
# display raw data and sample size
plot( dN$weight , dN$height ,
      xlim=range(d2$weight) , ylim=range(d2$height) ,
      col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",Nsamples))
# plot the lines, with transparency
for ( i in 1:Nsamples )
  curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,
         col=col.alpha("black",0.3) , add=TRUE )
```


Bayesian perspective: Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model.

```{r moresamples}
post <- extract.samples( m4.3 )
mu_at_50 <- post$a + post$b * ( 50 - xbar )
dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )
PI( mu_at_50 , prob=0.89 )

#What these numbers mean is that the central 89% of the ways for the model to produce the data place the average height between about 159 cm and 160 cm (conditional on the model and data), assuming the weight is 50 kg.


```
Expected value of mu for height in the middle; also a range of values.
But want to do that for all values, not just for 50.

ie need link function

```{r linkfunction}
#define sequence of wts to compute predictions for
# These values will be on horizontal axis
weight.seq <- seq(from=25,to=70,by=1)

#use link to compute mu
mu <- link( m4.3 ,data=data.frame(weight=weight.seq))
dim(mu)
#this gives matrix 1000 x 46; default is 1000 samples; 46 columns, one for each wt that we have specified in weight.seq
#NB we could have put original data in as argument to link function, in which case we'd have a column for each subject.

str(mu)
#mu has 1000 samples for each of 46 cols
dens(mu) #distribution of mu for each weight
```


#what does this look like for individual Ss?
```{r link_inds}
#pick the first few
#use link to compute mu
mu <- link( m4.3 ,data=d2)
dim(mu) #now we have 352 cols; we'll plot density function for first 50 cases

dens(mu[,1],xlim=c(140,170)) #specify range for x axis
for (i in 2:50){
  dens(mu[,i],add=T)
}
```
We need distribution to see distribution of my for each unique weight value on the horizontal axis. So create weight_seq
```{r wtseqposteriors}
#4.54 # define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq( from=25 , to=70 , by=1 )
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link( m4.3 , data=data.frame(weight=weight.seq) )
str(mu)


# use type="n" to hide raw data
plot( height ~ weight , d2 , type="n" )
# loop over samples and plot each mu value
for ( i in 1:100 )
points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )

# summarize the distribution of mu #4.56
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )

# plot raw data 4.57
# fading out points to make line and interval more visible
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight
lines( weight.seq , mu.mean )
# plot a shaded region for 89%
shade( mu.PI , weight.seq )
```
'Bow tie' shape of distribution which shows wider spread at the ends of the distribution.

If shown as shaded region, implies there is a boundary cutoff, but if you plot all the separate lines, you get a better sense of uncertainty.

Uncertainty gets narrower as estimate based on more cases.

To summarize, here’s the recipe for generating predictions and intervals from the posterior of a fit model.
(1) Use link to generate distributions of posterior values for mu. The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values if you want to plot posterior predictions across a range.
(2) Use summary functions like mean or PI to find averages and lower and upper bounds of mu for each value of the predictor variable.
(3) Finally, use plotting functions like lines and shade to draw the lines and intervals.

```{r explaining.link}
#Can make link function by code

#4.58 
# Struggling with this...
post <- extract.samples(m4.3)
#post contains matrix of 10000 estimates of a, b and sigma
mu.link <- function(weight) post$a + post$b*( weight - xbar )
#xbar is mean - previously computed as 44.99
#so mu.link subtracts any value of weight from mean, multiplies by value of b and adds a

weight.seq <- seq( from=25 , to=70 , by=1 )
mu <- sapply( weight.seq , mu.link ) #for each row of mu.link apply the formula for all the wts in range
#Again, remember mu has 1000 rows, and a column for each weight

mu.mean <- apply( mu , 2 , mean ) #find the mean for all the wt values
mu.HPDI <- apply( mu , 2 , HPDI , prob=0.89 ) #find the HPDI for all wt values


```
The syntax is what is difficult for me: the idea you make a function this way, so let's try and make sense of it.

```{r linkfunctsimple}
#try analogous formula with v simple algebra

myfunc <- function(weight) 3+2*weight #double weight and add 3
weight.seq <- seq( from=25 , to=70 , by=5 ) #25,30,35 ...etc
weight.seq
myout <- sapply(weight.seq,myfunc) #yes, this doubles weight and adds 3!
myout

#but in e.g. above, because formula contains reference to post$a and post$b, it is applied to whole column, i.e., all the rows in post


```

4.4.3.5 Prediction intervals
We got prediction interval for average height,but can get for the actual observed heights. For this we need the sd and its uncertainty.

```{r R4.59}
sim.height <- sim(m4.3,data=list(weight=weight.seq))
str(sim.height) #1000 rows and 46 columns
height.PI <- apply(sim.height,2,PI,prob=.89)

#plot raw data
plot(height~ weight, d2, col=col.alpha(rangi2,0.5)) 
# draw MAP line
lines(weight.seq,mu.mean)
shade(mu.HPDI,weight.seq) #HPDI region for line

shade(height.PI,weight.seq) #PI region for simulated heights

```
Narrow shaded interval around line is distribution of mu.
Wider shaded region represents region within which model expects to find 89% of actual heights, at each weight.

!!!!! 

Illustrates distinction between uncertainty in parameter values (narrow shaded area), and uncertainty in sampling process (broader shaded area).

```{r R4.63}
post <- extract.samples(m4.3)
weight.seq <- 25:70
sim.height <- sapply(weight.seq,function(weight)
  rnorm(
    n=nrow(post),
    mean=post$a+post$b*(weight-xbar),
    sd=post$sigma))
height.PI <- apply(sim.height,2,PI,prob=.89)
 

```

## 4.5 Curves from lines

Linear regression does not mean just straight lines!  

Linear models can make curves.


```{r R4.64}
library(rethinking)
data(Howell1)
d <- Howell1
str(d)
plot(d$weight,d$height)
```
```{r R4.65}
d$weight_s <- (d$weight-mean(d$weight))/sd(d$weight) #makes into zscore
d$weight_s2 <- d$weight_s^2
m4.5 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a  + b1*weight_s + b2*weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0,1),
    b2 ~ dnorm(0,1),
    sigma ~ dunif(0,50)
  ),
  data=d)
precis(m4.5)

```

This is my attempt to do what happens in lecture in terms of plotting.
Sort of correct, but still confused as to whether I should be using sim or link.
I think sim?
Sim gives a broader grey region that looks more as if it is reflecting distribution of scores. 
Link gives a very tight fitting grey region - even when a tiny sample used.

But is this because line is made for given range of weights - so the sample size is irrelevant here?

I think if I can understand this, the rest will be easy. Need to check with the book examples.
```{r plotparabola}


d$weight_s <- (d$weight-mean(d$weight))/sd(d$weight)
d$weight_s2 <- d$weight_s^2
post <- extract.samples(m4.5)
weight.seq <- seq(from=25,to=70,by=1)

#use link to compute mu
mu <- link( m4.5 ,data=d[1:10,])
dim(mu)
str(mu)

dens(mu) #distribution of mu for each weight

wtzseq<-seq(-2,2,.5)
wtzseq2 <- wtzseq^2
sim.height <- sim(m4.5,data=list(weight_s=wtzseq,weight_s2=wtzseq2))
str(sim.height) 
height.PI <- apply(sim.height,2,PI,prob=.89)
height.mu<- apply( sim.height , 2 , mean )
#plot raw data
plot(height~ weight_s, d, col=col.alpha(rangi2,0.5)) 
# draw MAP line
mu.mean <- apply( mu , 2 , mean )
lines(wtzseq,height.mu)


shade(height.PI,wtzseq)
```
Comment re polynomials. Not monotonic. Make absurd predictions outside the range of the data.
Also each parameter affects every part of the curve, so hard to understand.



## Dorothy interim comment
At this point, I'm trying to take stock of how this appoach compares with traditional non-Bayesian null hypothesis significance testing.
If we take simple linear regression, with NHST you are typically just asking whether there is a 'significant' relationship between two variables, X and Y. You do that by estimating the best-fitting line where Y = a + bX, and then test if b is significantly different from zero.  That depends both on the gradient of the slope and the standard error of the estimate, and the latter in turn will be influenced by the sample size, and the spread of Y-values. 
The question revolves around what is the probability of the observed data, given the null hypothesis, p(D|H.)

In the Bayesian approach, you are interested in the relative likelihood of a hypothesis of interest, H1, relative to another hypothesis (typically H0), given the data. So you need to compute the likelihood function for H1 and H2, so you can compare them. That requires you to specify H1 - and that means specifying not only the model (Y  = a + bX, for a linear model), but also the prior estimates of the model parameters. The formula in 4.4.2 specifies all of this, and the quap function takes that formula and does all the computation for you.

This is from quap help: 'The point estimates returned correspond to a maximum a posterior, or MAP, estimate. However the intention is that users will use extract.samples and other functions to work with the full posterior.'

Will recap this with the linear case (I haven't done much with polynomials here as I think I do understand them, having worked a lot with them in regular regression).



```{r 4.42}
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18,] #select those aged > 18

plot(d2$weight,d2$height)
# define the average weight, x-bar
xbar <- mean(d2$weight)
# fit model
#mu is no longer a parameter, since it has become a function of the parameters a and b.

m4.3 <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d2 )

#Just checking what this gives us
plot(d2$weight,d2$height)
summary(m4.3) #estimates of intercept and slope of the function, and interval around these
```
(Lecture 4)
We get estimates of a and b from the model, but with Bayesian methods don't just get a single line. We get multiple possible lines, reflecting uncertainty. Each line is ranked by its relative plausibility.

How to show the uncertainty around the inference.  
Sample from posterior:  
  Use mean and sd to approximate the posterior  
  Sample from multivariate normal distribution of parameters  
  Use samples to generate predictions that integrate over the uncertainty.  

For **any model**, you can sample from the posterior, and then push the samples back through the model itself to plot the uncertainty 

Linear regressions have analytic solutions, but other types of models don't.

Integrating over uncertainty: doing calculus but doesn't feel like it.



```{r summaryquap}

  #estimate of sd and its confidence interval (seems same as precis)
#We can also inspect covariance between the things we have estimated
round(vcov(m4.3),3) #looks awful so better if you round it to 3 places as in code 4.45

#We can also extract samples - I'll use head to just display first few rows, as it defaults to sampling 10000
head(extract.samples(m4.3)) #we have 10000 rows, each with posterior estimated a, b and sigma
#in code 4.46, the mean of the posterior a and b estimates is plugged in to plot the posterior line.

# We can plot all the lines and they will overlap more for the most plausible.


#This is skipping forward a bit from the book, to just bring together things quap can give you.

#We can also get the link function for pre-existing data
mywts <- data.frame(weight=25:75) #this is just a single column 'weight' with weights between 25 and 75 - NB the range here goes beyond what is seen in the data
mylink <- link(m4.3,mywts) #look at head of the link function
dim(mylink) #confirms we have 1000 rows and 51 columns. 
#The 1000 rows are probabilities so we can plot the density function for each weight
#Just try a few here for display
dens(mylink[,1],xlab='predicted height from link',main='weight=25')
dens(mylink[,10],xlab='predicted height from link',main='weight=34')
dens(mylink[,50],xlab='predicted height from link',main='weight=74')

#You can also use sim to simulate data
mysim <- sim(m4.3,mywts)
dim(mysim) #we have 1000 rows and 51 columns
#Comparing mysim and mylink - can see both have same dimensions, but simulated data is much noisier. For link function, each col has higher value with small variation; for sim , more variation
head(mylink[,1:6])
head(mysim[,1:6])


```

## Splines
Splines build up wiggly functions from simpler less-wiggly components called basis functions.

```{r R4.72}
library(rethinking)
data("cherry_blossoms")
d<-cherry_blossoms
precis(d)
plot(d$year,d$temp,type='l') #plot as line graph
```

Splines different from polynomials - don't transform the predictor directly; rather generate synthetic predictors, each of which turns a parameter on or off within a range of the predictor.
Each of these variables is a Basis function.
Model looks similar to polynomial, but with series of basis functions

How to generate basis functions. Divide horiz axis into parts using pivot points called knots.
E.g. divide into 4 year ranges using 5 knots.
B variables in formula gently transition from one part to the next - variables tell which knot you are close to.
So at start of function, basis function 1 has value 1 and all others are zero.
Then as move across, 1 declines and 2 increases, etc.
Influnce of each parameter is quite local
At any point, only two basis functions have nonzero values.
Weights for each function obtained by fitting model to data.

```{r R4.73}
#setting knots
#can specify as quantiles
d2 <- d[complete.cases(d$temp),] #just include complete case
num_knots <- 4 #I changed this from 15 to see the effect
knot_list <- quantile(d2$year, probs=seq(0,1,length.out=num_knots))
knot_list

#R4.74
library(splines)
B <- bs(d2$year,
        knots = knot_list[-c(1,num_knots)], #this is the kind of thing that I wish he wouldn't do : just cuts off first and last knot, but needs explaining more!
        degree = 3, intercept=TRUE)
dim(B)
head(B)
plot(B[,1])
for (i in 2:num_knots){
lines(B[,i])
}
```


```{r R4.76}
#B is synthetic data frame; temp x year
m4.7 <- quap(
  alist(
    T ~ dnorm(mu,sigma),
    mu <- a + B %*% w, #that looks like matrix multiplication
    a ~ dnorm(6,10),
    w ~ dnorm(0,1), #prior for each of the weights
    sigma ~ dexp(1)
  ),
  data=list(T=d2$temp,B=B),
  start=list(w=rep(0,ncol(B)))
    
  )
precis(m4.7)
mywts <- precis(m4.7,depth=2) #contains weight estimates as well

#R4.78
mu <- link(m4.7)
dim(mu) #1000 x 1124 dimension matrix!

head(mu[,1:10]) #check head of first 10 cols

mu_PI <- apply(mu,2,PI,.97) #compute 97% PI for columns (index 2)
plot(d2$year,d2$temp,col=col.alpha(rangi2,.3),pch=16) #pch determines symbol type
shade(mu_PI,d2$year,col=col.alpha("black",.5))
```

My reaction to this: it starts to look a bit p-hacky - you can just modify your splines until the fit looks good?

Lecture 4: argues that observed data is not used to build the Basis function.

More uncertainty at parameter level than at the prediction level.

nb once knots determined, you don't use original scale (except for plotting); all modeling based on synthetic variables

Need to worry about overfitting.
Knots and basis degree are choices, not priors.  

My thoughts: what would you use this for? For the cherry tree data, you could not predict outside the range of observations. I guess you could have a temperature model that might be applied to other datasets that deal with temperature over the same period.  I am just not sure that it's particularly useful to be able to model this, when there is so little systematicity in the data.



### Exercises

## 4E1 
In model definition below, which line is the likelihood?
$$y_i \sim Normal (\mu, \sigma)$$
$$\mu \sim Normal (0, 10)$$
$$\sigma \sim Exponential (1)$$
DB: likelihood is the y function: the one whose terms are variables - the other element of the model specify what values the variables take.

## 4E2
In model definition above, how many parameters are in the posterior distribution
DB: I *think* the answer is 2 : mean and sd

## 4E3
"Using the model definition above, write down the appropriate form of Bayes’ theorem that
includes the proper likelihood and priors"
? So this is something like the formula at bottom of p 80 (p 78 in book)
OOH THIS IS HARD
I'll use M and S for observed values and m and s for the mu and sigma
So I guess left hand side is probability of a given m and s when observed is M and S, so 


Left-hand side: Pr(m,s | height)
Right-hand side: Normal(height |m,s )

Then we multiply by prior:
Prior - I assume this is easy as we just adopt uniform prior as before.

## 4E4 
In model definition below, which line is the linear model

$$y_i \sim Normal (\mu, \sigma)$$
$$\mu_i = \alpha + \beta x_i$$
$$\alpha \sim Normal (0,10)$$
$$\beta \sim Normal (0,1)$$
$$\sigma \sim Exponential (2)$$
DB: The definition of \mu looks like linear model


## 4E5. 
In the model definition just above, how many parameters are in the posterior distribution?
Three - \alpha, \beta and \sigma


## 4M1.  For the model definition below, simulate observed y values from the prior (not the posterior).
$$y_i \sim Normal(\mu,\sigma)$$
$$\mu \sim Normal(0,10)$$

\sigma \sim Exponential(1)

see p 83, r code 4.15

```{r simprior}
sample_mu <- rnorm(1e4,0,10)
sample_sigma <- rexp(1e4,1)

#Look at mu and sigma - check first few values
head(sample_mu)
head(sample_sigma)
dens(sample_mu)
dens(sample_sigma)
#prior takes normal deviates using the mu and sigma values we've just simulated
prior_h <-rnorm(1e4,sample_mu,sample_sigma)
dens(prior_h)
```
## 4M2. 
Translate the model just above into a quap formula.

Reminder to self : Quadratic approximation: handy way to quickly make inferences about the shape of the *posterior*. So now we are going to incorporate data.

Estimate 2 things: a) Peak of posterior, = maximum a posteriori (MAP)  
b) Standard deviations and correlations of parameters (covariance matrix)

```{r task4M2}
#based on rcode 4.31
#all seems OK until we get to data.
#So data here is simulated data? Needs a data frame

#Just make random data for y?
mydf <- data.frame(matrix(NA,nrow=100,ncol=2))
colnames(mydf) <- c('x','y')
mydf$x<- runif(100)
mydf$y <- rnorm(100,0,10)
head(mydf)

my4M2 <- quap(
alist(
  y <- dnorm(mu,sigma),
  mu ~ dnorm(0,10), #mean zero and SD of 10
  sigma <- dexp(1)), #exponential peaking at 1
data=mydf)
precis(my4M2)

```


## 4M3
Translate quap formula into a mathematical model definition.
$$y \sim dnorm(\mu,\sigma), $$
$$mu <- a + b*x,$$
$$a \sim dnorm(0,10),$$
$$b \sim dunif(0,1),$$
$$sigma \sim dexp(1)$$
Mathematical model

$$y_i \sim Normal(\mu,\sigma), $$
$$mu_i = \alpha + \beta x_i,$$
$$\alpha \sim Normal(0,10)$$

$$\beta \sim Uniform(0,1)$$
$$\sigma \sim Exponential(1)$$
## 4M4 
A sample of students is measured for height each year for 3 years. After 3rd year, you want to fit a linear regression, predicting height using year as predictor. Write mathematical model definition for this regression.

$$h_i \sim Normal(\mu_i,\sigma)$$
$$\mu_i = \alpha + \beta.y$$

where y is year, defined as 0,1,2

$$\alpha ~ Normal(170,10)$$
$$\beta \sim Log-Normal(0,1)$$
$$\sigma \sim Exponential(1)$$

If first year is y = 0, then first alpha value is mean height at start.  
Whether any growth expected depends on whether these are children or adults!
Beta would be close to zero if adults. In fact, for adults, could simplify model to omit beta altogether, so only variation would be measurement error!

## 4M5
Suppose I remind you that every student got taller each year.
DB: Would mean beta has to be positive - hence use of log normal?

## 4M6
What if variance among hts for students of same age is never more than 64 cm.

DB: Means sigma max is eight. (Could either set to uniform between 0 and 8, or find a log normal that gives that max)

## 4M7
Refit model m4.3 but omit mean weight xbar
First redo original:
```{r log.exp}
#4.43 - note log_b is a defined variable, not a function here
m4.3 <- quap( 
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + (log_b)*( weight - xbar ),
a ~ dnorm( 178 , 20 ) ,
log_b ~ dnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d2 )
summary(m4.3)
#Note exp(-.1) equals .904, which was b value in original m4.3

#this gives plot from next section
pairs(m4.3)
vcov(m4.3)
```

Now do without xbar

```{r log.exp}
#4.43 - note log_b is a defined variable, not a function here
m4.3 <- quap( 
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + (log_b)*( weight ),
a ~ dnorm( 178 , 20 ) ,
log_b ~ dnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) ,
data=d2 )
summary(m4.3)
#Note exp(-.1) equals .904, which was b value in original m4.3

#this gives plot from next section
pairs(m4.3)
vcov(m4.3)
```

The estimate of a changes, but the other estimates remain same.

Variances are now more variable in magnitude - presumably because not centred.




















