---
title: "Chapter 12"
output: html_notebook
---

NB - because ulam models take a long time to run, I have saved the workspace with save.image(). This creates .RData.  

Can also save specific models
```{r savemodels}
objectlist<-c(m12.1,m12.3,m12.4,m12.5,m12.6a,m12.6) 
objectlist<-c(m12.5)
save(objectlist,file="ch12model125.RData") 

```

## Overdispersed counts
Models based on normal distributions can be overly sensitive to extreme observations.  
"Processes are often variable mixtures and this results in thicker tails." (?)
Student t has thicker tail - can be better for out-of-sample prediction.
Similarly for count models = if counts arise from mixture of different processes can have more variation and thicker tails.

Variance is 'dispersion'. Binomial has expected variance of Np(1-p). If observed variance exceeds this amount, after conditioning on predictors, then suggests there is an omitted variable that is producing over-dispersion.  

Best solutino is to discover source of extra dispersion and include it in the model. Can also mitigate effects of over-dispersion.  

Multilevel models (ch 14) can handle overdispersion but here we focus on various continuous mixture models.  

### Beta-binomial models
This is a mixture of binomial distributions.
Instead of having a single probability of success, we have a model where each observation has its own probability of success. So we need to model the distribution of probabilities.  

Back to UCBadmit data. If we ignore departments, data is over-dispersed.  A beta-binomial model can pick up this variation even if we don't enter department explicitly. 

Model assumes each row of data has its own unobserved probability of admission. Distribution of these probabilities follows beta distribution (which is a probability distribution for probabilities that makes the maths easy). 

Beta distribution has 2 parameters average probability p and a shape parameter theta, which determines how spread out the distribution is.  
When theta is 2, ever probability from 0 to 1 is equally likely. As theta increases above 2 distributino of probabilities is more concentrated. If theta is < 2 then v dispersed distribution so that extreme probabilities near 0 and 1 are more likely than the mean.  

```{r code12.1}
require(rethinking) # for dbeta
pbar <- .5
par(mfrow=c(2,3))
for (theta in seq(1:6)){
  if(theta==6){theta <- 60}
curve(dbeta2(x,pbar,theta),from=0 ,to =1,
xlab='probability',ylab='density', main=theta)
}
```

Linear model 
$$A_i \sim BetaBinomial(N_i,p_i,\theta)$$
$$logit(p_i) = \alpha_G_[i]$$
$$\alpha_j \sim Normal(0,1.5)$$
$$\theta = \phi +2$$
$$\phi \sim Exponential(1)$$

alpha gives intercept for each gender.
A is outcome (admission). N is applications.

We want dispersion of at least 2 (flat). 
We know exponential has minimum of zero. So if we add 2 to an exponentially distributed variable, new minimum is 2.

```{r code12.2}
data(UCBadmit)
d<-UCBadmit
d$gid <- ifelse(d$applicant.gender=='male',1L,2L) #codes males as 1 and females as 2.
dat <- list(A=d$admit,N=d$applications,gid=d$gid)
dat
m12.1 <- ulam(
  alist(
    A~ dbetabinom(N,pbar,theta),
    logit(pbar ) <- a[gid],
    a[gid] ~ dnorm(0,1.5),
    transpars>theta <<-phi+2.0,  #theta tagged with transpars (transformed parameters) so Stan will return it in the samples
    phi~dexp(1)
  ),data=dat, chains=4

  )
precis(m12.1) #note theta is just phi + 2

```

```{r code12.3}
post <- extract.samples(m12.1)
post$da <- post$a[,1] - post$a[,2]
precis(post,depth=2)

```
Parameter a[1] is log odds admissions for males and a[2] is log odds for females.  
da is the difference.
This model allows each row in the data (ie each combination of dept and gender) to have its own intercept.  
These unobserved intercepts sampled from beta distribution with mean p and dispersion theta.  

We can plot the distribution

```{r code12.4}
#couldn't get this to work but here's my version
#(found afterwards this was because I had missed first line of code that defined gid)
#draw posterior mean beta distribution
curve(dbeta2(x,mean(logistic(post$a[2])),mean(post$theta)),from=0,to=1,ylab='Density', xlab='proability admit',ylim=c(0,3),lwd=2,lty=2,col='red',main='female red, male blue')

curve(dbeta2(x,mean(logistic(post$a[1])),mean(post$theta)),from=0,to=1,ylab='Density', xlab='proability admit',ylim=c(0,3),lwd=2,col='blue',add=T)


#draw 50 beta distributions samples from posterior
par(mfrow=c(1,2))
for (gid in 1:2){
  mymain<-'Posterior distribution: females'
  if(gid==1){  mymain<-'Posterior distribution: males'}
  i=1
  p <- logistic(post$a[i,gid])
  theta <- post$theta[i]
  curve(dbeta2(x,p,theta),from=0,to=1,ylab='Density', xlab='probability admit',ylim=c(0,3),lwd=1, main=mymain)
     text(.4,2.5,'Plausbility of every combination\n of parameter values',cex=.6)
for (i in 2:50){
  p <- logistic(post$a[i,gid])
  theta <- post$theta[i]
  curve(dbeta2(x,p,theta),from=0,to=1,ylab='Density', xlab='probability admit',ylim=c(0,3),add=T)
}
}
dev.off()
postcheck(m12.1)
text(6,.7,'raw data is blue\nprediction is black,\n with + as 89% prediction interval')
```

Doesn't predict much difference between depts; does predict slightly higher for female than male in each dept. 
M stresses v wide prediction intervals that capture all the points, but in other respects this isn't a v good model!

I tried rerunning with different theta values - either +3 or +0.
Affects shape of the curves for males and females but prediction not much affected.

It's a nice idea to compare same dataset using different methods. In theory would like to do this with some of the other datasets we've worked with.  

## Negative-bionimial or gamma-Poisson
These are the same thing.  
Each Poisson count observation has its own rate.  
Predictor variables adjust shape of distribution not the expectec value of each observation.  
Like beta-binomial, but with gamme distribution of rates replacing beta distrbiutino of probabilities of success.  
In regular Poisson, variance equals the mean; since most real data has other sources of variance, observed variance usually exceeds the mean.  

Working again with Oceanic tools.  
With this model, Hawaii becomes less influential as a data point.  
Because gamma-Poisson expects more variation around mean rate, Hawaii pulls the regression trend less.  
```{r code12.6}
data(Kline)
d <- Kline
d$P <- standardize(log(d$population))
d$contact_id <- ifelse(d$contact=="high",2L,1L)

dat2<-list(
  T=d$total_tools,
  P=d$population,
  cid = d$contact_id)

m12.3 <- ulam(
  alist(
    T ~ dgampois(lambda,phi),
    lambda <- exp(a[cid])*P^b[cid]/g,
    a[cid] ~ dnorm(1,1),
    b[cid] ~ dexp(1),
    g~dexp(1),
    phi~dexp(1)
  ),data=dat2,chains=4,log_lik=TRUE)
```

I'll try and do some plots.
```{r plotm12.3}
precis(m12.3)
postcheck(m12.3)
text(3,100,'raw data is blue\nprediction is black,\n with + as 89% prediction interval')
#now try plot like 12.2

plot(d$population,d$total_tools,col=d$contact) #plotting raw data points
post <- extract.samples(m12.3)
```

?base plot on previous - below is still a mess
#draw 50 distributions samples from posterior

for (cid in 1:2){
  mymain<-'Posterior distribution high'
  if(cid==1){  mymain<-'Posterior distribution: low'}
  i=1
  b <- exp(post$a[i,cid])
  lambda <- exp(po)
  curve(dgampois(x,p,theta),from=0,to=1,ylab='Density', xlab='probability admit',ylim=c(0,3),lwd=1, main=mymain)
     text(.4,2.5,'Plausbility of every combination\n of parameter values',cex=.6)
for (i in 2:50){
  p <- logistic(post$a[i,gid])
  theta <- post$theta[i]
  curve(dbeta2(x,p,theta),from=0,to=1,ylab='Density', xlab='probability admit',ylim=c(0,3),add=T)
}
}


## Overdispersion, entroy and information criteria.  
Both beta-bionmial and gamma-Poisson are maximum entropy for same constraints as binomial and Poisson - they try harder to account for unobserved heterogeneity in probabilities and rates.
Should not use WAIC with models unless very sure of what you are doing.
Beta binomial and gamma Poisson likelihood applies unobserved parameter to each row in the data - so how the data are structured will determine how the beta-distributed or gamma-distributed variation enters the model.  

(I don't understand the next paragraph at all - argument is that this will be fixed in a multilevel model)

## Zero-inflated outcomes
Mixture model uses more than one simple probability distribution to model a mixture of causes. Use more than one likelihood for the same outcome variable.  
Count variables often need mixture treatment - zero can either mean v low rate of events or process not operating - e.g. consider counting birds; if we observe none could be because there are none, or because v few so not observed.

(Nice para here in Rethinking on how simulation liberates us to try out models and test their predictions without needing to wait for a mathematician to legalise the model)

## Zero-inflated Poissons
Back to monasteries and manuscripts.  
Each day many monks finish a small N manuscripts.  Binomial process with large N trials and low probability.
Suppose some days the monks take a break, so no manuscripts.
Then they drink.

Zero can mean that the monks had a day off drinking, or that they worked but failed to complete manuscripts. 
p is probability they spent day drinking
lambda is N manuscripts when monks work.  
Need likelihood function that mixes these two.  

Nice diagram here to show the logic, but his verbal account doesn't help! 

model is
$$y_i ~ ZIPOisson(p_i, \lambda_i)$$
$$logit(p_i) = \alpha_p + \beta_px_i$$
$$log(\lambda_i) = \alpha_\lambda+\beta_\lambda x_i$$
There are two linear models and two link functions, one for each process.
Parameters of linear models differ because any predictor such as x may be associated differently with each part of the mixture.  

We will simulate monk data

```{r code12.7}
prob_drink <- .2
rate_work <- 1 #avg one ms per day

#sample 1 yr
N <- 365

#simulate monk drink days
set.seed(365)
drink <- rbinom(N,1,prob_drink)

#simulate ms completed
y <- (1-drink)*rpois(N,rate_work)

simplehist(y,xlab='ms completed',lwd=4)
zero_drink <- sum(drink)
zeros_work <- sum(y==0 & drink==0)
zeros_total <- sum(y==0)
lines(c(0,0), c(zeros_work,zeros_total),lwd=4,col=rangi2)


```

Blue shows zeros due to drinking; black are on working days.
N zeroes is inflated relative to typical Poisson.  

```{r code12.4}
m12.4 <- ulam(
  alist(
    y ~ dzipois(p,lambda),
    logit(p) <- ap,
    log(lambda) <- al,
    ap ~ dnorm(-1.5,1), #probability of drinking - nudged so likely to be negative
    al ~ dnorm(1,.5)
  ), data = list(y=as.integer(y)),chains=4) #data need to be integers
precis(m12.4)

#restore esimates to natural scale
print(paste0('probability drink = ',inv_logit(precis(m12.4)[1,1])))
print(paste0('rate finish manuscripts = ',exp(precis(m12.4)[2,1])))
```

## Ordered categorical outcomes
E.g. of response options on scale from 1-7.  
Not on an interval scale - not good to treat as continuous measures.  
Like multinomial prediction but to ensure order is treated correctly need a cumulative link function.  
ie probability is probability of that value or any smaller value.  

example using trolley problem.

```{r code12.12}
data(Trolley)
d <- Trolley

#response is integer from 1 to 7
```
We need to redescribe histogram on log-cumulative-odds scale.  
Just as logit is log-odds, so cumulative logit is log-cumulative-odds.  

```{r code12.14}
par(mfrow=c(1,3))
simplehist(d$response,xlim=c(1,7),xlab='response')
#convert to cumulative probabilities
pr_k <- table(d$response)/nrow(d)

#cumsum converts to cumulative proportions
cum_pr_k <- cumsum(pr_k)

#plot
plot(1:7, cum_pr_k,type='b',xlab='response',
     ylab='cumulative proportion',ylim=c(0,1))

#log_cumulative-odds
logit <- function(x) log(x/(1-x))
(lco <- logit(cum_pr_k))
plot(1:6, lco[1:6],type='b',xlab='response',
     ylab='log cumulative odds',ylim=c(-2,1.5))
text(3,1,'These are intercepts')
```
Note we omit final point from log cum odds because for p  =1, value is infinity.

Now need posterior distribution of intercepts, so we can take into account sample size and prior information.  
Need to compute likelihood of each possible response value.
We will use cumulative probabilities to compute the likelihood of Pr (y_i  =k)

In effect we use inverse link to translate from log-cumulative-odds back to cumulative probability.  

```{r likelihoodplot}
#plot
plot(1:7, cum_pr_k,type='b',xlab='response',
     ylab='cumulative proportion',ylim=c(0,1))
#add blue lines to show difference from previous value - these are likelihood ie discrete probability of each individual outcome
segments(x0=1,y0=0,x1=1,y1=cum_pr_k[1],col="blue")
for (i in 2:7){
segments(x0=i,y0=cum_pr_k[i-1],x1=i,y1=cum_pr_k[i],col="blue")
segments(x0=(i-1),y0=cum_pr_k[i-1],x1=i,y1=cum_pr_k[i-1],lty='dotted')
}
text(3,.8,'Blue lines show likelihoods\nobtained by subtraction')
```


Can use ordered logit in ulam - really is a categorical distribution that takes a vector **p** of probabilities of each resonse value below max response.
Each response value k in this vector is defined by its link to an intercept parameter, alpha.  Some weakly regualriszing priors are placed on these intercepts.

We can fit the basic model as follows

```{r code12.16}
m12.5 <- ulam(
  alist(
    R ~ dordlogit(0,cutpoints),
    cutpoints ~ dnorm(0,1.5)
  ),
  data=list(R=d$response),chains=4,cores=3)
```

## Adding predictor variables
Need to define log-cumulative-odds of each response, k, as sum of its intercept and a typical linear model.

Oh god, he's now introducing some pretty hairy predictors, including interactions. I need to try something simpler. Will try just with action. NB this is a between subjects manipulation.

```{r m12.24simplified}
dat<-list(R  =d$response,
          A= d$action)
m12.6a <- ulam(
  alist(
    R ~ dordlogit(phi,cutpoints),
    phi <- bA*A,
    c(bA) ~ dnorm(0,0.5),
    cutpoints ~ dnorm(0,1.5)
  ),data=dat
)

precis(m12.6a)
```

Now try full model

```{r code12.24, message=F}
dat<-list(R  =d$response,
          A= d$action,
          I = d$intention,
          C = d$contact)
m12.6 <- ulam(
  alist(
    R ~ dordlogit(phi,cutpoints),
    phi <- bA*A + bC*C + BI*I,
    BI <- bI + bIA*A +bIC*C,
    c(bA,bI,bC,bIA,bIC) ~ dnorm(0,0.5),
    cutpoints ~ dnorm(0,1.5)
  ),data=dat,chain=4,cores=4)

precis(m12.6,depth=2) #use depth = 2 to see cutpoints
plot(precis(m12.6),xlim=c(-1.4,0))
```
Negative slopes mean each story features reduces the rating of acceptability.
Combination of intention and contact is worst.

Need to plot combinations of predictors to understand effects.

```{r code12.26}

#make list with different combinations of predictors
par(mfrow=c(1,3))

kA <- 0:1
kC <- 0:1
kI <- 0:1

pdat <- expand.grid(kA,kC,kI) #
colnames(pdat)<-c('A','C','I')
phi  <- link(m12.6,data=pdat)$phi #

#Use pordlogit to compute cum. prob for each possible outcome value from 1 to 7, using samples in phi and cutpoints
post <- extract.samples(m12.6)
  for (j in 1:3){
    plot(NULL,type='n',xlab='intention',ylab='probability',
     xlim=c(0,1),ylim=c(0,1),xaxp=c(0,1,1), yaxp=c(0,1,2))
    rowrange<-1:2+(j-1)*2
for (s in 1:50){
  pk <- pordlogit(1:6,phi[s,],post$cutpoints[s,])

  for (i in 1:6){
    lines(kI,pk[rowrange,i],col=col.alpha("black",0.1))
  }
  }
}

pk

```
I've kindof got the idea for the plots, though would like to go over again and think through what they are showing. 
For now will forge on.

## Ordered categorical predictors
In Trolley data, edu variable has levels of completed education for each individual. 

```{r code12.30}
library(rethinking)
data(Trolley)
d <- Trolley
levels(d$edu)
#need to reorder the levels
edu_levels <- c(6,1,8,4,7,2,5,3)
d$edu_new <- edu_levels[d$edu]
#NB edu_new is just a number, not a factor
```

Ordered predictors: each step up in value has its own incremental or marginal effect on the outcome.
For 8 education levels, need 7 parameters.
First level is absorbed into intercept. 
An incremental effect is added for each completed level.

So for 1st level, model is just intercept, for 2nd level add parameter1, for 3rd level parameters 1 and 2, and so on; Last level has all 7 parameters.
M also notes we need betas for 'other stuff' (e.g. ?gender) and these can just be added in.
Sum of all the parameters for education level will be set to 1. These are referred to as deltas.

We treat the maximum sum as a regular beta coeffient, and the deltas are fractions of it.
The overall beta represents maximum education effect.
For base level of education, this beta doesn't appear.

For the delta priors we use Dirichlet distribution - multivariate extension of beta distribution.  
Distribution of probabilities from 0 to 1; whereas beta is distribution for 2 probabilities, Dirichlet is distribution for any number. 
For beta we need alpha and beta parameters; we have vectors of these. When alpha is 2, flat prior.

Now we plot Dirichlet priors

```{r code12.32}
library(gtools)
set.seed(1805)
delta <- rdirichlet(10,alpha=rep(2,7))
str(delta)

#We have 10 vectors of 7 probabilities, each summing to 1

h <- 3
plot(NULL,xlim=c(1,7),ylim=c(0,.4),xlab='index',ylab='probability')
for (i in 1:nrow(delta)){
  lines(1:7,delta[i,],type='b',
        pch=ifelse(i==h,16,1),lwd = ifelse(i==h,4,1.5))
}

par(mfrow=c(1,3))
#I think stacked barchart would be better for this!
delta <- rdirichlet(10,alpha=rep(2,7))
barplot(t(delta),xlab='index', main='Dirichlet simulated runs\nalpha=2, beta=7') #t for transpose

#What happens if alpha increases to 40
delta <- rdirichlet(10,alpha=rep(40,7))
barplot((delta),xlab='index', main='Dirichlet simulated runs\nalpha=40, beta=7') #t for transpose


#What happens if alpha set to .1
delta <- rdirichlet(10,alpha=rep(.1,7))
barplot(t(delta),xlab='index', main='Dirichlet simulated runs\nalpha=.1, beta=7') #t for transpose

```

Oh - he's now going to bolt this on to the already complicated model for trolley problem!

Here we go - this took 1 hr 10 min to run on my mac!

```{r code12.34}
dat <- list(
  R = d$response,
  action=d$action,
  intention=d$intention,
  contact=d$contact,
  E = as.integer(d$edu_new), #edu_new as an index
  alpha = rep(2.1,7) )        #delta prior

  m12.5 <- ulam(
    alist(
      R ~ ordered_logistic(phi,kappa),
      phi <- bE * sum(delta_j[1:E]) + bA*action + bI*intention + bC*contact,
      kappa ~ normal (0,1.5),
      c(bA,bI,bC,bE) ~ normal (0,1),
      vector[8]:delta_j <<- append_row(0,delta),
      simplex[7]: delta ~ dirichlet(alpha)
    ),
  data=dat,chains=3,cores=3)


```
