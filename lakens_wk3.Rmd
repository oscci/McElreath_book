---
title: "lakens wk3"
author: "DVM Bishop"
date: "04/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Alpha levels

Why is 5 sigma equivalent to .0000003?

Is this just the p-value when a z-score = 5?
Yes! Answer is 2.866516e-07
```{r sigma}
z <- 5
 
1-pnorm(z)


```

## P-values and ANOVA
see also http://deevybee.blogspot.com/2013/06/interpreting-unexpected-significant.html

Why do you get that weird distribution of p-values if you use Bonferroni correction?

Suppose you have 4 measures, and you set overall p-value to .05, so for each test, p is .0125.
Alternative is to multiply each p-value by n tests.
I was *very* confused by this, because this gives p-values > 1!
However, I then found that all you do is to censor values at 1, so anything bigger than one is treated as one. This is just what the p.adjust method in stats package does!

```{r bonfdist}
#so for each test, p is just selected at random from 0 to 1.
#And we then multiply each p-value by 4

#make a vector of N p-values selected at random
N  <- 10
pN <- runif(N)
pN
#multiply them by n values
pN*N
#now use p.adjust for same values
p.adjust(pN,method='bonferroni')


require(stats)
allp<-vector() #initialise vector
allph<-allp #copy it so we can compare holm method
for (i in 1:1000){
  p4<-p.adjust(runif(4),method='bonferroni')
  p4h <- p.adjust(runif(4),method='holm')
  allp <- c(allp, p4)
  allph <- c(allph,p4h)
}
hist(allp)
hist(allph)

```

How does he get that weird plot illustrating optional stopping?
I guess if you simulated a 2-group difference when real null effect, you could do for 2 v big groups (e.g. 500) with a 'look' after each 100, retaining p-value if sig but not otherwise?
Yup. Chunk below does it.
I also tried varying total sample size - same basic effect each time

```{r optstop}
nsim <- 1000 #N simulations
totsamplesize <- 5000

allp<-vector() #initialise to hold selected p-value on each run
for (i in 1:nsim){
alldata<-rnorm(totsamplesize,0,1) #500 subs in group A
alldatb <- rnorm(totsamplesize,0,1) #500 subs in group B
  thisp<-1
for (j in 1:5){ 

  lastrow <- j*(totsamplesize/5)
  myp<-t.test(alldata[1:lastrow],alldatb[1:lastrow])$p.value
  if(myp<.05){thisp<-myp}
}
  if(thisp==1){thisp <-myp} #take last value if none is <.05

{allp<-c(allp,thisp)
}
}
hist(allp,breaks=20)

```

App for showing ppv predictions

http://shinyapps.org/apps/PPV/

May help to understand this by considering extremes:

The PPV will be same as alpha when prior likelihood of null is 100% (!) - this is because alpha is defined in terms of trying to find out how likely the null hypothesis is.
But in that case there are no true positives! Null hypothesis is always right.
Power has no effect, but power really makes no sense in this situation!

If the null hypothesis is never correct, we see an opposite situation. Alpha is immaterial, because the null makes no sense. All positives are true positives. 
Now varying power will affect the true positive rate - will be exactly that.

Now set the prior likelihood of the null to 50%.
Changing power affects rate of true positives (will be half the power)
Changing alpha affects rate of false positives (half the alpha, assuming 2-tailed test)

```{r Lakens_optstop}
n<-200 #total number of datapoints (per condition) you are willing to collect after initial 10

D<-0.0 #True effect size (Keep SD below to 1, otherwise, this is just mean dif, not d)
SD<-1 #Set True standard deviation.

p<-numeric(n) #store p-values
x<-numeric(n) #store x-values - i.e. this is group 1
y<-numeric(n) #store y-values -i.e. this is group 2

n<-n+10 #script calculates p-values after 10 people in each condition, so add 10 to number of datapoints

for(i in 10:n){ #for each simulated participants after the first 10
  x[i]<-rnorm(n = 1, mean = 0, sd = SD)
  y[i]<-rnorm(n = 1, mean = D, sd = SD)
  z<-t.test(x[1:i],y[1:i], var.equal=TRUE) #perform the t-test
  p[i]<-z$p.value 
}

p<-p[10:n] #Remove forst 10 empty p-values

#Create the plot
#png(file="p-value_over_time.png",width=4000,height=2000, , units = "px", res = 500)
plot(0, col="red", lty=1, lwd=3, ylim=c(0,1), xlim=c(10,n), type="l", xlab='sample size', ylab='p-value', cex.lab=1, cex.axis=1, xaxt = "n")
lines(p, lwd=2)
abline(h=0.05, col="darkgrey", lty=2, lwd=2) #draw ine at p = 0.05
axis(1, at=seq(0, n-10, by=(n-10)/4), labels = seq(10, n, by=(n-10)/4))
#dev.off()

min(p) #Return lowest p-value from all looks
cat("The lowest p-value was observed at sample size",which.min(p)+10) #Return the sample size at which the p-value was smallest
cat("The p-value dropped below 0.05 for the first time as sample size",which(p<0.05)[1]+10) #Return the sample size at which the p-value dropped below 0.05 for the first

#Š Daniel Lakens, 2016. 
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/
```

```{r optstopsim_lakens}
#Hmm - this is doing similar thing to my optstop chunk above, but seems quite clunky!
#In part because I just simulated max, then ran test at different points, where L actually simulates each stage.

N<-100 #total number of datapoints (per condition) you are willing to collect
Looks<-5 #set number of looks at the data
nSim<-500 #number of simulated studies (originally at 50000 - crashed my machine!)
alpha<-0.05 #set alpha

D<-0 #True effect size (must be 0 when simulating Type 1 errors)

#Take care of some settings
options(scipen=100, digits=4) #disable scientific notation for numbers
LookN<-ceiling(seq(0,N,N/Looks)) #Determine at which N's to look
LookN<-LookN[-1] #remove look at 0
LookN<-LookN[LookN > 2] #Remove looks at N of 1 or 2 (not possible with t-test)
Looks<-length(LookN) #if looks are removed, change number of looks
matp<-matrix(NA, nrow=nSim, ncol=Looks) #Matrix for p-values at sequential tests
SigSeq<-numeric(Looks) #Variable to store final p-values
OptStop<-numeric(nSim) #variable to store positions of optional stopping
p<-numeric(nSim) #Variable to save optional stopping p-values

#Loop data generation for each study, then loop to perform a test for each N 
for (i in 1:nSim){
  x<-rnorm(n = N, mean = 0, sd = 1)
  y<-rnorm(n = N, mean = D, sd = 1)
  for (j in 1:Looks){
  matp[i,j]<-t.test(x[1:LookN[j]],y[1:LookN[j]], var.equal=TRUE)$p.value #perform the t-test, store
  }
  if(i/100==round(i/100,0)){ #I altered progress marker to avoid slowing up script with each iteration being marked
  cat('Loop', i, 'of', nSim,'\n')
  }
}

#Save Type 1 error rate for each look
for (i in 1:Looks){
  SigSeq[i] <- sum(matp[,i]<alpha)
}

#Get the positions at which are stopped, and then these p-values
for (i in 1:nSim){
  OptStop[i] <- min(which(matp[i,]<alpha))
}
OptStop[is.infinite(OptStop)] <- Looks #If nothing significant, take last p-value (fixes error warning)
for (i in 1:nSim){
  p[i] <- matp[i,OptStop[i]]
}

breaks<-20 #changed this to smooth the histo with smaller N runs
hist(p, breaks=breaks,col="grey")
abline(h=nSim/breaks, col = "red", lty=3)

#Return Type 1 error rates for each look, and the the Type 1 error rate when only reporting the lowest p-value over all looks
cat("Type 1 error rates for look 1 to", Looks,":", SigSeq/nSim)
cat("Type 1 error rate when only the lowest p-value for all looks is reported:", sum(p<alpha)/nSim)

#Š Daniel Lakens, 2016. 
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/
```